{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SciTDM+SciModel-Bio.ipynb","provenance":[{"file_id":"1apRBKJ_i2t4gsKy0TfRAM0Gih4gDkI15","timestamp":1634538734401}],"collapsed_sections":["LyMVIljFrioE","52t8l945rf1Y","-DBrafz0KqEq","oGYV8ipcJ5fU","7LthVVOhx_Sz","1Nh8Lcpvb_F4","kv6pGzbGpv2D","BBA4owXH8oS1","1DkDHptAhniL","dcdDIwq0kXy2","k0w9Lc0d3iOT","hC35X0v72kXB","ymiejDO5srYE"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3fbd4d91bacb45e3abcd7cbf6111cc98":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ce05b9790b76452dacf316d115f37dd4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_883b51b9383d4d368e145853bf9a0080","IPY_MODEL_108415a183234c77bbaabd3d6a0636e8","IPY_MODEL_642f316b37334041a7ead6784627c25e"]}},"ce05b9790b76452dacf316d115f37dd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"883b51b9383d4d368e145853bf9a0080":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0ff2faf31afd4bd1b02447140023abab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1ed1a48181944c4aa5dfb63560f01ef4"}},"108415a183234c77bbaabd3d6a0636e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_688e96728be54cb9a1611ebb49921fb6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":385,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":385,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9899fd8c39844782b3593da1c8f18d92"}},"642f316b37334041a7ead6784627c25e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_72dc543f7f95479f97ebed17cc70ee94","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 385/385 [00:00&lt;00:00, 3.76kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b9781b21a4ef4cafa3e302323c13d83b"}},"0ff2faf31afd4bd1b02447140023abab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1ed1a48181944c4aa5dfb63560f01ef4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"688e96728be54cb9a1611ebb49921fb6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9899fd8c39844782b3593da1c8f18d92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72dc543f7f95479f97ebed17cc70ee94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b9781b21a4ef4cafa3e302323c13d83b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"24d819971aa64f87b434c521ea3cf08a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1b2f53e6ae4b4587b1ac854abb0c7954","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4e871f2e69d84f34911987e2282f95f3","IPY_MODEL_dc059039abb04e0189df4efce473b41c","IPY_MODEL_7f8dcee737f246ae89fe4cb143c14573"]}},"1b2f53e6ae4b4587b1ac854abb0c7954":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4e871f2e69d84f34911987e2282f95f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c5dee87b52e24f01b63d332949c96888","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c929cfff4ef148b887023ddd05a6a743"}},"dc059039abb04e0189df4efce473b41c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4e5e521a24e841a7a7c4de408579c173","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":227845,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":227845,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_400efb8f61f6491fa4e4059bdb1b8f38"}},"7f8dcee737f246ae89fe4cb143c14573":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d63f36ebc8e1408c96370d7fa46ed449","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 223k/223k [00:00&lt;00:00, 797kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d8b49ffa889b415ebc5c146cd17298c8"}},"c5dee87b52e24f01b63d332949c96888":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c929cfff4ef148b887023ddd05a6a743":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4e5e521a24e841a7a7c4de408579c173":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"400efb8f61f6491fa4e4059bdb1b8f38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d63f36ebc8e1408c96370d7fa46ed449":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d8b49ffa889b415ebc5c146cd17298c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b22f7af4284e47648e56e03b34623f53":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d958e5821a654134bbbefcfaf0343433","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_550178262abf4ac7880faa88b8c0f2cc","IPY_MODEL_6ed223d28f554a1a9f4b24d4858f4fd9","IPY_MODEL_81c39ecc98404915b33996cc4a288108"]}},"d958e5821a654134bbbefcfaf0343433":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"550178262abf4ac7880faa88b8c0f2cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_414a761072434ba49c60a21d4dbb5b38","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_85aa22c5ca3a48f98d0d4721b0dec343"}},"6ed223d28f554a1a9f4b24d4858f4fd9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_00c4068b072e4cf184f92371913df7a3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":442221694,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442221694,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef42d692f52949a889be2edf4167cf60"}},"81c39ecc98404915b33996cc4a288108":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_258757b920df4a639c5e6da7e2abbf69","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 422M/422M [00:08&lt;00:00, 53.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c2fa6df7a3a94b34aa744fe095c7a074"}},"414a761072434ba49c60a21d4dbb5b38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"85aa22c5ca3a48f98d0d4721b0dec343":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"00c4068b072e4cf184f92371913df7a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ef42d692f52949a889be2edf4167cf60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"258757b920df4a639c5e6da7e2abbf69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c2fa6df7a3a94b34aa744fe095c7a074":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQ6H5sYFfFCM","executionInfo":{"status":"ok","timestamp":1638577821479,"user_tz":360,"elapsed":242,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"1c6f9353-196c-43fa-a996-765dca75925e"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data\n"]}]},{"cell_type":"markdown","metadata":{"id":"LyMVIljFrioE"},"source":["#External Databases"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeeP8a6fKXOX","executionInfo":{"status":"ok","timestamp":1646967348982,"user_tz":360,"elapsed":34055,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"55d9e3f6-130b-41f1-bb1e-8e06f540655f"},"source":["! pip install git+https://github.com/huggingface/transformers.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-zvftc10h\n","  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-zvftc10h\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 9.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.2)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 55.1 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 59.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3835851 sha256=5c6f3f6b6eb96885e38fc39df988e2637dc6c916331038a16c84693ad744954c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-oq3q0ya2/wheels/90/a5/44/6bcd83827c8a60628c5ad602f429cd5076bcce5f2a90054947\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.18.0.dev0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_L4bHxBGsZk_","executionInfo":{"status":"ok","timestamp":1646967368467,"user_tz":360,"elapsed":19490,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"465307fd-6041-4a6a-d504-808ad59b7d7f"},"source":["! pip install ray[tune]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ray[tune]\n","  Downloading ray-1.11.0-cp37-cp37m-manylinux2014_x86_64.whl (52.7 MB)\n","\u001b[K     |████████████████████████████████| 52.7 MB 60 kB/s \n","\u001b[?25hCollecting redis>=3.5.0\n","  Downloading redis-4.1.4-py3-none-any.whl (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 12.2 MB/s \n","\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (21.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.6.0)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.21.5)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.17.3)\n","Collecting grpcio<=1.43.0,>=1.28.1\n","  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 41.4 MB/s \n","\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (6.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.3.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.5)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n","Collecting tensorboardX>=1.9\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 52.3 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.15.0)\n","Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (4.11.2)\n","Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (21.3)\n","Collecting deprecated>=1.2.3\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[tune]) (1.13.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis>=3.5.0->ray[tune]) (3.0.7)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (5.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (0.18.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2018.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n","Installing collected packages: deprecated, redis, grpcio, tensorboardX, ray\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.44.0\n","    Uninstalling grpcio-1.44.0:\n","      Successfully uninstalled grpcio-1.44.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n","Successfully installed deprecated-1.2.13 grpcio-1.43.0 ray-1.11.0 redis-4.1.4 tensorboardX-2.5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UI0tseYdav7D","executionInfo":{"status":"ok","timestamp":1646967383227,"user_tz":360,"elapsed":14766,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"6a2c17ef-f1a8-4d42-99f3-e30a7c73ff59"},"source":["!pip install datasets"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n","\u001b[K     |████████████████████████████████| 312 kB 9.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n","\u001b[K     |████████████████████████████████| 134 kB 50.4 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.2)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 26.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 53.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 51.2 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 50.9 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 1.7 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 49.1 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.4 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install seqeval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AnZ5cqvYDdud","executionInfo":{"status":"ok","timestamp":1646967392510,"user_tz":360,"elapsed":9296,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"70da669e-6c0b-41aa-928b-b975128c4b71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 35.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=4385ff5c7284c18d98381824f968d5ed6faaaaead0d8c271731c897a70ab797c\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"52t8l945rf1Y"},"source":["#Libraries"]},{"cell_type":"code","metadata":{"id":"qSbdGMYCgquq","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["3fbd4d91bacb45e3abcd7cbf6111cc98","ce05b9790b76452dacf316d115f37dd4","883b51b9383d4d368e145853bf9a0080","108415a183234c77bbaabd3d6a0636e8","642f316b37334041a7ead6784627c25e","0ff2faf31afd4bd1b02447140023abab","1ed1a48181944c4aa5dfb63560f01ef4","688e96728be54cb9a1611ebb49921fb6","9899fd8c39844782b3593da1c8f18d92","72dc543f7f95479f97ebed17cc70ee94","b9781b21a4ef4cafa3e302323c13d83b","24d819971aa64f87b434c521ea3cf08a","1b2f53e6ae4b4587b1ac854abb0c7954","4e871f2e69d84f34911987e2282f95f3","dc059039abb04e0189df4efce473b41c","7f8dcee737f246ae89fe4cb143c14573","c5dee87b52e24f01b63d332949c96888","c929cfff4ef148b887023ddd05a6a743","4e5e521a24e841a7a7c4de408579c173","400efb8f61f6491fa4e4059bdb1b8f38","d63f36ebc8e1408c96370d7fa46ed449","d8b49ffa889b415ebc5c146cd17298c8","b22f7af4284e47648e56e03b34623f53","d958e5821a654134bbbefcfaf0343433","550178262abf4ac7880faa88b8c0f2cc","6ed223d28f554a1a9f4b24d4858f4fd9","81c39ecc98404915b33996cc4a288108","414a761072434ba49c60a21d4dbb5b38","85aa22c5ca3a48f98d0d4721b0dec343","00c4068b072e4cf184f92371913df7a3","ef42d692f52949a889be2edf4167cf60","258757b920df4a639c5e6da7e2abbf69","c2fa6df7a3a94b34aa744fe095c7a074"]},"executionInfo":{"status":"ok","timestamp":1646967419024,"user_tz":360,"elapsed":26524,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"0ec0c3a7-7479-49f8-e36a-75bd8e64a04c"},"source":["from google.colab import files\n","\n","import os\n","import re\n","import json\n","import string\n","\n","import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","#import tensorflow as tf\n","#from tensorflow import keras\n","#from tensorflow.keras import layers\n","\n","from datasets import load_metric\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from seqeval.metrics import classification_report\n","from sklearn.metrics import precision_recall_fscore_support as prfs\n","\n","import tensorflow_hub as hub\n","from keras import backend as K\n","\n","import transformers\n","print(transformers.__version__)\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers.trainer_utils import EvalLoopOutput\n","\n","from transformers import AutoTokenizer, AutoModel\n","BertTokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n","BertEmbModel = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.18.0.dev0\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fbd4d91bacb45e3abcd7cbf6111cc98","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24d819971aa64f87b434c521ea3cf08a","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/223k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b22f7af4284e47648e56e03b34623f53","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/422M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","metadata":{"id":"-DBrafz0KqEq"},"source":["#Loading Dataset"]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/"},"id":"ICN0rgXve6o1","executionInfo":{"status":"ok","timestamp":1646967493606,"user_tz":360,"elapsed":74597,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"c480ae5f-b84c-48f0-bc69-495ad796a5f7"},"source":["uploaded = files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-dd9013c0-b291-44b2-8475-5f14dc4d3295\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-dd9013c0-b291-44b2-8475-5f14dc4d3295\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving test_500_v2.conll to test_500_v2.conll\n","Saving train_1500_v2.conll to train_1500_v2.conll\n"]}]},{"cell_type":"code","metadata":{"id":"Yf9yoQx4D7r9"},"source":["class TDMSciDataset(torch.utils.data.Dataset):\n","  def __init__(self, raw_x, raw_y, max_length=250):\n","    self.raw_x = raw_x\n","    self.raw_y = raw_y\n","    \n","    self.max_length = max_length\n","\n","  def tokenize_and_preserve_labels(self, sentence, text_labels):\n","    \"\"\"\n","    The tokenizer can split single words into multiple tokens - this breaks\n","    the labels, so we need to keep track of this!\n","    \"\"\"\n","    tokenized_sentence = []\n","    labels = []\n","\n","    for word, label in zip(sentence, text_labels):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = BertTokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels\n","\n","  def __getitem__(self, idx):\n","    tokens = self.raw_x[idx]\n","    labels = np.zeros((len(tokens)))\n","    for i, label in enumerate(self.raw_y[idx]):\n","      labels[i] = label\n","\n","    # This could be moved to __init__ to save time?\n","    tokens, labels = self.tokenize_and_preserve_labels(tokens, labels)\n","\n","    # Convert each token to an id number \n","    input_ids = BertTokenizer.convert_tokens_to_ids(tokens)\n","    \n","    # add and adjust for special tokens\n","    input_ids = [BertTokenizer.cls_token_id] + input_ids + [BertTokenizer.sep_token_id]  \n","    labels = [0] + labels + [0]\n","\n","    # Pad inputs\n","    input_ids = torch.tensor(np.pad(input_ids, [0, self.max_length-len(input_ids)]))\n","    labels = torch.tensor(np.pad(labels, [0, self.max_length-len(labels)], constant_values=-100))\n","\n","    attention_mask = torch.tensor([int(i != 0) for i in input_ids])\n","\n","    #return {input_ids, labels}\n","    #return {'input_ids': [input_ids], 'labels': labels}\n","    return {'input_ids': [input_ids], 'attention_mask': [attention_mask], 'labels': labels}\n","\n","  def __len__(self):\n","    return len(self.raw_y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYy-iTnlEU8K"},"source":["# Entity maps\n","def Entity2ID(Sc_Entity):\n","  return {\n","        'B-TASK': 1,\n","        'I-TASK': 2,\n","        'B-DATASET': 3,\n","        'I-DATASET': 4,\n","        'B-METRIC': 5,\n","        'I-METRIC': 6,\n","        'None': 0,\n","        'O': 0,\n","    }[Sc_Entity]\n","\n","def ID2Entity(Sc_Entity):\n","  return {\n","        1: 'B-TASK',\n","        2: 'I-TASK',\n","        3: 'B-DATASET',\n","        4: 'I-DATASET',\n","        5: 'B-METRIC',\n","        6: 'I-METRIC',\n","        0: 'None',\n","    }[Sc_Entity]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NxRNljBqETC0","executionInfo":{"status":"ok","timestamp":1646967493771,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"cbcb52b5-29ed-4bda-aacf-81da555f9855"},"source":["# Load raw data train\n","raw_x = []\n","raw_y = []\n","\n","cur_x = []\n","cur_y = []\n","with open('train_1500_v2.conll', 'r') as f:\n","  for line in f:\n","    if len(line.strip()) <= 3:\n","      raw_x.append(cur_x)\n","      raw_y.append(cur_y)\n","      cur_x = []\n","      cur_y = []\n","    else:\n","      line = line.strip().split('\\t')\n","      cur_x.append(line[0])\n","      cur_y.append(Entity2ID(line[-1]))  # use this for BIO tags\n","      # cur_y.append(Entity2ID(line[-1].split('-')[-1]))  # use this for span tags\n","\n","print(raw_x[10])\n","print(raw_y[10])\n","\n","# Create actual dataset\n","train = TDMSciDataset(raw_x, raw_y)\n","\n","#print(train[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['The', 'results', 'show', 'that', 'proposed', 'shallow', 'representations', 'of', 'sentence', 'structure', 'are', 'robust', 'to', 'reductions', 'in', 'parsing', 'accuracy', ',', 'and', 'that', 'the', 'contribution', 'of', 'alternative', 'representations', 'of', 'sentence', 'structure', 'to', 'successful', 'semantic', 'role', 'labeling', 'varies', 'with', 'the', 'integrity', 'of', 'the', 'parsing', 'and', 'argument', '-', 'identification', 'stages', '.', '.']\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 0, 0, 0]\n"]}]},{"cell_type":"code","source":["# Load raw data test\n","raw_x = []\n","raw_x = []\n","raw_y = []\n","\n","cur_x = []\n","cur_y = []\n","with open('test_500_v2.conll', 'r') as f:\n","  for line in f:\n","    if len(line.strip()) <= 3:\n","      raw_x.append(cur_x)\n","      raw_y.append(cur_y)\n","      cur_x = []\n","      cur_y = []\n","    else:\n","      line = line.strip().split('\\t')\n","      cur_x.append(line[0])\n","      cur_y.append(Entity2ID(line[-1]))  # use this for BIO tags\n","      # cur_y.append(Entity2ID(line[-1].split('-')[-1]))  # use this for span tags\n","\n","print(raw_x[10])\n","print(raw_y[10])\n","\n","# Create actual dataset\n","test = TDMSciDataset(raw_x, raw_y)\n","\n","#print(test[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nb6gP0ISYNFU","executionInfo":{"status":"ok","timestamp":1646967493772,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"1876e7f5-2153-46be-c84f-1f0ad018624a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['We', 'evaluate', 'our', 'approach', 'on', 'three', 'datasets', ':', 'TriviaQA', 'unfiltered', '(', ',', 'a', 'dataset', 'of', 'questions', 'from', 'trivia', 'databases', 'paired', 'with', 'documents', 'found', 'by', 'completing', 'a', 'web', 'search', 'of', 'the', 'questions', ';', 'TriviaQA', 'web', ',', 'a', 'dataset', 'derived', 'from', 'TriviaQA', 'unfiltered', 'by', 'treating', 'each', 'question', 'document', 'pair', 'where', 'the', 'document', 'contains', 'the', 'question', 'answer', 'as', 'an', 'individual', 'training', 'point', ';', 'and', 'SQuAD', '(', ',', 'a', 'collection', 'of', 'Wikipedia', 'articles', 'and', 'crowdsourced', 'questions', '.']\n","[0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iH9xrFbXFG0g","executionInfo":{"status":"ok","timestamp":1646967493772,"user_tz":360,"elapsed":4,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"860c2e5b-52e0-4124-fc8c-7e4c2c069654"},"source":["len(test), len(train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(486, 1522)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"3xW0jveILkHM"},"source":["#train[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["local_hp_train = []\n","local_hp_val = []\n","for x in range(500):\n","  local_hp_train.append(train[x])\n","\n","for x in range(180):\n","  local_hp_val.append(test[x])\n","\n","#print(len(local_hp_train))\n","#print(local_hp_train[1])"],"metadata":{"id":"sFt85TdsW7PO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGYV8ipcJ5fU"},"source":["# Pytorch Model Definition"]},{"cell_type":"code","metadata":{"id":"eFp2ZFd25KwK"},"source":["class NerModel(nn.Module):\n","  def __init__(self, b_embeddings, emb_dims=768, ff_dims=14, out_dims=7):\n","    super(NerModel, self).__init__()\n","    self.sci_embeddings = b_embeddings\n","    self.embd_dropout = nn.Dropout(0.1)\n","    self.ff_dropout = nn.Dropout(0.1)\n","    self.ff = nn.Linear(emb_dims, ff_dims)\n","    self.tanh = nn.Tanh()\n","    self.lstm = nn.LSTM(768, 100, 1, bidirectional=True)\n","    self.lstm_drop = nn.Dropout(0.4)\n","    self.ff = nn.Linear(200, 14)\n","    self.ff_act = nn.ReLU()\n","    self.classifier = nn.Linear(ff_dims, out_dims)\n","  def forward(self, **inputs):\n","    embds = self.sci_embeddings(**inputs)['last_hidden_state']\n","    out = self.embd_dropout(embds)\n","    out, _ = self.lstm(out)\n","    out = self.tanh(out)\n","    out = self.lstm_drop(out)\n","    out = self.ff(out)\n","    out = self.ff_act(out)\n","    out = self.ff_dropout(out)\n","    out = self.classifier(out)\n","    return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LthVVOhx_Sz"},"source":["# Metrics and Configs"]},{"cell_type":"code","metadata":{"id":"VZJjqYf2AKA8"},"source":["def reset_weights(m):\n","  '''\n","    Try resetting model weights to avoid\n","    weight leakage.\n","  '''\n","  for layer in m.children():\n","   if hasattr(layer, 'reset_parameters'):\n","    #print(f'Reset trainable parameters of layer = {layer}')\n","    layer.reset_parameters()\n","\n","import math\n","\n","def load_param():\n","  for n, v in best_run.hyperparameters.items():\n","    if n == 'seed':\n","      setattr(trainer.args, n, math.ceil(v))\n","      print(n, math.ceil(v))\n","    else:\n","      setattr(trainer.args, n, v)\n","      print(n, v)   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjMXqFluyLCW"},"source":["from transformers import TrainingArguments\n","from transformers import Trainer\n","\n","batch_size = 4\n","training_args = TrainingArguments(\n","    \"trained_scibert_ner_model\", # output dir\n","    learning_rate=1e-5, \n","    num_train_epochs=10, \n","    dataloader_drop_last=True,\n","    per_device_eval_batch_size=batch_size, \n","    per_device_train_batch_size=batch_size,\n","    save_steps=len(train) // batch_size,\n","    eval_steps=len(train) // batch_size,\n","    lr_scheduler_type='cosine',\n","    evaluation_strategy='steps'\n","    )\n","#print(training_args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fabdHzMRybBR"},"source":["def collator(batch):\n","  out =  {\n","      'input_ids': torch.stack([(x['input_ids'][0]) for x in batch]),\n","      'attention_mask': torch.stack([x['attention_mask'][0] for x in batch]),\n","      'labels': torch.stack([x['labels'].clone().detach() for x in batch])\n","  }\n","  return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dZluJPsy2eb5"},"source":["import torch.nn.functional as F\n","\n","class FocalLoss(nn.modules.loss._WeightedLoss):\n","    def __init__(self, weight, gamma, reduction='mean'):\n","        super(FocalLoss, self).__init__(weight,reduction=reduction)\n","        self.gamma = gamma\n","        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n","\n","    def forward(self, input, target):\n","        ce_loss = F.cross_entropy(input, target, ignore_index=-100, reduction=self.reduction, weight=self.weight)\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n","        return focal_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLbpwNckLvV6"},"outputs":[],"source":["class MultilabelTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False, num_labels=7):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs\n","\n","        weights = torch.tensor([0.85, 1.55, 1.55, 1.65, 1.65, 1.85, 1.85]).cuda()  # The no-class label has too many examples, we need to weight the loss - this probably needs further tuning \n","        gamma=5\n","        loss_fct = FocalLoss(weight=weights, gamma=gamma)\n","        loss = loss_fct(logits.view(-1, num_labels), labels.long().view(-1))\n","        return (loss, outputs) if return_outputs else loss\n","\n","    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, ignore_keys=None, metric_key_prefix=\"eval\", num_labels=4):\n","      args = self.args\n","      prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n","\n","      self.model.eval()\n","\n","      all_losses = []\n","      all_preds = []\n","      all_labels = []\n","      for step, sample in enumerate(dataloader):\n","        for i in range(0, len(sample['labels'])):\n","          inputs = {}\n","          inputs['input_ids'] = torch.stack([sample['input_ids'][i].cuda()])\n","          inputs['attention_mask'] = torch.stack([sample['attention_mask'][i].cuda()])\n","          inputs['labels'] = torch.stack([sample['labels'][i].cuda()])\n","          labels = inputs['labels'][0].cpu().numpy()\n","          \n","          (loss, logits) = self.compute_loss(self.model, inputs, return_outputs=True)\n","          logits = logits[0].cpu().detach().numpy()\n","          preds = np.argmax(nn.Softmax(dim=-1)(torch.tensor(logits)).numpy(), axis=-1)\n","\n","          all_losses = np.concatenate((all_losses, [loss.detach().cpu().numpy()]), axis=0)\n","\n","          preds = preds[labels != -100]\n","          labels = labels[labels != -100]\n","          all_preds = np.concatenate((all_preds, preds))\n","          all_labels = np.concatenate((all_labels, labels))\n","\n","      metrics = {}\n","      metrics['macro_f1'] = f1_score(all_labels, all_preds, average='macro')\n","      metrics['macro_precision'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n","      metrics['macro_recall'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n","      metrics['micro_f1'] = f1_score(all_labels, all_preds, average='micro')\n","      metrics['micro_precision'] = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n","      metrics['micro_recall'] = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n","\n","      metrics['macro_f1_no_o'] = f1_score(all_labels, all_preds, average='macro', labels=[1, 2, 3, 4, 5, 6])\n","      metrics['macro_precision_no_o'] = precision_score(all_labels, all_preds, average='macro', labels=[1, 2, 3, 4, 5, 6], zero_division=0)\n","      metrics['macro_recall_no_o'] = recall_score(all_labels, all_preds, average='macro', labels=[1, 2, 3, 4, 5, 6], zero_division=0)\n","      metrics['micro_f1_no_o'] = f1_score(all_labels, all_preds, average='micro', labels=[1, 2, 3, 4, 5, 6])\n","      metrics['micro_precision_no_o'] = precision_score(all_labels, all_preds, average='micro', labels=[1, 2, 3, 4, 5, 6], zero_division=0)\n","      metrics['micro_recall_no_o'] = recall_score(all_labels, all_preds, average='micro', labels=[1, 2, 3, 4, 5, 6], zero_division=0)\n","\n","      for key in list(metrics.keys()):\n","        if not key.startswith(metric_key_prefix):\n","          metrics[metric_key_prefix + '_' + key] = metrics.pop(key)\n","      \n","      metrics[metric_key_prefix + '_loss'] = all_losses.mean().item()\n","\n","      return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=len(dataloader))"]},{"cell_type":"code","metadata":{"id":"infNapI_OmFr"},"source":["def my_hp_space_ray(trial):\n","    from ray import tune\n","\n","    return {\n","        \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n","        \"num_train_epochs\": tune.choice(range(8, 20)),\n","        \"weight_decay\": tune.uniform(0.0, 0.5),\n","        \"per_device_train_batch_size\": tune.choice([4, 8, 16]),  #<16 definetly not working\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Nh8Lcpvb_F4"},"source":["# Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"id":"w6AqbEPyPK7O"},"source":["def model_init():\n","    x = NerModel(BertEmbModel)\n","    x.sci_embeddings.requires_grad = False\n","    return x\n","\n","trainer = MultilabelTrainer(model_init=model_init,\n","                            args=training_args,\n","                            train_dataset=local_hp_train,\n","                            eval_dataset=local_hp_val,\n","                            data_collator=collator  # defines how to merge data into batches, using the collator function above\n","                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGyHLY640NRz","colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1Up6eB_pWTdqtGZsMWbn101KWGxBJqsHm"},"executionInfo":{"status":"ok","timestamp":1645133082435,"user_tz":360,"elapsed":3613486,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"e67eb006-1c93-49d8-cbba-6dda9c924f6c"},"source":["from ray.tune.suggest.hyperopt import HyperOptSearch\n","from ray.tune.schedulers import ASHAScheduler\n","\n","best_run = trainer.hyperparameter_search(backend=\"ray\", \n","                                         resources_per_trial={\"gpu\": 1, \"cpu\": 0},\n","                                         n_trials=16, \n","                                         direction=\"maximize\", \n","                                         hp_space=my_hp_space_ray,\n","                                         search_alg=HyperOptSearch(metric='eval_micro_recall_no_o', mode=\"max\"),  #'eval_*f1_micro'\n","                                         scheduler=ASHAScheduler(metric='eval_micro_recall_no_o', mode=\"max\")\n","                                         )"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zH0H8agAbzZi","executionInfo":{"status":"ok","timestamp":1645133082436,"user_tz":360,"elapsed":6,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"476bf036-5634-42e1-faa7-01c6f0b65b80"},"source":["best_run"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='c691df3e', objective=5.921519281375328, hyperparameters={'learning_rate': 2.0067205312195233e-05, 'num_train_epochs': 18, 'weight_decay': 0.24799733516738603, 'per_device_train_batch_size': 4})"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"kv6pGzbGpv2D"},"source":["# Cossvalidation"]},{"cell_type":"code","metadata":{"id":"LhipElJSzRUF"},"source":["#import tensorflow as tf\n","from torch.utils.data import DataLoader, ConcatDataset\n","from sklearn.model_selection import KFold\n","from torch import nn\n","from transformers import Trainer\n","\n","#print('GPU detected:', tf.config.list_physical_devices('GPU'))\n","k_folds = 5\n","kfold = KFold(n_splits=k_folds, shuffle=True)\n","results = np.zeros(5)\n","resultss = np.zeros(5)\n","dataset = ConcatDataset([train, test])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zQyLOxyMjZq2","executionInfo":{"status":"ok","timestamp":1646808841879,"user_tz":360,"elapsed":6005893,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"bd4e2e99-89c0-4166-fc02-38797dc18997"},"source":["for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n","    \n","    # Print\n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","    \n","    # Sample elements randomly from a given list of ids, no replacement.\n","    #train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n","    #test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n","    \n","    local_train = []\n","    #i = 0\n","    for idx in train_ids:\n","      #if i <= 1920:\n","        local_train.append(dataset[idx])\n","      #i += 1\n","    \n","    local_test = []\n","    #i = 0\n","    for idx in test_ids:\n","     # if i <= 480:\n","        local_test.append(dataset[idx])\n","      #i += 1\n","\n","    training_args = TrainingArguments(\"trained_scibert_ner_model\", # output dir\n","                                      learning_rate=2.0067205312195233e-05, \n","                                      weight_decay=0.24799733516738603,\n","                                      num_train_epochs=18, \n","                                      dataloader_drop_last=True,\n","                                      per_device_eval_batch_size=4, \n","                                      per_device_train_batch_size=4,\n","                                      logging_steps=50,\n","                                      save_steps=len(local_train) // batch_size,\n","                                      lr_scheduler_type='cosine',\n","                                      evaluation_strategy='steps',\n","                                      eval_steps=len(local_train) // batch_size\n","                                      )\n","    #learning_rate 2.0067205312195233e-05\n","    #num_train_epochs 18\n","    #weight_decay 0.24799733516738603\n","    #per_device_train_batch_size 4\n","\n","    # Init the neural network\n","    ner_model = NerModel(BertEmbModel).to('cuda')  # make sure we move the model to the GPU for training\n","    \n","    trainer = MultilabelTrainer(model=ner_model,\n","                                args=training_args,\n","                                train_dataset=local_train,\n","                                eval_dataset=local_test,\n","                                data_collator=collator  # defines how to merge data into batches, using the collator function above\n","                                )\n","\n","    #Loading Best parameters\n","    #load_param()\n","\n","    #Training\n","    trainer.train()\n","          \n","    # Process is complete.\n","    print('Training process has finished. Saving trained model.')\n","\n","    # Print about testing\n","    print('Starting testing')\n","    \n","    # Saving the model\n","    save_path = f'./model-fold-{fold}.pth'\n","    torch.save(ner_model.state_dict(), save_path)\n","\n","    # Evaluationfor this fold\n","    #correct, total = 0, 0\n","    with torch.no_grad():\n","      result = trainer.evaluate(local_test)\n","      print(result)\n","\n","      # Print accuracy\n","      print('Accuracy for fold ', fold, ': ', result['eval_micro_f1_no_o'], ' -- ', result['eval_micro_f1'])\n","      print('--------------------------------')\n","      results[fold] = result['eval_micro_f1_no_o']\n","      resultss[fold] = result['eval_micro_f1']\n","      del result\n","    \n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FOLD 0\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1606\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7218\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7218' max='7218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7218/7218 19:47, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.055800</td>\n","      <td>0.094043</td>\n","      <td>0.225512</td>\n","      <td>0.204765</td>\n","      <td>0.283355</td>\n","      <td>0.865782</td>\n","      <td>0.865782</td>\n","      <td>0.865782</td>\n","      <td>0.104192</td>\n","      <td>0.082090</td>\n","      <td>0.169515</td>\n","      <td>0.280429</td>\n","      <td>0.308009</td>\n","      <td>0.257382</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.021000</td>\n","      <td>0.063082</td>\n","      <td>0.412475</td>\n","      <td>0.432455</td>\n","      <td>0.456696</td>\n","      <td>0.889804</td>\n","      <td>0.889804</td>\n","      <td>0.889804</td>\n","      <td>0.321172</td>\n","      <td>0.344004</td>\n","      <td>0.373238</td>\n","      <td>0.472323</td>\n","      <td>0.464133</td>\n","      <td>0.480807</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.013600</td>\n","      <td>0.053966</td>\n","      <td>0.459713</td>\n","      <td>0.499144</td>\n","      <td>0.495224</td>\n","      <td>0.887640</td>\n","      <td>0.887640</td>\n","      <td>0.887640</td>\n","      <td>0.376487</td>\n","      <td>0.420387</td>\n","      <td>0.419966</td>\n","      <td>0.491893</td>\n","      <td>0.458884</td>\n","      <td>0.530020</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.008200</td>\n","      <td>0.040598</td>\n","      <td>0.574147</td>\n","      <td>0.537785</td>\n","      <td>0.642506</td>\n","      <td>0.901816</td>\n","      <td>0.901816</td>\n","      <td>0.901816</td>\n","      <td>0.510323</td>\n","      <td>0.464511</td>\n","      <td>0.593326</td>\n","      <td>0.610296</td>\n","      <td>0.549941</td>\n","      <td>0.685531</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.005000</td>\n","      <td>0.040516</td>\n","      <td>0.630098</td>\n","      <td>0.685535</td>\n","      <td>0.681431</td>\n","      <td>0.914665</td>\n","      <td>0.914665</td>\n","      <td>0.914665</td>\n","      <td>0.575049</td>\n","      <td>0.637449</td>\n","      <td>0.637152</td>\n","      <td>0.663034</td>\n","      <td>0.615514</td>\n","      <td>0.718504</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.002600</td>\n","      <td>0.046042</td>\n","      <td>0.716589</td>\n","      <td>0.716774</td>\n","      <td>0.739156</td>\n","      <td>0.920461</td>\n","      <td>0.920461</td>\n","      <td>0.920461</td>\n","      <td>0.675780</td>\n","      <td>0.674528</td>\n","      <td>0.703549</td>\n","      <td>0.687515</td>\n","      <td>0.653795</td>\n","      <td>0.724902</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.002500</td>\n","      <td>0.045439</td>\n","      <td>0.741102</td>\n","      <td>0.744778</td>\n","      <td>0.743408</td>\n","      <td>0.928701</td>\n","      <td>0.928701</td>\n","      <td>0.928701</td>\n","      <td>0.703833</td>\n","      <td>0.707746</td>\n","      <td>0.706895</td>\n","      <td>0.714390</td>\n","      <td>0.704643</td>\n","      <td>0.724409</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.001500</td>\n","      <td>0.041846</td>\n","      <td>0.744103</td>\n","      <td>0.738045</td>\n","      <td>0.765022</td>\n","      <td>0.923883</td>\n","      <td>0.923883</td>\n","      <td>0.923883</td>\n","      <td>0.707845</td>\n","      <td>0.698766</td>\n","      <td>0.734214</td>\n","      <td>0.713860</td>\n","      <td>0.667810</td>\n","      <td>0.766732</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.000800</td>\n","      <td>0.042301</td>\n","      <td>0.752162</td>\n","      <td>0.737203</td>\n","      <td>0.774768</td>\n","      <td>0.924791</td>\n","      <td>0.924791</td>\n","      <td>0.924791</td>\n","      <td>0.717263</td>\n","      <td>0.698113</td>\n","      <td>0.745299</td>\n","      <td>0.717759</td>\n","      <td>0.677744</td>\n","      <td>0.762795</td>\n","    </tr>\n","    <tr>\n","      <td>4010</td>\n","      <td>0.001400</td>\n","      <td>0.053005</td>\n","      <td>0.742845</td>\n","      <td>0.756787</td>\n","      <td>0.743160</td>\n","      <td>0.929050</td>\n","      <td>0.929050</td>\n","      <td>0.929050</td>\n","      <td>0.705860</td>\n","      <td>0.721693</td>\n","      <td>0.706660</td>\n","      <td>0.717191</td>\n","      <td>0.705910</td>\n","      <td>0.728839</td>\n","    </tr>\n","    <tr>\n","      <td>4411</td>\n","      <td>0.000300</td>\n","      <td>0.040520</td>\n","      <td>0.750858</td>\n","      <td>0.720961</td>\n","      <td>0.793670</td>\n","      <td>0.920950</td>\n","      <td>0.920950</td>\n","      <td>0.920950</td>\n","      <td>0.716070</td>\n","      <td>0.678157</td>\n","      <td>0.768939</td>\n","      <td>0.714381</td>\n","      <td>0.649738</td>\n","      <td>0.793307</td>\n","    </tr>\n","    <tr>\n","      <td>4812</td>\n","      <td>0.000500</td>\n","      <td>0.046022</td>\n","      <td>0.747039</td>\n","      <td>0.724746</td>\n","      <td>0.777096</td>\n","      <td>0.924372</td>\n","      <td>0.924372</td>\n","      <td>0.924372</td>\n","      <td>0.711214</td>\n","      <td>0.683589</td>\n","      <td>0.747867</td>\n","      <td>0.711864</td>\n","      <td>0.673846</td>\n","      <td>0.754429</td>\n","    </tr>\n","    <tr>\n","      <td>5213</td>\n","      <td>0.000600</td>\n","      <td>0.059866</td>\n","      <td>0.750683</td>\n","      <td>0.772643</td>\n","      <td>0.739674</td>\n","      <td>0.931075</td>\n","      <td>0.931075</td>\n","      <td>0.931075</td>\n","      <td>0.714886</td>\n","      <td>0.740942</td>\n","      <td>0.701604</td>\n","      <td>0.719039</td>\n","      <td>0.731298</td>\n","      <td>0.707185</td>\n","    </tr>\n","    <tr>\n","      <td>5614</td>\n","      <td>0.000700</td>\n","      <td>0.065438</td>\n","      <td>0.749210</td>\n","      <td>0.767476</td>\n","      <td>0.742484</td>\n","      <td>0.930936</td>\n","      <td>0.930936</td>\n","      <td>0.930936</td>\n","      <td>0.713119</td>\n","      <td>0.734872</td>\n","      <td>0.704827</td>\n","      <td>0.716216</td>\n","      <td>0.728615</td>\n","      <td>0.704232</td>\n","    </tr>\n","    <tr>\n","      <td>6015</td>\n","      <td>0.000400</td>\n","      <td>0.063385</td>\n","      <td>0.752054</td>\n","      <td>0.763647</td>\n","      <td>0.749314</td>\n","      <td>0.930866</td>\n","      <td>0.930866</td>\n","      <td>0.930866</td>\n","      <td>0.716457</td>\n","      <td>0.730256</td>\n","      <td>0.712985</td>\n","      <td>0.718051</td>\n","      <td>0.725628</td>\n","      <td>0.710630</td>\n","    </tr>\n","    <tr>\n","      <td>6416</td>\n","      <td>0.000400</td>\n","      <td>0.060028</td>\n","      <td>0.755146</td>\n","      <td>0.761415</td>\n","      <td>0.756162</td>\n","      <td>0.931704</td>\n","      <td>0.931704</td>\n","      <td>0.931704</td>\n","      <td>0.719999</td>\n","      <td>0.727345</td>\n","      <td>0.721151</td>\n","      <td>0.723824</td>\n","      <td>0.724716</td>\n","      <td>0.722933</td>\n","    </tr>\n","    <tr>\n","      <td>6817</td>\n","      <td>0.000400</td>\n","      <td>0.058676</td>\n","      <td>0.756409</td>\n","      <td>0.760836</td>\n","      <td>0.759190</td>\n","      <td>0.931075</td>\n","      <td>0.931075</td>\n","      <td>0.931075</td>\n","      <td>0.721547</td>\n","      <td>0.726548</td>\n","      <td>0.724955</td>\n","      <td>0.723893</td>\n","      <td>0.719494</td>\n","      <td>0.728346</td>\n","    </tr>\n","    <tr>\n","      <td>7218</td>\n","      <td>0.000400</td>\n","      <td>0.058523</td>\n","      <td>0.756409</td>\n","      <td>0.760836</td>\n","      <td>0.759190</td>\n","      <td>0.931075</td>\n","      <td>0.931075</td>\n","      <td>0.931075</td>\n","      <td>0.721547</td>\n","      <td>0.726548</td>\n","      <td>0.724955</td>\n","      <td>0.723893</td>\n","      <td>0.719494</td>\n","      <td>0.728346</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4010\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4411\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4812\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5213\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5614\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6015\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6416\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6817\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7218\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.7564090883256013, 'eval_macro_precision': 0.7608361258145833, 'eval_macro_recall': 0.7591901593098086, 'eval_micro_f1': 0.9310754189944134, 'eval_micro_precision': 0.9310754189944134, 'eval_micro_recall': 0.9310754189944134, 'eval_macro_f1_no_o': 0.7215469613754554, 'eval_macro_precision_no_o': 0.7265477979294035, 'eval_macro_recall_no_o': 0.7249552509656101, 'eval_micro_f1_no_o': 0.7238933724627049, 'eval_micro_precision_no_o': 0.7194944093339816, 'eval_micro_recall_no_o': 0.7283464566929134, 'eval_loss': 0.058522603419891614, 'eval_runtime': 5.062, 'eval_samples_per_second': 19.755, 'eval_steps_per_second': 4.939, 'epoch': 18.0}\n","Accuracy for fold  0 :  0.7238933724627049  --  0.9310754189944134\n","--------------------------------\n","FOLD 1\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1606\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7218\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7218' max='7218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7218/7218 19:47, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.061100</td>\n","      <td>0.053419</td>\n","      <td>0.949893</td>\n","      <td>0.928093</td>\n","      <td>0.973550</td>\n","      <td>0.984428</td>\n","      <td>0.984428</td>\n","      <td>0.984428</td>\n","      <td>0.942782</td>\n","      <td>0.916305</td>\n","      <td>0.971412</td>\n","      <td>0.939548</td>\n","      <td>0.908118</td>\n","      <td>0.973231</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.024000</td>\n","      <td>0.020677</td>\n","      <td>0.923803</td>\n","      <td>0.886415</td>\n","      <td>0.966400</td>\n","      <td>0.975651</td>\n","      <td>0.975651</td>\n","      <td>0.975651</td>\n","      <td>0.913006</td>\n","      <td>0.867767</td>\n","      <td>0.964290</td>\n","      <td>0.905797</td>\n","      <td>0.860585</td>\n","      <td>0.956023</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.008200</td>\n","      <td>0.011177</td>\n","      <td>0.923860</td>\n","      <td>0.888792</td>\n","      <td>0.963421</td>\n","      <td>0.976713</td>\n","      <td>0.976713</td>\n","      <td>0.976713</td>\n","      <td>0.913044</td>\n","      <td>0.870525</td>\n","      <td>0.960773</td>\n","      <td>0.911645</td>\n","      <td>0.866494</td>\n","      <td>0.961759</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.003700</td>\n","      <td>0.008069</td>\n","      <td>0.926918</td>\n","      <td>0.890464</td>\n","      <td>0.967517</td>\n","      <td>0.976784</td>\n","      <td>0.976784</td>\n","      <td>0.976784</td>\n","      <td>0.916625</td>\n","      <td>0.872491</td>\n","      <td>0.965565</td>\n","      <td>0.912551</td>\n","      <td>0.867356</td>\n","      <td>0.962715</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.002300</td>\n","      <td>0.005649</td>\n","      <td>0.938528</td>\n","      <td>0.911077</td>\n","      <td>0.968372</td>\n","      <td>0.980677</td>\n","      <td>0.980677</td>\n","      <td>0.980677</td>\n","      <td>0.929830</td>\n","      <td>0.896706</td>\n","      <td>0.965732</td>\n","      <td>0.925380</td>\n","      <td>0.892889</td>\n","      <td>0.960325</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.002100</td>\n","      <td>0.007603</td>\n","      <td>0.902681</td>\n","      <td>0.852407</td>\n","      <td>0.965657</td>\n","      <td>0.961141</td>\n","      <td>0.961141</td>\n","      <td>0.961141</td>\n","      <td>0.889962</td>\n","      <td>0.828067</td>\n","      <td>0.966553</td>\n","      <td>0.870370</td>\n","      <td>0.791928</td>\n","      <td>0.966061</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.001800</td>\n","      <td>0.005439</td>\n","      <td>0.921101</td>\n","      <td>0.879976</td>\n","      <td>0.968403</td>\n","      <td>0.971687</td>\n","      <td>0.971687</td>\n","      <td>0.971687</td>\n","      <td>0.910398</td>\n","      <td>0.860355</td>\n","      <td>0.967596</td>\n","      <td>0.899308</td>\n","      <td>0.843737</td>\n","      <td>0.962715</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.002000</td>\n","      <td>0.003935</td>\n","      <td>0.938534</td>\n","      <td>0.915562</td>\n","      <td>0.963353</td>\n","      <td>0.981597</td>\n","      <td>0.981597</td>\n","      <td>0.981597</td>\n","      <td>0.929717</td>\n","      <td>0.902021</td>\n","      <td>0.959557</td>\n","      <td>0.926965</td>\n","      <td>0.900045</td>\n","      <td>0.955545</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.001100</td>\n","      <td>0.003844</td>\n","      <td>0.939100</td>\n","      <td>0.913581</td>\n","      <td>0.966721</td>\n","      <td>0.981172</td>\n","      <td>0.981172</td>\n","      <td>0.981172</td>\n","      <td>0.930442</td>\n","      <td>0.899612</td>\n","      <td>0.963708</td>\n","      <td>0.926661</td>\n","      <td>0.895276</td>\n","      <td>0.960325</td>\n","    </tr>\n","    <tr>\n","      <td>4010</td>\n","      <td>0.000600</td>\n","      <td>0.003570</td>\n","      <td>0.935014</td>\n","      <td>0.905264</td>\n","      <td>0.967745</td>\n","      <td>0.980252</td>\n","      <td>0.980252</td>\n","      <td>0.980252</td>\n","      <td>0.925759</td>\n","      <td>0.889854</td>\n","      <td>0.965125</td>\n","      <td>0.923783</td>\n","      <td>0.888693</td>\n","      <td>0.961759</td>\n","    </tr>\n","    <tr>\n","      <td>4411</td>\n","      <td>0.000700</td>\n","      <td>0.003383</td>\n","      <td>0.932137</td>\n","      <td>0.901034</td>\n","      <td>0.966499</td>\n","      <td>0.979473</td>\n","      <td>0.979473</td>\n","      <td>0.979473</td>\n","      <td>0.922472</td>\n","      <td>0.884933</td>\n","      <td>0.963796</td>\n","      <td>0.921173</td>\n","      <td>0.884683</td>\n","      <td>0.960803</td>\n","    </tr>\n","    <tr>\n","      <td>4812</td>\n","      <td>0.000500</td>\n","      <td>0.003150</td>\n","      <td>0.938053</td>\n","      <td>0.910581</td>\n","      <td>0.968038</td>\n","      <td>0.981526</td>\n","      <td>0.981526</td>\n","      <td>0.981526</td>\n","      <td>0.929178</td>\n","      <td>0.896057</td>\n","      <td>0.965218</td>\n","      <td>0.927616</td>\n","      <td>0.895815</td>\n","      <td>0.961759</td>\n","    </tr>\n","    <tr>\n","      <td>5213</td>\n","      <td>0.000400</td>\n","      <td>0.003059</td>\n","      <td>0.940436</td>\n","      <td>0.913330</td>\n","      <td>0.969923</td>\n","      <td>0.981880</td>\n","      <td>0.981880</td>\n","      <td>0.981880</td>\n","      <td>0.931958</td>\n","      <td>0.899278</td>\n","      <td>0.967403</td>\n","      <td>0.929889</td>\n","      <td>0.898396</td>\n","      <td>0.963671</td>\n","    </tr>\n","    <tr>\n","      <td>5614</td>\n","      <td>0.000500</td>\n","      <td>0.003022</td>\n","      <td>0.942593</td>\n","      <td>0.918180</td>\n","      <td>0.968919</td>\n","      <td>0.982800</td>\n","      <td>0.982800</td>\n","      <td>0.982800</td>\n","      <td>0.934390</td>\n","      <td>0.904992</td>\n","      <td>0.966010</td>\n","      <td>0.932808</td>\n","      <td>0.905126</td>\n","      <td>0.962237</td>\n","    </tr>\n","    <tr>\n","      <td>6015</td>\n","      <td>0.000400</td>\n","      <td>0.003009</td>\n","      <td>0.941085</td>\n","      <td>0.914474</td>\n","      <td>0.969995</td>\n","      <td>0.982305</td>\n","      <td>0.982305</td>\n","      <td>0.982305</td>\n","      <td>0.932680</td>\n","      <td>0.900627</td>\n","      <td>0.967403</td>\n","      <td>0.931393</td>\n","      <td>0.901207</td>\n","      <td>0.963671</td>\n","    </tr>\n","    <tr>\n","      <td>6416</td>\n","      <td>0.001000</td>\n","      <td>0.002961</td>\n","      <td>0.940424</td>\n","      <td>0.913411</td>\n","      <td>0.969803</td>\n","      <td>0.982163</td>\n","      <td>0.982163</td>\n","      <td>0.982163</td>\n","      <td>0.931916</td>\n","      <td>0.899372</td>\n","      <td>0.967207</td>\n","      <td>0.930748</td>\n","      <td>0.900000</td>\n","      <td>0.963671</td>\n","    </tr>\n","    <tr>\n","      <td>6817</td>\n","      <td>0.000300</td>\n","      <td>0.002949</td>\n","      <td>0.942368</td>\n","      <td>0.914990</td>\n","      <td>0.972121</td>\n","      <td>0.982375</td>\n","      <td>0.982375</td>\n","      <td>0.982375</td>\n","      <td>0.934191</td>\n","      <td>0.901214</td>\n","      <td>0.969926</td>\n","      <td>0.932379</td>\n","      <td>0.901383</td>\n","      <td>0.965583</td>\n","    </tr>\n","    <tr>\n","      <td>7218</td>\n","      <td>0.000400</td>\n","      <td>0.002948</td>\n","      <td>0.940584</td>\n","      <td>0.913421</td>\n","      <td>0.970149</td>\n","      <td>0.982163</td>\n","      <td>0.982163</td>\n","      <td>0.982163</td>\n","      <td>0.932109</td>\n","      <td>0.899384</td>\n","      <td>0.967624</td>\n","      <td>0.930995</td>\n","      <td>0.900045</td>\n","      <td>0.964149</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4010\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4411\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4812\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5213\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5614\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6015\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6416\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6817\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7218\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9405838059340584, 'eval_macro_precision': 0.9134209938843906, 'eval_macro_recall': 0.9701487060237126, 'eval_micro_f1': 0.9821630804077011, 'eval_micro_precision': 0.9821630804077011, 'eval_micro_recall': 0.9821630804077011, 'eval_macro_f1_no_o': 0.9321093001541291, 'eval_macro_precision_no_o': 0.8993837452697101, 'eval_macro_recall_no_o': 0.9676244707531548, 'eval_micro_f1_no_o': 0.9309946918993769, 'eval_micro_precision_no_o': 0.9000446229361893, 'eval_micro_recall_no_o': 0.9641491395793499, 'eval_loss': 0.0029478267878221233, 'eval_runtime': 5.0519, 'eval_samples_per_second': 19.794, 'eval_steps_per_second': 4.949, 'epoch': 18.0}\n","Accuracy for fold  1 :  0.9309946918993769  --  0.9821630804077011\n","--------------------------------\n","FOLD 2\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1606\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7218\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7218' max='7218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7218/7218 19:46, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.033600</td>\n","      <td>0.034118</td>\n","      <td>0.959738</td>\n","      <td>0.945070</td>\n","      <td>0.975701</td>\n","      <td>0.989281</td>\n","      <td>0.989281</td>\n","      <td>0.989281</td>\n","      <td>0.953905</td>\n","      <td>0.936209</td>\n","      <td>0.973108</td>\n","      <td>0.951807</td>\n","      <td>0.929412</td>\n","      <td>0.975309</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.012100</td>\n","      <td>0.013293</td>\n","      <td>0.955076</td>\n","      <td>0.936907</td>\n","      <td>0.975151</td>\n","      <td>0.986618</td>\n","      <td>0.986618</td>\n","      <td>0.986618</td>\n","      <td>0.948729</td>\n","      <td>0.926673</td>\n","      <td>0.972998</td>\n","      <td>0.942243</td>\n","      <td>0.910410</td>\n","      <td>0.976382</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.006800</td>\n","      <td>0.006689</td>\n","      <td>0.959892</td>\n","      <td>0.943187</td>\n","      <td>0.978154</td>\n","      <td>0.988682</td>\n","      <td>0.988682</td>\n","      <td>0.988682</td>\n","      <td>0.954135</td>\n","      <td>0.933999</td>\n","      <td>0.976084</td>\n","      <td>0.949321</td>\n","      <td>0.924682</td>\n","      <td>0.975309</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.003100</td>\n","      <td>0.004538</td>\n","      <td>0.962902</td>\n","      <td>0.950514</td>\n","      <td>0.976591</td>\n","      <td>0.990146</td>\n","      <td>0.990146</td>\n","      <td>0.990146</td>\n","      <td>0.957507</td>\n","      <td>0.942546</td>\n","      <td>0.973981</td>\n","      <td>0.954808</td>\n","      <td>0.935152</td>\n","      <td>0.975309</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.002200</td>\n","      <td>0.003614</td>\n","      <td>0.961232</td>\n","      <td>0.942391</td>\n","      <td>0.981909</td>\n","      <td>0.988415</td>\n","      <td>0.988415</td>\n","      <td>0.988415</td>\n","      <td>0.955764</td>\n","      <td>0.933045</td>\n","      <td>0.980616</td>\n","      <td>0.950026</td>\n","      <td>0.922183</td>\n","      <td>0.979603</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.001400</td>\n","      <td>0.002737</td>\n","      <td>0.967274</td>\n","      <td>0.954560</td>\n","      <td>0.981057</td>\n","      <td>0.990812</td>\n","      <td>0.990812</td>\n","      <td>0.990812</td>\n","      <td>0.962588</td>\n","      <td>0.947267</td>\n","      <td>0.979154</td>\n","      <td>0.959243</td>\n","      <td>0.940206</td>\n","      <td>0.979066</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.001900</td>\n","      <td>0.002480</td>\n","      <td>0.961555</td>\n","      <td>0.947951</td>\n","      <td>0.976210</td>\n","      <td>0.990479</td>\n","      <td>0.990479</td>\n","      <td>0.990479</td>\n","      <td>0.955865</td>\n","      <td>0.939531</td>\n","      <td>0.973423</td>\n","      <td>0.954462</td>\n","      <td>0.936467</td>\n","      <td>0.973162</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.000900</td>\n","      <td>0.002087</td>\n","      <td>0.960698</td>\n","      <td>0.940253</td>\n","      <td>0.982599</td>\n","      <td>0.989348</td>\n","      <td>0.989348</td>\n","      <td>0.989348</td>\n","      <td>0.955019</td>\n","      <td>0.930513</td>\n","      <td>0.981219</td>\n","      <td>0.952207</td>\n","      <td>0.927263</td>\n","      <td>0.978529</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.000600</td>\n","      <td>0.001842</td>\n","      <td>0.967679</td>\n","      <td>0.953694</td>\n","      <td>0.982549</td>\n","      <td>0.991545</td>\n","      <td>0.991545</td>\n","      <td>0.991545</td>\n","      <td>0.962972</td>\n","      <td>0.946218</td>\n","      <td>0.980756</td>\n","      <td>0.961265</td>\n","      <td>0.944099</td>\n","      <td>0.979066</td>\n","    </tr>\n","    <tr>\n","      <td>4010</td>\n","      <td>0.000700</td>\n","      <td>0.001762</td>\n","      <td>0.964239</td>\n","      <td>0.946436</td>\n","      <td>0.983278</td>\n","      <td>0.989680</td>\n","      <td>0.989680</td>\n","      <td>0.989680</td>\n","      <td>0.959149</td>\n","      <td>0.937738</td>\n","      <td>0.981998</td>\n","      <td>0.954795</td>\n","      <td>0.930244</td>\n","      <td>0.980676</td>\n","    </tr>\n","    <tr>\n","      <td>4411</td>\n","      <td>0.000600</td>\n","      <td>0.001705</td>\n","      <td>0.962433</td>\n","      <td>0.941788</td>\n","      <td>0.984581</td>\n","      <td>0.990013</td>\n","      <td>0.990013</td>\n","      <td>0.990013</td>\n","      <td>0.957017</td>\n","      <td>0.932315</td>\n","      <td>0.983468</td>\n","      <td>0.956317</td>\n","      <td>0.932653</td>\n","      <td>0.981213</td>\n","    </tr>\n","    <tr>\n","      <td>4812</td>\n","      <td>0.000600</td>\n","      <td>0.001635</td>\n","      <td>0.966208</td>\n","      <td>0.949166</td>\n","      <td>0.984321</td>\n","      <td>0.990546</td>\n","      <td>0.990546</td>\n","      <td>0.990546</td>\n","      <td>0.961376</td>\n","      <td>0.940948</td>\n","      <td>0.983051</td>\n","      <td>0.958552</td>\n","      <td>0.937404</td>\n","      <td>0.980676</td>\n","    </tr>\n","    <tr>\n","      <td>5213</td>\n","      <td>0.000300</td>\n","      <td>0.001645</td>\n","      <td>0.968839</td>\n","      <td>0.954764</td>\n","      <td>0.983810</td>\n","      <td>0.991212</td>\n","      <td>0.991212</td>\n","      <td>0.991212</td>\n","      <td>0.964375</td>\n","      <td>0.947504</td>\n","      <td>0.982290</td>\n","      <td>0.960758</td>\n","      <td>0.943123</td>\n","      <td>0.979066</td>\n","    </tr>\n","    <tr>\n","      <td>5614</td>\n","      <td>0.000500</td>\n","      <td>0.001614</td>\n","      <td>0.966011</td>\n","      <td>0.950576</td>\n","      <td>0.982379</td>\n","      <td>0.990546</td>\n","      <td>0.990546</td>\n","      <td>0.990546</td>\n","      <td>0.961115</td>\n","      <td>0.942606</td>\n","      <td>0.980709</td>\n","      <td>0.957162</td>\n","      <td>0.937693</td>\n","      <td>0.977456</td>\n","    </tr>\n","    <tr>\n","      <td>6015</td>\n","      <td>0.000400</td>\n","      <td>0.001699</td>\n","      <td>0.968144</td>\n","      <td>0.955084</td>\n","      <td>0.982058</td>\n","      <td>0.991079</td>\n","      <td>0.991079</td>\n","      <td>0.991079</td>\n","      <td>0.963571</td>\n","      <td>0.947891</td>\n","      <td>0.980246</td>\n","      <td>0.959958</td>\n","      <td>0.942576</td>\n","      <td>0.977992</td>\n","    </tr>\n","    <tr>\n","      <td>6416</td>\n","      <td>0.000800</td>\n","      <td>0.001641</td>\n","      <td>0.968486</td>\n","      <td>0.955197</td>\n","      <td>0.982729</td>\n","      <td>0.990945</td>\n","      <td>0.990945</td>\n","      <td>0.990945</td>\n","      <td>0.963989</td>\n","      <td>0.948010</td>\n","      <td>0.981079</td>\n","      <td>0.959747</td>\n","      <td>0.941176</td>\n","      <td>0.979066</td>\n","    </tr>\n","    <tr>\n","      <td>6817</td>\n","      <td>0.000300</td>\n","      <td>0.001661</td>\n","      <td>0.967913</td>\n","      <td>0.953853</td>\n","      <td>0.983021</td>\n","      <td>0.990613</td>\n","      <td>0.990613</td>\n","      <td>0.990613</td>\n","      <td>0.963353</td>\n","      <td>0.946430</td>\n","      <td>0.981496</td>\n","      <td>0.958508</td>\n","      <td>0.938303</td>\n","      <td>0.979603</td>\n","    </tr>\n","    <tr>\n","      <td>7218</td>\n","      <td>0.000500</td>\n","      <td>0.001662</td>\n","      <td>0.968187</td>\n","      <td>0.954351</td>\n","      <td>0.983043</td>\n","      <td>0.990746</td>\n","      <td>0.990746</td>\n","      <td>0.990746</td>\n","      <td>0.963660</td>\n","      <td>0.947010</td>\n","      <td>0.981496</td>\n","      <td>0.959012</td>\n","      <td>0.939269</td>\n","      <td>0.979603</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4010\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4411\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4812\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5213\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5614\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6015\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6416\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6817\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7218\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9681874143037837, 'eval_macro_precision': 0.9543509420986663, 'eval_macro_recall': 0.9830427194666037, 'eval_micro_f1': 0.990745672436751, 'eval_micro_precision': 0.990745672436751, 'eval_micro_recall': 0.990745672436751, 'eval_macro_f1_no_o': 0.9636603922893844, 'eval_macro_precision_no_o': 0.9470104112662158, 'eval_macro_recall_no_o': 0.9814959263782871, 'eval_micro_f1_no_o': 0.9590120861797161, 'eval_micro_precision_no_o': 0.939269171384457, 'eval_micro_recall_no_o': 0.9796027911969941, 'eval_loss': 0.0016622885283004507, 'eval_runtime': 5.0645, 'eval_samples_per_second': 19.745, 'eval_steps_per_second': 4.936, 'epoch': 18.0}\n","Accuracy for fold  2 :  0.9590120861797161  --  0.990745672436751\n","--------------------------------\n","FOLD 3\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1607\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7218\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7218' max='7218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7218/7218 19:46, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.030300</td>\n","      <td>0.031717</td>\n","      <td>0.979648</td>\n","      <td>0.971384</td>\n","      <td>0.988246</td>\n","      <td>0.995258</td>\n","      <td>0.995258</td>\n","      <td>0.995258</td>\n","      <td>0.976640</td>\n","      <td>0.966787</td>\n","      <td>0.986883</td>\n","      <td>0.979767</td>\n","      <td>0.971902</td>\n","      <td>0.987761</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.009300</td>\n","      <td>0.012394</td>\n","      <td>0.979481</td>\n","      <td>0.968663</td>\n","      <td>0.990897</td>\n","      <td>0.994433</td>\n","      <td>0.994433</td>\n","      <td>0.994433</td>\n","      <td>0.976558</td>\n","      <td>0.963613</td>\n","      <td>0.990201</td>\n","      <td>0.978091</td>\n","      <td>0.966169</td>\n","      <td>0.990311</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.006000</td>\n","      <td>0.006351</td>\n","      <td>0.978863</td>\n","      <td>0.970464</td>\n","      <td>0.987722</td>\n","      <td>0.994640</td>\n","      <td>0.994640</td>\n","      <td>0.994640</td>\n","      <td>0.975764</td>\n","      <td>0.965754</td>\n","      <td>0.986311</td>\n","      <td>0.976732</td>\n","      <td>0.968891</td>\n","      <td>0.984702</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.002900</td>\n","      <td>0.004034</td>\n","      <td>0.974860</td>\n","      <td>0.959938</td>\n","      <td>0.990953</td>\n","      <td>0.992028</td>\n","      <td>0.992028</td>\n","      <td>0.992028</td>\n","      <td>0.971368</td>\n","      <td>0.953301</td>\n","      <td>0.990796</td>\n","      <td>0.968416</td>\n","      <td>0.945146</td>\n","      <td>0.992861</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.001700</td>\n","      <td>0.002639</td>\n","      <td>0.980723</td>\n","      <td>0.968165</td>\n","      <td>0.993859</td>\n","      <td>0.994708</td>\n","      <td>0.994708</td>\n","      <td>0.994708</td>\n","      <td>0.977988</td>\n","      <td>0.962912</td>\n","      <td>0.993735</td>\n","      <td>0.979428</td>\n","      <td>0.963951</td>\n","      <td>0.995411</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.001700</td>\n","      <td>0.002190</td>\n","      <td>0.979692</td>\n","      <td>0.969816</td>\n","      <td>0.990121</td>\n","      <td>0.994708</td>\n","      <td>0.994708</td>\n","      <td>0.994708</td>\n","      <td>0.976778</td>\n","      <td>0.965025</td>\n","      <td>0.989176</td>\n","      <td>0.979025</td>\n","      <td>0.970441</td>\n","      <td>0.987761</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.001500</td>\n","      <td>0.001838</td>\n","      <td>0.980746</td>\n","      <td>0.971067</td>\n","      <td>0.990886</td>\n","      <td>0.994777</td>\n","      <td>0.994777</td>\n","      <td>0.994777</td>\n","      <td>0.978014</td>\n","      <td>0.966484</td>\n","      <td>0.990082</td>\n","      <td>0.979788</td>\n","      <td>0.970956</td>\n","      <td>0.988781</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.001100</td>\n","      <td>0.001401</td>\n","      <td>0.979460</td>\n","      <td>0.965905</td>\n","      <td>0.993870</td>\n","      <td>0.994846</td>\n","      <td>0.994846</td>\n","      <td>0.994846</td>\n","      <td>0.976508</td>\n","      <td>0.960276</td>\n","      <td>0.993735</td>\n","      <td>0.980176</td>\n","      <td>0.964921</td>\n","      <td>0.995920</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.000600</td>\n","      <td>0.001207</td>\n","      <td>0.981151</td>\n","      <td>0.970460</td>\n","      <td>0.992486</td>\n","      <td>0.995464</td>\n","      <td>0.995464</td>\n","      <td>0.995464</td>\n","      <td>0.978420</td>\n","      <td>0.965683</td>\n","      <td>0.991908</td>\n","      <td>0.982332</td>\n","      <td>0.972514</td>\n","      <td>0.992351</td>\n","    </tr>\n","    <tr>\n","      <td>4010</td>\n","      <td>0.000600</td>\n","      <td>0.001104</td>\n","      <td>0.981502</td>\n","      <td>0.971076</td>\n","      <td>0.992486</td>\n","      <td>0.995464</td>\n","      <td>0.995464</td>\n","      <td>0.995464</td>\n","      <td>0.978830</td>\n","      <td>0.966401</td>\n","      <td>0.991908</td>\n","      <td>0.982332</td>\n","      <td>0.972514</td>\n","      <td>0.992351</td>\n","    </tr>\n","    <tr>\n","      <td>4411</td>\n","      <td>0.000400</td>\n","      <td>0.001148</td>\n","      <td>0.984029</td>\n","      <td>0.975113</td>\n","      <td>0.993296</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.981758</td>\n","      <td>0.971111</td>\n","      <td>0.992814</td>\n","      <td>0.983582</td>\n","      <td>0.974474</td>\n","      <td>0.992861</td>\n","    </tr>\n","    <tr>\n","      <td>4812</td>\n","      <td>0.000400</td>\n","      <td>0.001071</td>\n","      <td>0.983080</td>\n","      <td>0.973328</td>\n","      <td>0.993262</td>\n","      <td>0.995533</td>\n","      <td>0.995533</td>\n","      <td>0.995533</td>\n","      <td>0.980671</td>\n","      <td>0.969028</td>\n","      <td>0.992814</td>\n","      <td>0.982837</td>\n","      <td>0.973013</td>\n","      <td>0.992861</td>\n","    </tr>\n","    <tr>\n","      <td>5213</td>\n","      <td>0.000300</td>\n","      <td>0.001029</td>\n","      <td>0.983446</td>\n","      <td>0.974020</td>\n","      <td>0.993251</td>\n","      <td>0.995464</td>\n","      <td>0.995464</td>\n","      <td>0.995464</td>\n","      <td>0.981105</td>\n","      <td>0.969837</td>\n","      <td>0.992814</td>\n","      <td>0.982589</td>\n","      <td>0.972527</td>\n","      <td>0.992861</td>\n","    </tr>\n","    <tr>\n","      <td>5614</td>\n","      <td>0.000400</td>\n","      <td>0.000976</td>\n","      <td>0.984029</td>\n","      <td>0.975113</td>\n","      <td>0.993296</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.981758</td>\n","      <td>0.971111</td>\n","      <td>0.992814</td>\n","      <td>0.983582</td>\n","      <td>0.974474</td>\n","      <td>0.992861</td>\n","    </tr>\n","    <tr>\n","      <td>6015</td>\n","      <td>0.000400</td>\n","      <td>0.000956</td>\n","      <td>0.984029</td>\n","      <td>0.975113</td>\n","      <td>0.993296</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.981758</td>\n","      <td>0.971111</td>\n","      <td>0.992814</td>\n","      <td>0.983582</td>\n","      <td>0.974474</td>\n","      <td>0.992861</td>\n","    </tr>\n","    <tr>\n","      <td>6416</td>\n","      <td>0.000900</td>\n","      <td>0.000941</td>\n","      <td>0.984029</td>\n","      <td>0.975113</td>\n","      <td>0.993296</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.981758</td>\n","      <td>0.971111</td>\n","      <td>0.992814</td>\n","      <td>0.983582</td>\n","      <td>0.974474</td>\n","      <td>0.992861</td>\n","    </tr>\n","    <tr>\n","      <td>6817</td>\n","      <td>0.000400</td>\n","      <td>0.000936</td>\n","      <td>0.984029</td>\n","      <td>0.975113</td>\n","      <td>0.993296</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.981758</td>\n","      <td>0.971111</td>\n","      <td>0.992814</td>\n","      <td>0.983582</td>\n","      <td>0.974474</td>\n","      <td>0.992861</td>\n","    </tr>\n","    <tr>\n","      <td>7218</td>\n","      <td>0.000300</td>\n","      <td>0.000935</td>\n","      <td>0.984029</td>\n","      <td>0.975113</td>\n","      <td>0.993296</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.995739</td>\n","      <td>0.981758</td>\n","      <td>0.971111</td>\n","      <td>0.992814</td>\n","      <td>0.983582</td>\n","      <td>0.974474</td>\n","      <td>0.992861</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4010\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4411\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4812\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5213\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5614\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6015\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6416\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6817\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7218\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9840289037510175, 'eval_macro_precision': 0.97511306033152, 'eval_macro_recall': 0.9932961650838309, 'eval_micro_f1': 0.9957391244588001, 'eval_micro_precision': 0.9957391244588001, 'eval_micro_recall': 0.9957391244588001, 'eval_macro_f1_no_o': 0.9817581506388976, 'eval_macro_precision_no_o': 0.9711112844790223, 'eval_macro_recall_no_o': 0.9928142842048985, 'eval_micro_f1_no_o': 0.9835817125536752, 'eval_micro_precision_no_o': 0.9744744744744744, 'eval_micro_recall_no_o': 0.9928607853136155, 'eval_loss': 0.0009351297551529569, 'eval_runtime': 5.0313, 'eval_samples_per_second': 19.876, 'eval_steps_per_second': 4.969, 'epoch': 18.0}\n","Accuracy for fold  3 :  0.9835817125536752  --  0.9957391244588001\n","--------------------------------\n","FOLD 4\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1607\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7218\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7218' max='7218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7218/7218 19:46, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.030000</td>\n","      <td>0.032422</td>\n","      <td>0.983030</td>\n","      <td>0.982142</td>\n","      <td>0.984094</td>\n","      <td>0.995726</td>\n","      <td>0.995726</td>\n","      <td>0.995726</td>\n","      <td>0.980501</td>\n","      <td>0.979285</td>\n","      <td>0.981921</td>\n","      <td>0.980035</td>\n","      <td>0.973394</td>\n","      <td>0.986768</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.009400</td>\n","      <td>0.012264</td>\n","      <td>0.987466</td>\n","      <td>0.986102</td>\n","      <td>0.988938</td>\n","      <td>0.995726</td>\n","      <td>0.995726</td>\n","      <td>0.995726</td>\n","      <td>0.985769</td>\n","      <td>0.983999</td>\n","      <td>0.987665</td>\n","      <td>0.983573</td>\n","      <td>0.976908</td>\n","      <td>0.990331</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.005700</td>\n","      <td>0.005665</td>\n","      <td>0.986691</td>\n","      <td>0.985022</td>\n","      <td>0.988448</td>\n","      <td>0.995933</td>\n","      <td>0.995933</td>\n","      <td>0.995933</td>\n","      <td>0.984839</td>\n","      <td>0.982752</td>\n","      <td>0.987027</td>\n","      <td>0.984055</td>\n","      <td>0.978852</td>\n","      <td>0.989313</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.004400</td>\n","      <td>0.003766</td>\n","      <td>0.984756</td>\n","      <td>0.979936</td>\n","      <td>0.989827</td>\n","      <td>0.994761</td>\n","      <td>0.994761</td>\n","      <td>0.994761</td>\n","      <td>0.982694</td>\n","      <td>0.976739</td>\n","      <td>0.988942</td>\n","      <td>0.979899</td>\n","      <td>0.967742</td>\n","      <td>0.992366</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.001900</td>\n","      <td>0.002973</td>\n","      <td>0.984621</td>\n","      <td>0.980860</td>\n","      <td>0.988658</td>\n","      <td>0.995450</td>\n","      <td>0.995450</td>\n","      <td>0.995450</td>\n","      <td>0.982464</td>\n","      <td>0.977804</td>\n","      <td>0.987445</td>\n","      <td>0.982120</td>\n","      <td>0.972084</td>\n","      <td>0.992366</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.001600</td>\n","      <td>0.001747</td>\n","      <td>0.988779</td>\n","      <td>0.987822</td>\n","      <td>0.989843</td>\n","      <td>0.996760</td>\n","      <td>0.996760</td>\n","      <td>0.996760</td>\n","      <td>0.987194</td>\n","      <td>0.985926</td>\n","      <td>0.988589</td>\n","      <td>0.987098</td>\n","      <td>0.981388</td>\n","      <td>0.992875</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.001200</td>\n","      <td>0.001371</td>\n","      <td>0.989131</td>\n","      <td>0.987824</td>\n","      <td>0.990523</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.987605</td>\n","      <td>0.985928</td>\n","      <td>0.989383</td>\n","      <td>0.987604</td>\n","      <td>0.981891</td>\n","      <td>0.993384</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.001000</td>\n","      <td>0.001559</td>\n","      <td>0.986740</td>\n","      <td>0.983952</td>\n","      <td>0.989659</td>\n","      <td>0.995795</td>\n","      <td>0.995795</td>\n","      <td>0.995795</td>\n","      <td>0.984909</td>\n","      <td>0.981423</td>\n","      <td>0.988547</td>\n","      <td>0.983607</td>\n","      <td>0.975000</td>\n","      <td>0.992366</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.000600</td>\n","      <td>0.000948</td>\n","      <td>0.990241</td>\n","      <td>0.988989</td>\n","      <td>0.991559</td>\n","      <td>0.997036</td>\n","      <td>0.997036</td>\n","      <td>0.997036</td>\n","      <td>0.988881</td>\n","      <td>0.987287</td>\n","      <td>0.990551</td>\n","      <td>0.988354</td>\n","      <td>0.983375</td>\n","      <td>0.993384</td>\n","    </tr>\n","    <tr>\n","      <td>4010</td>\n","      <td>0.000700</td>\n","      <td>0.000890</td>\n","      <td>0.987853</td>\n","      <td>0.985356</td>\n","      <td>0.990455</td>\n","      <td>0.996415</td>\n","      <td>0.996415</td>\n","      <td>0.996415</td>\n","      <td>0.986154</td>\n","      <td>0.983048</td>\n","      <td>0.989383</td>\n","      <td>0.986108</td>\n","      <td>0.978937</td>\n","      <td>0.993384</td>\n","    </tr>\n","    <tr>\n","      <td>4411</td>\n","      <td>0.001400</td>\n","      <td>0.000732</td>\n","      <td>0.989603</td>\n","      <td>0.987552</td>\n","      <td>0.991714</td>\n","      <td>0.996898</td>\n","      <td>0.996898</td>\n","      <td>0.996898</td>\n","      <td>0.988156</td>\n","      <td>0.985610</td>\n","      <td>0.990771</td>\n","      <td>0.988110</td>\n","      <td>0.982394</td>\n","      <td>0.993893</td>\n","    </tr>\n","    <tr>\n","      <td>4812</td>\n","      <td>0.000400</td>\n","      <td>0.000671</td>\n","      <td>0.990007</td>\n","      <td>0.988350</td>\n","      <td>0.991725</td>\n","      <td>0.996967</td>\n","      <td>0.996967</td>\n","      <td>0.996967</td>\n","      <td>0.988620</td>\n","      <td>0.986541</td>\n","      <td>0.990771</td>\n","      <td>0.988360</td>\n","      <td>0.982889</td>\n","      <td>0.993893</td>\n","    </tr>\n","    <tr>\n","      <td>5213</td>\n","      <td>0.000600</td>\n","      <td>0.000640</td>\n","      <td>0.989811</td>\n","      <td>0.987989</td>\n","      <td>0.991703</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.988405</td>\n","      <td>0.986120</td>\n","      <td>0.990771</td>\n","      <td>0.987860</td>\n","      <td>0.981900</td>\n","      <td>0.993893</td>\n","    </tr>\n","    <tr>\n","      <td>5614</td>\n","      <td>0.000300</td>\n","      <td>0.000617</td>\n","      <td>0.989191</td>\n","      <td>0.986987</td>\n","      <td>0.991479</td>\n","      <td>0.996553</td>\n","      <td>0.996553</td>\n","      <td>0.996553</td>\n","      <td>0.987702</td>\n","      <td>0.984952</td>\n","      <td>0.990551</td>\n","      <td>0.986606</td>\n","      <td>0.979920</td>\n","      <td>0.993384</td>\n","    </tr>\n","    <tr>\n","      <td>6015</td>\n","      <td>0.000700</td>\n","      <td>0.000582</td>\n","      <td>0.989680</td>\n","      <td>0.987886</td>\n","      <td>0.991536</td>\n","      <td>0.996898</td>\n","      <td>0.996898</td>\n","      <td>0.996898</td>\n","      <td>0.988239</td>\n","      <td>0.986000</td>\n","      <td>0.990551</td>\n","      <td>0.987854</td>\n","      <td>0.982386</td>\n","      <td>0.993384</td>\n","    </tr>\n","    <tr>\n","      <td>6416</td>\n","      <td>0.002900</td>\n","      <td>0.000566</td>\n","      <td>0.989579</td>\n","      <td>0.987871</td>\n","      <td>0.991347</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.988129</td>\n","      <td>0.985996</td>\n","      <td>0.990331</td>\n","      <td>0.987598</td>\n","      <td>0.982377</td>\n","      <td>0.992875</td>\n","    </tr>\n","    <tr>\n","      <td>6817</td>\n","      <td>0.000600</td>\n","      <td>0.000564</td>\n","      <td>0.989579</td>\n","      <td>0.987871</td>\n","      <td>0.991347</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.988129</td>\n","      <td>0.985996</td>\n","      <td>0.990331</td>\n","      <td>0.987598</td>\n","      <td>0.982377</td>\n","      <td>0.992875</td>\n","    </tr>\n","    <tr>\n","      <td>7218</td>\n","      <td>0.001300</td>\n","      <td>0.000562</td>\n","      <td>0.989579</td>\n","      <td>0.987871</td>\n","      <td>0.991347</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.996829</td>\n","      <td>0.988129</td>\n","      <td>0.985996</td>\n","      <td>0.990331</td>\n","      <td>0.987598</td>\n","      <td>0.982377</td>\n","      <td>0.992875</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4010\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4411\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4812\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5213\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5614\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6015\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6416\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6817\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7218\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9895794222891139, 'eval_macro_precision': 0.987870806181, 'eval_macro_recall': 0.9913473548142961, 'eval_micro_f1': 0.9968288983868744, 'eval_micro_precision': 0.9968288983868744, 'eval_micro_recall': 0.9968288983868744, 'eval_macro_f1_no_o': 0.9881286282398439, 'eval_macro_precision_no_o': 0.9859957062527002, 'eval_macro_recall_no_o': 0.9903305190585892, 'eval_micro_f1_no_o': 0.9875980764363452, 'eval_micro_precision_no_o': 0.9823766364551864, 'eval_micro_recall_no_o': 0.9928753180661578, 'eval_loss': 0.0005622432717973425, 'eval_runtime': 5.0315, 'eval_samples_per_second': 19.875, 'eval_steps_per_second': 4.969, 'epoch': 18.0}\n","Accuracy for fold  4 :  0.9875980764363452  --  0.9968288983868744\n","--------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"BBA4owXH8oS1"},"source":["# Results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhusoblW-oCA","executionInfo":{"status":"ok","timestamp":1646808841880,"user_tz":360,"elapsed":17,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"c5d5297c-4427-46ad-e92f-03eb1f8f5b91"},"source":["results"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.72389337, 0.93099469, 0.95901209, 0.98358171, 0.98759808])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["resultss"],"metadata":{"id":"rG0_H3xFqYHr","executionInfo":{"status":"ok","timestamp":1646812306082,"user_tz":360,"elapsed":3,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"ce093fa3-0e8b-471e-fff0-00b05a0d807f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.93107542, 0.98216308, 0.99074567, 0.99573912, 0.9968289 ])"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPIH-gt_-foF","executionInfo":{"status":"ok","timestamp":1646808841880,"user_tz":360,"elapsed":8,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"1e2287cc-8315-4ae5-c1a1-9ad6a46e7c21"},"source":["# Print fold results\n","print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n","print('--------------------------------')\n","sum = 0.0\n","key = 0\n","for value in results:\n","  print(f'Fold {key}: {value} %')\n","  sum += value\n","  key += 1\n","print(f'Average: {sum/len(results)} %')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n","--------------------------------\n","Fold 0: 0.7238933724627049 %\n","Fold 1: 0.9309946918993769 %\n","Fold 2: 0.9590120861797161 %\n","Fold 3: 0.9835817125536752 %\n","Fold 4: 0.9875980764363452 %\n","Average: 0.9170159879063636 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"1DkDHptAhniL"},"source":["# Pytorch Training - Loop UPDT"]},{"cell_type":"code","metadata":{"id":"rirS-e83GVn0"},"source":["device = 'cuda'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSDdzo_so0vk"},"source":["training_args = TrainingArguments(\"trained_scibert_ner_model\", # output dir\n","                                      learning_rate=2.0067205312195233e-05, \n","                                      num_train_epochs=18, \n","                                      dataloader_drop_last=True,\n","                                      per_device_eval_batch_size=4, \n","                                      per_device_train_batch_size=4,\n","                                      logging_steps=50,\n","                                      save_steps=len(train) // batch_size,\n","                                      lr_scheduler_type='cosine',\n","                                      evaluation_strategy='steps',\n","                                      weight_decay=0.24799733516738603,\n","                                      eval_steps=len(train) // batch_size,\n","                                      report_to='all'\n","                                      )\n","\n","#learning_rate 2.0067205312195233e-05\n","#num_train_epochs 18\n","#weight_decay 0.24799733516738603\n","#per_device_train_batch_size 4\n","\n","#load_param()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fEEFqMOs1mF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"abaed5e8-7bb5-4b31-b092-8952a9c8c529"},"source":["loop_val = 10\n","loop_results = np.zeros(loop_val)\n","loop_resultss = np.zeros(loop_val)\n","for r in range(loop_val):\n","\n","  ner_model = NerModel(BertEmbModel).to('cuda')  # make sure we move the model to the GPU for training\n","\n","  trainer = MultilabelTrainer(\n","      model=ner_model, \n","      args=training_args, \n","      train_dataset=train, \n","      eval_dataset=test,\n","      data_collator=collator  # defines how to merge data into batches, using the collator function above\n","  )\n","\n","  # Print\n","  print(f'Train run #{r}')\n","  print('--------------------------------')\n","\n","  trainer.train()\n","\n","  # Process is complete.\n","  print('Training process has finished.')\n","\n","  # Print about testing\n","  print('Starting testing')\n","\n","  with torch.no_grad():\n","    result = trainer.evaluate(test)\n","    print(result)\n","\n","    # Print accuracy\n","    print('Accuracy for fold ', r, ': ', result['eval_micro_f1_no_o'], ' -- ', result['eval_micro_f1'])\n","    print('--------------------------------')\n","    loop_results[r] = result['eval_micro_f1_no_o']\n","    loop_resultss[r] = result['eval_micro_f1']\n","    del result\n","\n","  if r > 0:\n","    if loop_results[r] < loop_results[r-1]:\n","      save_path = f'./model-fold-{r}.pth'\n","      torch.save(ner_model.state_dict(), save_path)\n","\n","  print('Testing process has finished.')"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train run #0\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 39:24, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.077100</td>\n","      <td>0.118436</td>\n","      <td>0.177925</td>\n","      <td>0.158943</td>\n","      <td>0.229566</td>\n","      <td>0.851687</td>\n","      <td>0.851687</td>\n","      <td>0.851687</td>\n","      <td>0.049659</td>\n","      <td>0.032567</td>\n","      <td>0.104508</td>\n","      <td>0.147969</td>\n","      <td>0.195402</td>\n","      <td>0.119066</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.041500</td>\n","      <td>0.069224</td>\n","      <td>0.222765</td>\n","      <td>0.245620</td>\n","      <td>0.245851</td>\n","      <td>0.865596</td>\n","      <td>0.865596</td>\n","      <td>0.865596</td>\n","      <td>0.103458</td>\n","      <td>0.138006</td>\n","      <td>0.121623</td>\n","      <td>0.217516</td>\n","      <td>0.411255</td>\n","      <td>0.147860</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.014500</td>\n","      <td>0.060430</td>\n","      <td>0.383633</td>\n","      <td>0.392691</td>\n","      <td>0.454025</td>\n","      <td>0.880897</td>\n","      <td>0.880897</td>\n","      <td>0.880897</td>\n","      <td>0.287732</td>\n","      <td>0.298431</td>\n","      <td>0.369726</td>\n","      <td>0.431978</td>\n","      <td>0.434014</td>\n","      <td>0.429961</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.014000</td>\n","      <td>0.051201</td>\n","      <td>0.386408</td>\n","      <td>0.393632</td>\n","      <td>0.477451</td>\n","      <td>0.877652</td>\n","      <td>0.877652</td>\n","      <td>0.877652</td>\n","      <td>0.290846</td>\n","      <td>0.297719</td>\n","      <td>0.398589</td>\n","      <td>0.436900</td>\n","      <td>0.415439</td>\n","      <td>0.460700</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.008300</td>\n","      <td>0.046760</td>\n","      <td>0.498633</td>\n","      <td>0.543901</td>\n","      <td>0.552000</td>\n","      <td>0.889707</td>\n","      <td>0.889707</td>\n","      <td>0.889707</td>\n","      <td>0.421724</td>\n","      <td>0.472824</td>\n","      <td>0.485664</td>\n","      <td>0.514317</td>\n","      <td>0.486796</td>\n","      <td>0.545136</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.004800</td>\n","      <td>0.043132</td>\n","      <td>0.606463</td>\n","      <td>0.658266</td>\n","      <td>0.672886</td>\n","      <td>0.895676</td>\n","      <td>0.895676</td>\n","      <td>0.895676</td>\n","      <td>0.547886</td>\n","      <td>0.605828</td>\n","      <td>0.627799</td>\n","      <td>0.573321</td>\n","      <td>0.531012</td>\n","      <td>0.622957</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.002700</td>\n","      <td>0.043045</td>\n","      <td>0.663321</td>\n","      <td>0.673452</td>\n","      <td>0.707205</td>\n","      <td>0.907384</td>\n","      <td>0.907384</td>\n","      <td>0.907384</td>\n","      <td>0.613404</td>\n","      <td>0.623591</td>\n","      <td>0.666204</td>\n","      <td>0.610712</td>\n","      <td>0.579462</td>\n","      <td>0.645525</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.001700</td>\n","      <td>0.045243</td>\n","      <td>0.669025</td>\n","      <td>0.663476</td>\n","      <td>0.722137</td>\n","      <td>0.906804</td>\n","      <td>0.906804</td>\n","      <td>0.906804</td>\n","      <td>0.620266</td>\n","      <td>0.611841</td>\n","      <td>0.684135</td>\n","      <td>0.617234</td>\n","      <td>0.580336</td>\n","      <td>0.659144</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.002600</td>\n","      <td>0.043919</td>\n","      <td>0.732891</td>\n","      <td>0.715239</td>\n","      <td>0.760199</td>\n","      <td>0.916135</td>\n","      <td>0.916135</td>\n","      <td>0.916135</td>\n","      <td>0.694904</td>\n","      <td>0.672645</td>\n","      <td>0.728393</td>\n","      <td>0.677330</td>\n","      <td>0.642036</td>\n","      <td>0.716732</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.001400</td>\n","      <td>0.048893</td>\n","      <td>0.750041</td>\n","      <td>0.737443</td>\n","      <td>0.771126</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.714359</td>\n","      <td>0.698518</td>\n","      <td>0.740085</td>\n","      <td>0.695473</td>\n","      <td>0.669669</td>\n","      <td>0.723346</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.001500</td>\n","      <td>0.047385</td>\n","      <td>0.755351</td>\n","      <td>0.736317</td>\n","      <td>0.779286</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.720535</td>\n","      <td>0.697095</td>\n","      <td>0.749673</td>\n","      <td>0.704961</td>\n","      <td>0.676934</td>\n","      <td>0.735409</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.001100</td>\n","      <td>0.040713</td>\n","      <td>0.748268</td>\n","      <td>0.705107</td>\n","      <td>0.806891</td>\n","      <td>0.914223</td>\n","      <td>0.914223</td>\n","      <td>0.914223</td>\n","      <td>0.713246</td>\n","      <td>0.659394</td>\n","      <td>0.784989</td>\n","      <td>0.693537</td>\n","      <td>0.626491</td>\n","      <td>0.776654</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.001400</td>\n","      <td>0.046020</td>\n","      <td>0.756682</td>\n","      <td>0.732539</td>\n","      <td>0.787893</td>\n","      <td>0.921467</td>\n","      <td>0.921467</td>\n","      <td>0.921467</td>\n","      <td>0.722330</td>\n","      <td>0.692261</td>\n","      <td>0.760600</td>\n","      <td>0.702555</td>\n","      <td>0.661512</td>\n","      <td>0.749027</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000700</td>\n","      <td>0.048106</td>\n","      <td>0.745312</td>\n","      <td>0.726208</td>\n","      <td>0.772080</td>\n","      <td>0.919903</td>\n","      <td>0.919903</td>\n","      <td>0.919903</td>\n","      <td>0.709010</td>\n","      <td>0.685224</td>\n","      <td>0.741709</td>\n","      <td>0.687373</td>\n","      <td>0.654808</td>\n","      <td>0.723346</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.002300</td>\n","      <td>0.043806</td>\n","      <td>0.742333</td>\n","      <td>0.704688</td>\n","      <td>0.796373</td>\n","      <td>0.916947</td>\n","      <td>0.916947</td>\n","      <td>0.916947</td>\n","      <td>0.705839</td>\n","      <td>0.659202</td>\n","      <td>0.771515</td>\n","      <td>0.689079</td>\n","      <td>0.634666</td>\n","      <td>0.753696</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.001000</td>\n","      <td>0.044809</td>\n","      <td>0.748948</td>\n","      <td>0.717198</td>\n","      <td>0.792283</td>\n","      <td>0.919671</td>\n","      <td>0.919671</td>\n","      <td>0.919671</td>\n","      <td>0.713341</td>\n","      <td>0.674110</td>\n","      <td>0.766029</td>\n","      <td>0.694756</td>\n","      <td>0.648986</td>\n","      <td>0.747471</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000800</td>\n","      <td>0.045721</td>\n","      <td>0.752290</td>\n","      <td>0.724937</td>\n","      <td>0.788785</td>\n","      <td>0.920946</td>\n","      <td>0.920946</td>\n","      <td>0.920946</td>\n","      <td>0.717148</td>\n","      <td>0.683356</td>\n","      <td>0.761562</td>\n","      <td>0.697224</td>\n","      <td>0.656917</td>\n","      <td>0.742802</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.001100</td>\n","      <td>0.045811</td>\n","      <td>0.751918</td>\n","      <td>0.724851</td>\n","      <td>0.788228</td>\n","      <td>0.920888</td>\n","      <td>0.920888</td>\n","      <td>0.920888</td>\n","      <td>0.716732</td>\n","      <td>0.683268</td>\n","      <td>0.760935</td>\n","      <td>0.697462</td>\n","      <td>0.657035</td>\n","      <td>0.743191</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7519184176884395, 'eval_macro_precision': 0.7248514555707218, 'eval_macro_recall': 0.7882280891013532, 'eval_micro_f1': 0.9208879100498435, 'eval_micro_precision': 0.9208879100498435, 'eval_micro_recall': 0.9208879100498435, 'eval_macro_f1_no_o': 0.7167315690548702, 'eval_macro_precision_no_o': 0.6832683584432521, 'eval_macro_recall_no_o': 0.7609346774556195, 'eval_micro_f1_no_o': 0.6974621142961476, 'eval_micro_precision_no_o': 0.6570347437220502, 'eval_micro_recall_no_o': 0.7431906614785992, 'eval_loss': 0.04581120306027153, 'eval_runtime': 22.3957, 'eval_samples_per_second': 5.403, 'eval_steps_per_second': 1.384, 'epoch': 18.0}\n","Accuracy for fold  0 :  0.6974621142961476  --  0.9208879100498435\n","--------------------------------\n","Testing process has finished.\n","Train run #1\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 41:05, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.048900</td>\n","      <td>0.107999</td>\n","      <td>0.765101</td>\n","      <td>0.735846</td>\n","      <td>0.799672</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.732089</td>\n","      <td>0.695729</td>\n","      <td>0.774592</td>\n","      <td>0.713021</td>\n","      <td>0.665319</td>\n","      <td>0.768093</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.016100</td>\n","      <td>0.076316</td>\n","      <td>0.729618</td>\n","      <td>0.748260</td>\n","      <td>0.716424</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.690739</td>\n","      <td>0.712352</td>\n","      <td>0.675483</td>\n","      <td>0.694288</td>\n","      <td>0.690944</td>\n","      <td>0.697665</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.006100</td>\n","      <td>0.059093</td>\n","      <td>0.741208</td>\n","      <td>0.691151</td>\n","      <td>0.805295</td>\n","      <td>0.915498</td>\n","      <td>0.915498</td>\n","      <td>0.915498</td>\n","      <td>0.704781</td>\n","      <td>0.643069</td>\n","      <td>0.782730</td>\n","      <td>0.692872</td>\n","      <td>0.628725</td>\n","      <td>0.771595</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003800</td>\n","      <td>0.059552</td>\n","      <td>0.760478</td>\n","      <td>0.750989</td>\n","      <td>0.779849</td>\n","      <td>0.922337</td>\n","      <td>0.922337</td>\n","      <td>0.922337</td>\n","      <td>0.726839</td>\n","      <td>0.714033</td>\n","      <td>0.751137</td>\n","      <td>0.709228</td>\n","      <td>0.670948</td>\n","      <td>0.752140</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002700</td>\n","      <td>0.059774</td>\n","      <td>0.764252</td>\n","      <td>0.763136</td>\n","      <td>0.770928</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.731164</td>\n","      <td>0.728799</td>\n","      <td>0.740003</td>\n","      <td>0.715304</td>\n","      <td>0.690442</td>\n","      <td>0.742023</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001800</td>\n","      <td>0.054921</td>\n","      <td>0.759416</td>\n","      <td>0.742465</td>\n","      <td>0.784322</td>\n","      <td>0.919961</td>\n","      <td>0.919961</td>\n","      <td>0.919961</td>\n","      <td>0.725913</td>\n","      <td>0.703873</td>\n","      <td>0.757173</td>\n","      <td>0.708514</td>\n","      <td>0.660390</td>\n","      <td>0.764202</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001400</td>\n","      <td>0.054361</td>\n","      <td>0.757690</td>\n","      <td>0.732852</td>\n","      <td>0.789049</td>\n","      <td>0.920598</td>\n","      <td>0.920598</td>\n","      <td>0.920598</td>\n","      <td>0.723746</td>\n","      <td>0.692760</td>\n","      <td>0.762289</td>\n","      <td>0.705583</td>\n","      <td>0.662342</td>\n","      <td>0.754864</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000900</td>\n","      <td>0.056640</td>\n","      <td>0.766573</td>\n","      <td>0.757491</td>\n","      <td>0.784246</td>\n","      <td>0.919729</td>\n","      <td>0.919729</td>\n","      <td>0.919729</td>\n","      <td>0.734246</td>\n","      <td>0.721534</td>\n","      <td>0.756924</td>\n","      <td>0.705328</td>\n","      <td>0.660109</td>\n","      <td>0.757198</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.001000</td>\n","      <td>0.055340</td>\n","      <td>0.769961</td>\n","      <td>0.753797</td>\n","      <td>0.790597</td>\n","      <td>0.923786</td>\n","      <td>0.923786</td>\n","      <td>0.923786</td>\n","      <td>0.737811</td>\n","      <td>0.717326</td>\n","      <td>0.763483</td>\n","      <td>0.714654</td>\n","      <td>0.678197</td>\n","      <td>0.755253</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000800</td>\n","      <td>0.057730</td>\n","      <td>0.775993</td>\n","      <td>0.780798</td>\n","      <td>0.774328</td>\n","      <td>0.927959</td>\n","      <td>0.927959</td>\n","      <td>0.927959</td>\n","      <td>0.744502</td>\n","      <td>0.749534</td>\n","      <td>0.743129</td>\n","      <td>0.721587</td>\n","      <td>0.707554</td>\n","      <td>0.736187</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000600</td>\n","      <td>0.051734</td>\n","      <td>0.760612</td>\n","      <td>0.725200</td>\n","      <td>0.803631</td>\n","      <td>0.921467</td>\n","      <td>0.921467</td>\n","      <td>0.921467</td>\n","      <td>0.727024</td>\n","      <td>0.683193</td>\n","      <td>0.779653</td>\n","      <td>0.710936</td>\n","      <td>0.658270</td>\n","      <td>0.772763</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000600</td>\n","      <td>0.053355</td>\n","      <td>0.765962</td>\n","      <td>0.730871</td>\n","      <td>0.808110</td>\n","      <td>0.920308</td>\n","      <td>0.920308</td>\n","      <td>0.920308</td>\n","      <td>0.733465</td>\n","      <td>0.689773</td>\n","      <td>0.785299</td>\n","      <td>0.711798</td>\n","      <td>0.655003</td>\n","      <td>0.779377</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000700</td>\n","      <td>0.053604</td>\n","      <td>0.765044</td>\n","      <td>0.735905</td>\n","      <td>0.800629</td>\n","      <td>0.921004</td>\n","      <td>0.921004</td>\n","      <td>0.921004</td>\n","      <td>0.732275</td>\n","      <td>0.695786</td>\n","      <td>0.776209</td>\n","      <td>0.710371</td>\n","      <td>0.658148</td>\n","      <td>0.771595</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.055140</td>\n","      <td>0.769264</td>\n","      <td>0.751172</td>\n","      <td>0.791177</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.736957</td>\n","      <td>0.714097</td>\n","      <td>0.764239</td>\n","      <td>0.713971</td>\n","      <td>0.675104</td>\n","      <td>0.757588</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000600</td>\n","      <td>0.055310</td>\n","      <td>0.770901</td>\n","      <td>0.747592</td>\n","      <td>0.798918</td>\n","      <td>0.922974</td>\n","      <td>0.922974</td>\n","      <td>0.922974</td>\n","      <td>0.738949</td>\n","      <td>0.709659</td>\n","      <td>0.773678</td>\n","      <td>0.714545</td>\n","      <td>0.669158</td>\n","      <td>0.766537</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000500</td>\n","      <td>0.054940</td>\n","      <td>0.768622</td>\n","      <td>0.742605</td>\n","      <td>0.799579</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.736323</td>\n","      <td>0.703775</td>\n","      <td>0.774575</td>\n","      <td>0.713433</td>\n","      <td>0.666329</td>\n","      <td>0.767704</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000400</td>\n","      <td>0.054947</td>\n","      <td>0.766691</td>\n","      <td>0.738650</td>\n","      <td>0.800265</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.734126</td>\n","      <td>0.699097</td>\n","      <td>0.775545</td>\n","      <td>0.712176</td>\n","      <td>0.662978</td>\n","      <td>0.769261</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000600</td>\n","      <td>0.054880</td>\n","      <td>0.767489</td>\n","      <td>0.739193</td>\n","      <td>0.801387</td>\n","      <td>0.921815</td>\n","      <td>0.921815</td>\n","      <td>0.921815</td>\n","      <td>0.735081</td>\n","      <td>0.699720</td>\n","      <td>0.776911</td>\n","      <td>0.712846</td>\n","      <td>0.662985</td>\n","      <td>0.770817</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7674889926526093, 'eval_macro_precision': 0.7391927805517977, 'eval_macro_recall': 0.801387083126964, 'eval_micro_f1': 0.9218152312507245, 'eval_micro_precision': 0.9218152312507245, 'eval_micro_recall': 0.9218152312507245, 'eval_macro_f1_no_o': 0.7350814298275837, 'eval_macro_precision_no_o': 0.6997204244528206, 'eval_macro_recall_no_o': 0.7769110993877053, 'eval_micro_f1_no_o': 0.7128463476070529, 'eval_micro_precision_no_o': 0.6629852744310576, 'eval_micro_recall_no_o': 0.7708171206225681, 'eval_loss': 0.05487955467942661, 'eval_runtime': 22.2527, 'eval_samples_per_second': 5.438, 'eval_steps_per_second': 1.393, 'epoch': 18.0}\n","Accuracy for fold  1 :  0.7128463476070529  --  0.9218152312507245\n","--------------------------------\n","Testing process has finished.\n","Train run #2\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 42:00, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.032900</td>\n","      <td>0.100425</td>\n","      <td>0.758505</td>\n","      <td>0.761311</td>\n","      <td>0.759915</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.724562</td>\n","      <td>0.726914</td>\n","      <td>0.727119</td>\n","      <td>0.715282</td>\n","      <td>0.693460</td>\n","      <td>0.738521</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.012000</td>\n","      <td>0.071028</td>\n","      <td>0.760019</td>\n","      <td>0.728746</td>\n","      <td>0.796168</td>\n","      <td>0.919439</td>\n","      <td>0.919439</td>\n","      <td>0.919439</td>\n","      <td>0.726701</td>\n","      <td>0.688148</td>\n","      <td>0.770890</td>\n","      <td>0.706352</td>\n","      <td>0.661905</td>\n","      <td>0.757198</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.005200</td>\n","      <td>0.064889</td>\n","      <td>0.773248</td>\n","      <td>0.764194</td>\n","      <td>0.784838</td>\n","      <td>0.925872</td>\n","      <td>0.925872</td>\n","      <td>0.925872</td>\n","      <td>0.741516</td>\n","      <td>0.729895</td>\n","      <td>0.756082</td>\n","      <td>0.719190</td>\n","      <td>0.694314</td>\n","      <td>0.745914</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003300</td>\n","      <td>0.063021</td>\n","      <td>0.751525</td>\n","      <td>0.753474</td>\n","      <td>0.759230</td>\n","      <td>0.919497</td>\n","      <td>0.919497</td>\n","      <td>0.919497</td>\n","      <td>0.716533</td>\n","      <td>0.717017</td>\n","      <td>0.727273</td>\n","      <td>0.696209</td>\n","      <td>0.657558</td>\n","      <td>0.739689</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002200</td>\n","      <td>0.057299</td>\n","      <td>0.775015</td>\n","      <td>0.742212</td>\n","      <td>0.813340</td>\n","      <td>0.926684</td>\n","      <td>0.926684</td>\n","      <td>0.926684</td>\n","      <td>0.743520</td>\n","      <td>0.703415</td>\n","      <td>0.790028</td>\n","      <td>0.728603</td>\n","      <td>0.687371</td>\n","      <td>0.775097</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.058294</td>\n","      <td>0.769956</td>\n","      <td>0.744885</td>\n","      <td>0.800869</td>\n","      <td>0.924887</td>\n","      <td>0.924887</td>\n","      <td>0.924887</td>\n","      <td>0.737874</td>\n","      <td>0.707326</td>\n","      <td>0.775217</td>\n","      <td>0.721250</td>\n","      <td>0.691155</td>\n","      <td>0.754086</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001200</td>\n","      <td>0.053793</td>\n","      <td>0.751619</td>\n","      <td>0.712585</td>\n","      <td>0.812211</td>\n","      <td>0.904544</td>\n","      <td>0.904544</td>\n","      <td>0.904544</td>\n","      <td>0.718339</td>\n","      <td>0.667575</td>\n","      <td>0.793932</td>\n","      <td>0.684524</td>\n","      <td>0.595170</td>\n","      <td>0.805447</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.058825</td>\n","      <td>0.764204</td>\n","      <td>0.752243</td>\n","      <td>0.779452</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.731042</td>\n","      <td>0.715827</td>\n","      <td>0.750071</td>\n","      <td>0.714738</td>\n","      <td>0.685735</td>\n","      <td>0.746304</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.057792</td>\n","      <td>0.767381</td>\n","      <td>0.747414</td>\n","      <td>0.790412</td>\n","      <td>0.924076</td>\n","      <td>0.924076</td>\n","      <td>0.924076</td>\n","      <td>0.734804</td>\n","      <td>0.709835</td>\n","      <td>0.763312</td>\n","      <td>0.716912</td>\n","      <td>0.679443</td>\n","      <td>0.758755</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.001000</td>\n","      <td>0.058537</td>\n","      <td>0.768528</td>\n","      <td>0.750919</td>\n","      <td>0.789696</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.735882</td>\n","      <td>0.714031</td>\n","      <td>0.761864</td>\n","      <td>0.722605</td>\n","      <td>0.692335</td>\n","      <td>0.755642</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000600</td>\n","      <td>0.060117</td>\n","      <td>0.768749</td>\n","      <td>0.751952</td>\n","      <td>0.787601</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.736206</td>\n","      <td>0.715360</td>\n","      <td>0.759432</td>\n","      <td>0.719851</td>\n","      <td>0.690877</td>\n","      <td>0.751362</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000600</td>\n","      <td>0.058294</td>\n","      <td>0.765520</td>\n","      <td>0.740280</td>\n","      <td>0.794875</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.732542</td>\n","      <td>0.701518</td>\n","      <td>0.768338</td>\n","      <td>0.717315</td>\n","      <td>0.681739</td>\n","      <td>0.756809</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.057987</td>\n","      <td>0.768971</td>\n","      <td>0.749750</td>\n","      <td>0.790643</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.736431</td>\n","      <td>0.712604</td>\n","      <td>0.763094</td>\n","      <td>0.721439</td>\n","      <td>0.689227</td>\n","      <td>0.756809</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.059711</td>\n","      <td>0.766321</td>\n","      <td>0.749904</td>\n","      <td>0.785511</td>\n","      <td>0.925988</td>\n","      <td>0.925988</td>\n","      <td>0.925988</td>\n","      <td>0.733338</td>\n","      <td>0.712969</td>\n","      <td>0.756925</td>\n","      <td>0.718044</td>\n","      <td>0.689853</td>\n","      <td>0.748638</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000500</td>\n","      <td>0.059856</td>\n","      <td>0.767297</td>\n","      <td>0.752961</td>\n","      <td>0.783418</td>\n","      <td>0.926915</td>\n","      <td>0.926915</td>\n","      <td>0.926915</td>\n","      <td>0.734374</td>\n","      <td>0.716595</td>\n","      <td>0.754222</td>\n","      <td>0.719325</td>\n","      <td>0.694565</td>\n","      <td>0.745914</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000500</td>\n","      <td>0.058835</td>\n","      <td>0.769473</td>\n","      <td>0.751403</td>\n","      <td>0.789338</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.736902</td>\n","      <td>0.714591</td>\n","      <td>0.761287</td>\n","      <td>0.721507</td>\n","      <td>0.692942</td>\n","      <td>0.752529</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.058659</td>\n","      <td>0.769694</td>\n","      <td>0.751984</td>\n","      <td>0.789372</td>\n","      <td>0.926973</td>\n","      <td>0.926973</td>\n","      <td>0.926973</td>\n","      <td>0.737156</td>\n","      <td>0.715193</td>\n","      <td>0.761396</td>\n","      <td>0.721250</td>\n","      <td>0.691155</td>\n","      <td>0.754086</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.058635</td>\n","      <td>0.769888</td>\n","      <td>0.752203</td>\n","      <td>0.789540</td>\n","      <td>0.927031</td>\n","      <td>0.927031</td>\n","      <td>0.927031</td>\n","      <td>0.737382</td>\n","      <td>0.715448</td>\n","      <td>0.761591</td>\n","      <td>0.721623</td>\n","      <td>0.691512</td>\n","      <td>0.754475</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7698877626726546, 'eval_macro_precision': 0.7522026910705988, 'eval_macro_recall': 0.7895401684411103, 'eval_micro_f1': 0.9270314130056798, 'eval_micro_precision': 0.9270314130056798, 'eval_micro_recall': 0.9270314130056798, 'eval_macro_f1_no_o': 0.7373822712520068, 'eval_macro_precision_no_o': 0.7154475455800585, 'eval_macro_recall_no_o': 0.7615914695555802, 'eval_micro_f1_no_o': 0.7216226274655749, 'eval_micro_precision_no_o': 0.6915121255349501, 'eval_micro_recall_no_o': 0.7544747081712062, 'eval_loss': 0.058634633921090515, 'eval_runtime': 24.4346, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 1.269, 'epoch': 18.0}\n","Accuracy for fold  2 :  0.7216226274655749  --  0.9270314130056798\n","--------------------------------\n","Testing process has finished.\n","Train run #3\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 34:19, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.031600</td>\n","      <td>0.101353</td>\n","      <td>0.758658</td>\n","      <td>0.752284</td>\n","      <td>0.768538</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.724537</td>\n","      <td>0.715981</td>\n","      <td>0.737169</td>\n","      <td>0.712093</td>\n","      <td>0.686147</td>\n","      <td>0.740078</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.011200</td>\n","      <td>0.074897</td>\n","      <td>0.754605</td>\n","      <td>0.740181</td>\n","      <td>0.773974</td>\n","      <td>0.922105</td>\n","      <td>0.922105</td>\n","      <td>0.922105</td>\n","      <td>0.719955</td>\n","      <td>0.701466</td>\n","      <td>0.744180</td>\n","      <td>0.706142</td>\n","      <td>0.669456</td>\n","      <td>0.747082</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.005100</td>\n","      <td>0.067902</td>\n","      <td>0.764349</td>\n","      <td>0.752955</td>\n","      <td>0.781765</td>\n","      <td>0.923786</td>\n","      <td>0.923786</td>\n","      <td>0.923786</td>\n","      <td>0.731252</td>\n","      <td>0.716365</td>\n","      <td>0.753134</td>\n","      <td>0.713970</td>\n","      <td>0.678221</td>\n","      <td>0.753696</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002900</td>\n","      <td>0.058466</td>\n","      <td>0.764892</td>\n","      <td>0.730834</td>\n","      <td>0.808908</td>\n","      <td>0.920192</td>\n","      <td>0.920192</td>\n","      <td>0.920192</td>\n","      <td>0.732242</td>\n","      <td>0.689392</td>\n","      <td>0.786593</td>\n","      <td>0.714587</td>\n","      <td>0.651603</td>\n","      <td>0.791051</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002300</td>\n","      <td>0.056028</td>\n","      <td>0.770348</td>\n","      <td>0.730022</td>\n","      <td>0.824371</td>\n","      <td>0.915614</td>\n","      <td>0.915614</td>\n","      <td>0.915614</td>\n","      <td>0.739325</td>\n","      <td>0.688360</td>\n","      <td>0.806086</td>\n","      <td>0.714482</td>\n","      <td>0.639042</td>\n","      <td>0.810117</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001500</td>\n","      <td>0.052751</td>\n","      <td>0.762902</td>\n","      <td>0.719669</td>\n","      <td>0.820989</td>\n","      <td>0.916831</td>\n","      <td>0.916831</td>\n","      <td>0.916831</td>\n","      <td>0.730375</td>\n","      <td>0.676054</td>\n","      <td>0.801846</td>\n","      <td>0.713623</td>\n","      <td>0.638880</td>\n","      <td>0.808171</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001200</td>\n","      <td>0.054359</td>\n","      <td>0.781224</td>\n","      <td>0.749514</td>\n","      <td>0.820538</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.751043</td>\n","      <td>0.711692</td>\n","      <td>0.799197</td>\n","      <td>0.734665</td>\n","      <td>0.683172</td>\n","      <td>0.794553</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.055497</td>\n","      <td>0.774146</td>\n","      <td>0.742289</td>\n","      <td>0.814249</td>\n","      <td>0.921641</td>\n","      <td>0.921641</td>\n","      <td>0.921641</td>\n","      <td>0.743024</td>\n","      <td>0.702982</td>\n","      <td>0.792587</td>\n","      <td>0.721190</td>\n","      <td>0.661578</td>\n","      <td>0.792607</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.059093</td>\n","      <td>0.781069</td>\n","      <td>0.784573</td>\n","      <td>0.783689</td>\n","      <td>0.930161</td>\n","      <td>0.930161</td>\n","      <td>0.930161</td>\n","      <td>0.750262</td>\n","      <td>0.753357</td>\n","      <td>0.754300</td>\n","      <td>0.733973</td>\n","      <td>0.710076</td>\n","      <td>0.759533</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000800</td>\n","      <td>0.052781</td>\n","      <td>0.778462</td>\n","      <td>0.744132</td>\n","      <td>0.820778</td>\n","      <td>0.926915</td>\n","      <td>0.926915</td>\n","      <td>0.926915</td>\n","      <td>0.747487</td>\n","      <td>0.705052</td>\n","      <td>0.799171</td>\n","      <td>0.732339</td>\n","      <td>0.680588</td>\n","      <td>0.792607</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000600</td>\n","      <td>0.051485</td>\n","      <td>0.786154</td>\n","      <td>0.763851</td>\n","      <td>0.811776</td>\n","      <td>0.931030</td>\n","      <td>0.931030</td>\n","      <td>0.931030</td>\n","      <td>0.756076</td>\n","      <td>0.728508</td>\n","      <td>0.787488</td>\n","      <td>0.739893</td>\n","      <td>0.703899</td>\n","      <td>0.779767</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000800</td>\n","      <td>0.051880</td>\n","      <td>0.783586</td>\n","      <td>0.764188</td>\n","      <td>0.805960</td>\n","      <td>0.930161</td>\n","      <td>0.930161</td>\n","      <td>0.930161</td>\n","      <td>0.753115</td>\n","      <td>0.728902</td>\n","      <td>0.780771</td>\n","      <td>0.735755</td>\n","      <td>0.699264</td>\n","      <td>0.776265</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.052997</td>\n","      <td>0.779075</td>\n","      <td>0.744786</td>\n","      <td>0.820429</td>\n","      <td>0.925930</td>\n","      <td>0.925930</td>\n","      <td>0.925930</td>\n","      <td>0.748260</td>\n","      <td>0.705873</td>\n","      <td>0.798820</td>\n","      <td>0.728024</td>\n","      <td>0.676579</td>\n","      <td>0.787938</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.057331</td>\n","      <td>0.776436</td>\n","      <td>0.754417</td>\n","      <td>0.802503</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.745122</td>\n","      <td>0.717574</td>\n","      <td>0.777351</td>\n","      <td>0.724050</td>\n","      <td>0.682633</td>\n","      <td>0.770817</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000600</td>\n","      <td>0.057757</td>\n","      <td>0.776887</td>\n","      <td>0.759000</td>\n","      <td>0.797955</td>\n","      <td>0.927321</td>\n","      <td>0.927321</td>\n","      <td>0.927321</td>\n","      <td>0.745546</td>\n","      <td>0.723103</td>\n","      <td>0.771669</td>\n","      <td>0.725563</td>\n","      <td>0.689691</td>\n","      <td>0.765370</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000700</td>\n","      <td>0.057906</td>\n","      <td>0.775974</td>\n","      <td>0.758801</td>\n","      <td>0.795886</td>\n","      <td>0.927321</td>\n","      <td>0.927321</td>\n","      <td>0.927321</td>\n","      <td>0.744483</td>\n","      <td>0.722981</td>\n","      <td>0.769154</td>\n","      <td>0.724783</td>\n","      <td>0.691140</td>\n","      <td>0.761868</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.057932</td>\n","      <td>0.776547</td>\n","      <td>0.758840</td>\n","      <td>0.796989</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.745147</td>\n","      <td>0.723005</td>\n","      <td>0.770452</td>\n","      <td>0.725120</td>\n","      <td>0.691114</td>\n","      <td>0.762646</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000600</td>\n","      <td>0.057930</td>\n","      <td>0.776386</td>\n","      <td>0.758578</td>\n","      <td>0.796979</td>\n","      <td>0.927321</td>\n","      <td>0.927321</td>\n","      <td>0.927321</td>\n","      <td>0.744954</td>\n","      <td>0.722677</td>\n","      <td>0.770452</td>\n","      <td>0.724718</td>\n","      <td>0.690384</td>\n","      <td>0.762646</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7763861069856998, 'eval_macro_precision': 0.7585783086711924, 'eval_macro_recall': 0.7969789512657297, 'eval_micro_f1': 0.9273212008809552, 'eval_micro_precision': 0.9273212008809552, 'eval_micro_recall': 0.9273212008809552, 'eval_macro_f1_no_o': 0.7449535224019046, 'eval_macro_precision_no_o': 0.722677121476086, 'eval_macro_recall_no_o': 0.7704516530770661, 'eval_micro_f1_no_o': 0.7247180624884452, 'eval_micro_precision_no_o': 0.6903839380063402, 'eval_micro_recall_no_o': 0.7626459143968871, 'eval_loss': 0.05792972616475235, 'eval_runtime': 7.7095, 'eval_samples_per_second': 15.695, 'eval_steps_per_second': 4.021, 'epoch': 18.0}\n","Accuracy for fold  3 :  0.7247180624884452  --  0.9273212008809552\n","--------------------------------\n","Testing process has finished.\n","Train run #4\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 34:02, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.030200</td>\n","      <td>0.103982</td>\n","      <td>0.755349</td>\n","      <td>0.752851</td>\n","      <td>0.764448</td>\n","      <td>0.920366</td>\n","      <td>0.920366</td>\n","      <td>0.920366</td>\n","      <td>0.720913</td>\n","      <td>0.716406</td>\n","      <td>0.733089</td>\n","      <td>0.697383</td>\n","      <td>0.662465</td>\n","      <td>0.736187</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.010500</td>\n","      <td>0.072480</td>\n","      <td>0.778745</td>\n","      <td>0.793905</td>\n","      <td>0.767377</td>\n","      <td>0.927147</td>\n","      <td>0.927147</td>\n","      <td>0.927147</td>\n","      <td>0.747830</td>\n","      <td>0.765154</td>\n","      <td>0.734929</td>\n","      <td>0.718402</td>\n","      <td>0.709408</td>\n","      <td>0.727626</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004700</td>\n","      <td>0.064603</td>\n","      <td>0.757043</td>\n","      <td>0.718420</td>\n","      <td>0.810653</td>\n","      <td>0.911731</td>\n","      <td>0.911731</td>\n","      <td>0.911731</td>\n","      <td>0.723928</td>\n","      <td>0.675023</td>\n","      <td>0.790139</td>\n","      <td>0.694635</td>\n","      <td>0.622304</td>\n","      <td>0.785992</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002900</td>\n","      <td>0.062427</td>\n","      <td>0.763606</td>\n","      <td>0.739769</td>\n","      <td>0.792856</td>\n","      <td>0.920250</td>\n","      <td>0.920250</td>\n","      <td>0.920250</td>\n","      <td>0.730739</td>\n","      <td>0.700647</td>\n","      <td>0.767083</td>\n","      <td>0.708491</td>\n","      <td>0.660060</td>\n","      <td>0.764591</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002200</td>\n","      <td>0.056089</td>\n","      <td>0.770758</td>\n","      <td>0.733431</td>\n","      <td>0.815365</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.738601</td>\n","      <td>0.692717</td>\n","      <td>0.792913</td>\n","      <td>0.721872</td>\n","      <td>0.671692</td>\n","      <td>0.780156</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.055100</td>\n","      <td>0.780684</td>\n","      <td>0.759228</td>\n","      <td>0.804776</td>\n","      <td>0.927727</td>\n","      <td>0.927727</td>\n","      <td>0.927727</td>\n","      <td>0.749981</td>\n","      <td>0.723425</td>\n","      <td>0.779582</td>\n","      <td>0.727877</td>\n","      <td>0.692930</td>\n","      <td>0.766537</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.053073</td>\n","      <td>0.779824</td>\n","      <td>0.754301</td>\n","      <td>0.809253</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.749055</td>\n","      <td>0.717482</td>\n","      <td>0.785146</td>\n","      <td>0.730269</td>\n","      <td>0.689727</td>\n","      <td>0.775875</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.057125</td>\n","      <td>0.780259</td>\n","      <td>0.775026</td>\n","      <td>0.787730</td>\n","      <td>0.929408</td>\n","      <td>0.929408</td>\n","      <td>0.929408</td>\n","      <td>0.749331</td>\n","      <td>0.742652</td>\n","      <td>0.758617</td>\n","      <td>0.726163</td>\n","      <td>0.712042</td>\n","      <td>0.740856</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.054563</td>\n","      <td>0.783283</td>\n","      <td>0.774043</td>\n","      <td>0.796576</td>\n","      <td>0.928654</td>\n","      <td>0.928654</td>\n","      <td>0.928654</td>\n","      <td>0.752960</td>\n","      <td>0.741002</td>\n","      <td>0.769630</td>\n","      <td>0.729447</td>\n","      <td>0.701653</td>\n","      <td>0.759533</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.001100</td>\n","      <td>0.050802</td>\n","      <td>0.776725</td>\n","      <td>0.758135</td>\n","      <td>0.800409</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.745320</td>\n","      <td>0.722148</td>\n","      <td>0.774408</td>\n","      <td>0.726970</td>\n","      <td>0.692877</td>\n","      <td>0.764591</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.051700</td>\n","      <td>0.776170</td>\n","      <td>0.754295</td>\n","      <td>0.801056</td>\n","      <td>0.927147</td>\n","      <td>0.927147</td>\n","      <td>0.927147</td>\n","      <td>0.744774</td>\n","      <td>0.717815</td>\n","      <td>0.775220</td>\n","      <td>0.725454</td>\n","      <td>0.692362</td>\n","      <td>0.761868</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.054675</td>\n","      <td>0.782877</td>\n","      <td>0.773871</td>\n","      <td>0.793703</td>\n","      <td>0.929929</td>\n","      <td>0.929929</td>\n","      <td>0.929929</td>\n","      <td>0.752414</td>\n","      <td>0.741144</td>\n","      <td>0.765801</td>\n","      <td>0.732095</td>\n","      <td>0.713442</td>\n","      <td>0.751751</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.050143</td>\n","      <td>0.779471</td>\n","      <td>0.762522</td>\n","      <td>0.801127</td>\n","      <td>0.928596</td>\n","      <td>0.928596</td>\n","      <td>0.928596</td>\n","      <td>0.748504</td>\n","      <td>0.727298</td>\n","      <td>0.775178</td>\n","      <td>0.730776</td>\n","      <td>0.697559</td>\n","      <td>0.767315</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.051823</td>\n","      <td>0.778208</td>\n","      <td>0.758255</td>\n","      <td>0.802252</td>\n","      <td>0.928712</td>\n","      <td>0.928712</td>\n","      <td>0.928712</td>\n","      <td>0.747027</td>\n","      <td>0.722418</td>\n","      <td>0.776388</td>\n","      <td>0.730619</td>\n","      <td>0.699537</td>\n","      <td>0.764591</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000500</td>\n","      <td>0.051849</td>\n","      <td>0.777518</td>\n","      <td>0.756020</td>\n","      <td>0.802863</td>\n","      <td>0.928249</td>\n","      <td>0.928249</td>\n","      <td>0.928249</td>\n","      <td>0.746270</td>\n","      <td>0.719801</td>\n","      <td>0.777203</td>\n","      <td>0.729634</td>\n","      <td>0.697410</td>\n","      <td>0.764981</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000600</td>\n","      <td>0.052191</td>\n","      <td>0.771203</td>\n","      <td>0.739404</td>\n","      <td>0.807500</td>\n","      <td>0.925525</td>\n","      <td>0.925525</td>\n","      <td>0.925525</td>\n","      <td>0.739099</td>\n","      <td>0.700085</td>\n","      <td>0.783317</td>\n","      <td>0.722729</td>\n","      <td>0.680288</td>\n","      <td>0.770817</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.052186</td>\n","      <td>0.771853</td>\n","      <td>0.740690</td>\n","      <td>0.807392</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.739851</td>\n","      <td>0.701596</td>\n","      <td>0.783168</td>\n","      <td>0.723125</td>\n","      <td>0.680990</td>\n","      <td>0.770817</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.052245</td>\n","      <td>0.772425</td>\n","      <td>0.741676</td>\n","      <td>0.807431</td>\n","      <td>0.925872</td>\n","      <td>0.925872</td>\n","      <td>0.925872</td>\n","      <td>0.740501</td>\n","      <td>0.702756</td>\n","      <td>0.783168</td>\n","      <td>0.723785</td>\n","      <td>0.682163</td>\n","      <td>0.770817</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7724254788592757, 'eval_macro_precision': 0.7416757757349812, 'eval_macro_recall': 0.8074307818603534, 'eval_micro_f1': 0.9258722615045787, 'eval_micro_precision': 0.9258722615045787, 'eval_micro_recall': 0.9258722615045787, 'eval_macro_f1_no_o': 0.7405008465500303, 'eval_macro_precision_no_o': 0.7027564654190345, 'eval_macro_recall_no_o': 0.7831675656708209, 'eval_micro_f1_no_o': 0.7237851662404093, 'eval_micro_precision_no_o': 0.6821625344352618, 'eval_micro_recall_no_o': 0.7708171206225681, 'eval_loss': 0.05224547496515494, 'eval_runtime': 25.1918, 'eval_samples_per_second': 4.803, 'eval_steps_per_second': 1.231, 'epoch': 18.0}\n","Accuracy for fold  4 :  0.7237851662404093  --  0.9258722615045787\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #5\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 37:56, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.030600</td>\n","      <td>0.101777</td>\n","      <td>0.750330</td>\n","      <td>0.721916</td>\n","      <td>0.787951</td>\n","      <td>0.921525</td>\n","      <td>0.921525</td>\n","      <td>0.921525</td>\n","      <td>0.714978</td>\n","      <td>0.680189</td>\n","      <td>0.760475</td>\n","      <td>0.702613</td>\n","      <td>0.666550</td>\n","      <td>0.742802</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.010400</td>\n","      <td>0.072446</td>\n","      <td>0.758597</td>\n","      <td>0.742206</td>\n","      <td>0.783216</td>\n","      <td>0.923844</td>\n","      <td>0.923844</td>\n","      <td>0.923844</td>\n","      <td>0.724549</td>\n","      <td>0.703934</td>\n","      <td>0.754735</td>\n","      <td>0.713757</td>\n","      <td>0.680056</td>\n","      <td>0.750973</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004600</td>\n","      <td>0.062889</td>\n","      <td>0.783165</td>\n","      <td>0.771235</td>\n","      <td>0.797134</td>\n","      <td>0.929002</td>\n","      <td>0.929002</td>\n","      <td>0.929002</td>\n","      <td>0.752765</td>\n","      <td>0.737831</td>\n","      <td>0.770064</td>\n","      <td>0.728537</td>\n","      <td>0.704323</td>\n","      <td>0.754475</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002800</td>\n","      <td>0.062209</td>\n","      <td>0.777631</td>\n","      <td>0.760164</td>\n","      <td>0.798276</td>\n","      <td>0.926510</td>\n","      <td>0.926510</td>\n","      <td>0.926510</td>\n","      <td>0.746550</td>\n","      <td>0.724666</td>\n","      <td>0.772112</td>\n","      <td>0.724214</td>\n","      <td>0.689789</td>\n","      <td>0.762257</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002400</td>\n","      <td>0.063574</td>\n","      <td>0.769317</td>\n","      <td>0.778550</td>\n","      <td>0.771575</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.736804</td>\n","      <td>0.746539</td>\n","      <td>0.740461</td>\n","      <td>0.709593</td>\n","      <td>0.685528</td>\n","      <td>0.735409</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001500</td>\n","      <td>0.064523</td>\n","      <td>0.750871</td>\n","      <td>0.732932</td>\n","      <td>0.775639</td>\n","      <td>0.916773</td>\n","      <td>0.916773</td>\n","      <td>0.916773</td>\n","      <td>0.716019</td>\n","      <td>0.692964</td>\n","      <td>0.746985</td>\n","      <td>0.689855</td>\n","      <td>0.645424</td>\n","      <td>0.740856</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001100</td>\n","      <td>0.065026</td>\n","      <td>0.748427</td>\n","      <td>0.727897</td>\n","      <td>0.774014</td>\n","      <td>0.918859</td>\n","      <td>0.918859</td>\n","      <td>0.918859</td>\n","      <td>0.712879</td>\n","      <td>0.687273</td>\n","      <td>0.744352</td>\n","      <td>0.689719</td>\n","      <td>0.653994</td>\n","      <td>0.729572</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.063531</td>\n","      <td>0.756368</td>\n","      <td>0.732708</td>\n","      <td>0.786089</td>\n","      <td>0.919033</td>\n","      <td>0.919033</td>\n","      <td>0.919033</td>\n","      <td>0.722341</td>\n","      <td>0.692600</td>\n","      <td>0.759097</td>\n","      <td>0.701195</td>\n","      <td>0.655827</td>\n","      <td>0.753307</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.054773</td>\n","      <td>0.778172</td>\n","      <td>0.759476</td>\n","      <td>0.798625</td>\n","      <td>0.927901</td>\n","      <td>0.927901</td>\n","      <td>0.927901</td>\n","      <td>0.747106</td>\n","      <td>0.724111</td>\n","      <td>0.772134</td>\n","      <td>0.728190</td>\n","      <td>0.700323</td>\n","      <td>0.758366</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.001600</td>\n","      <td>0.061127</td>\n","      <td>0.753606</td>\n","      <td>0.738592</td>\n","      <td>0.775669</td>\n","      <td>0.917758</td>\n","      <td>0.917758</td>\n","      <td>0.917758</td>\n","      <td>0.719039</td>\n","      <td>0.699349</td>\n","      <td>0.746895</td>\n","      <td>0.691027</td>\n","      <td>0.645707</td>\n","      <td>0.743191</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.065500</td>\n","      <td>0.770243</td>\n","      <td>0.781850</td>\n","      <td>0.761774</td>\n","      <td>0.928770</td>\n","      <td>0.928770</td>\n","      <td>0.928770</td>\n","      <td>0.737748</td>\n","      <td>0.751503</td>\n","      <td>0.727654</td>\n","      <td>0.718683</td>\n","      <td>0.724220</td>\n","      <td>0.713230</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.063802</td>\n","      <td>0.762432</td>\n","      <td>0.752448</td>\n","      <td>0.775189</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.729053</td>\n","      <td>0.716342</td>\n","      <td>0.744984</td>\n","      <td>0.706302</td>\n","      <td>0.681752</td>\n","      <td>0.732685</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000800</td>\n","      <td>0.063618</td>\n","      <td>0.768071</td>\n","      <td>0.760245</td>\n","      <td>0.777425</td>\n","      <td>0.925988</td>\n","      <td>0.925988</td>\n","      <td>0.925988</td>\n","      <td>0.735379</td>\n","      <td>0.725520</td>\n","      <td>0.747014</td>\n","      <td>0.713961</td>\n","      <td>0.696521</td>\n","      <td>0.732296</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.064098</td>\n","      <td>0.768447</td>\n","      <td>0.761262</td>\n","      <td>0.777035</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.735852</td>\n","      <td>0.726752</td>\n","      <td>0.746583</td>\n","      <td>0.713093</td>\n","      <td>0.695926</td>\n","      <td>0.731128</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000500</td>\n","      <td>0.063832</td>\n","      <td>0.768427</td>\n","      <td>0.760397</td>\n","      <td>0.777792</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.735767</td>\n","      <td>0.725642</td>\n","      <td>0.747442</td>\n","      <td>0.713663</td>\n","      <td>0.695604</td>\n","      <td>0.732685</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.063925</td>\n","      <td>0.769190</td>\n","      <td>0.759539</td>\n","      <td>0.780498</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.736685</td>\n","      <td>0.724534</td>\n","      <td>0.750758</td>\n","      <td>0.714205</td>\n","      <td>0.693153</td>\n","      <td>0.736576</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.063690</td>\n","      <td>0.766725</td>\n","      <td>0.755465</td>\n","      <td>0.779759</td>\n","      <td>0.925177</td>\n","      <td>0.925177</td>\n","      <td>0.925177</td>\n","      <td>0.733847</td>\n","      <td>0.719730</td>\n","      <td>0.750021</td>\n","      <td>0.712056</td>\n","      <td>0.689115</td>\n","      <td>0.736576</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000700</td>\n","      <td>0.063672</td>\n","      <td>0.766452</td>\n","      <td>0.754686</td>\n","      <td>0.780032</td>\n","      <td>0.925119</td>\n","      <td>0.925119</td>\n","      <td>0.925119</td>\n","      <td>0.733540</td>\n","      <td>0.718822</td>\n","      <td>0.750362</td>\n","      <td>0.712164</td>\n","      <td>0.688978</td>\n","      <td>0.736965</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7664522758128262, 'eval_macro_precision': 0.7546864431425784, 'eval_macro_recall': 0.7800320767546012, 'eval_micro_f1': 0.9251188130288629, 'eval_micro_precision': 0.9251188130288629, 'eval_micro_recall': 0.9251188130288629, 'eval_macro_f1_no_o': 0.7335401940851237, 'eval_macro_precision_no_o': 0.7188221073708112, 'eval_macro_recall_no_o': 0.7503624932517474, 'eval_micro_f1_no_o': 0.7121639405903365, 'eval_micro_precision_no_o': 0.6889778101127683, 'eval_micro_recall_no_o': 0.7369649805447471, 'eval_loss': 0.06367204671514178, 'eval_runtime': 25.3527, 'eval_samples_per_second': 4.773, 'eval_steps_per_second': 1.223, 'epoch': 18.0}\n","Accuracy for fold  5 :  0.7121639405903365  --  0.9251188130288629\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #6\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 41:00, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.031800</td>\n","      <td>0.108737</td>\n","      <td>0.734845</td>\n","      <td>0.718896</td>\n","      <td>0.774494</td>\n","      <td>0.904602</td>\n","      <td>0.904602</td>\n","      <td>0.904602</td>\n","      <td>0.698068</td>\n","      <td>0.675133</td>\n","      <td>0.748430</td>\n","      <td>0.657623</td>\n","      <td>0.582807</td>\n","      <td>0.754475</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.010200</td>\n","      <td>0.080065</td>\n","      <td>0.755166</td>\n","      <td>0.790081</td>\n","      <td>0.728062</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.720320</td>\n","      <td>0.761776</td>\n","      <td>0.687971</td>\n","      <td>0.713401</td>\n","      <td>0.732868</td>\n","      <td>0.694942</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004700</td>\n","      <td>0.066784</td>\n","      <td>0.760872</td>\n","      <td>0.739803</td>\n","      <td>0.788560</td>\n","      <td>0.921120</td>\n","      <td>0.921120</td>\n","      <td>0.921120</td>\n","      <td>0.727354</td>\n","      <td>0.700908</td>\n","      <td>0.761480</td>\n","      <td>0.704421</td>\n","      <td>0.663912</td>\n","      <td>0.750195</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003200</td>\n","      <td>0.068368</td>\n","      <td>0.761345</td>\n","      <td>0.762208</td>\n","      <td>0.763194</td>\n","      <td>0.921062</td>\n","      <td>0.921062</td>\n","      <td>0.921062</td>\n","      <td>0.728088</td>\n","      <td>0.728285</td>\n","      <td>0.731047</td>\n","      <td>0.700965</td>\n","      <td>0.682002</td>\n","      <td>0.721012</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002300</td>\n","      <td>0.070003</td>\n","      <td>0.754192</td>\n","      <td>0.767560</td>\n","      <td>0.750143</td>\n","      <td>0.920019</td>\n","      <td>0.920019</td>\n","      <td>0.920019</td>\n","      <td>0.719749</td>\n","      <td>0.734391</td>\n","      <td>0.715968</td>\n","      <td>0.695652</td>\n","      <td>0.673715</td>\n","      <td>0.719066</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001700</td>\n","      <td>0.064471</td>\n","      <td>0.764010</td>\n","      <td>0.769658</td>\n","      <td>0.762995</td>\n","      <td>0.926626</td>\n","      <td>0.926626</td>\n","      <td>0.926626</td>\n","      <td>0.730594</td>\n","      <td>0.736980</td>\n","      <td>0.729611</td>\n","      <td>0.711995</td>\n","      <td>0.706943</td>\n","      <td>0.717121</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001100</td>\n","      <td>0.056448</td>\n","      <td>0.762605</td>\n","      <td>0.742811</td>\n","      <td>0.787033</td>\n","      <td>0.920424</td>\n","      <td>0.920424</td>\n","      <td>0.920424</td>\n","      <td>0.729604</td>\n","      <td>0.704551</td>\n","      <td>0.760017</td>\n","      <td>0.708068</td>\n","      <td>0.665526</td>\n","      <td>0.756420</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.066502</td>\n","      <td>0.760329</td>\n","      <td>0.775275</td>\n","      <td>0.747906</td>\n","      <td>0.926973</td>\n","      <td>0.926973</td>\n","      <td>0.926973</td>\n","      <td>0.726407</td>\n","      <td>0.744111</td>\n","      <td>0.711645</td>\n","      <td>0.713809</td>\n","      <td>0.720746</td>\n","      <td>0.707004</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.063706</td>\n","      <td>0.756284</td>\n","      <td>0.752964</td>\n","      <td>0.760943</td>\n","      <td>0.923612</td>\n","      <td>0.923612</td>\n","      <td>0.923612</td>\n","      <td>0.721920</td>\n","      <td>0.717420</td>\n","      <td>0.727978</td>\n","      <td>0.707271</td>\n","      <td>0.692250</td>\n","      <td>0.722957</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000900</td>\n","      <td>0.063770</td>\n","      <td>0.754974</td>\n","      <td>0.761948</td>\n","      <td>0.753337</td>\n","      <td>0.921931</td>\n","      <td>0.921931</td>\n","      <td>0.921931</td>\n","      <td>0.720463</td>\n","      <td>0.727744</td>\n","      <td>0.719400</td>\n","      <td>0.700661</td>\n","      <td>0.680734</td>\n","      <td>0.721790</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.062027</td>\n","      <td>0.760884</td>\n","      <td>0.762793</td>\n","      <td>0.760937</td>\n","      <td>0.924249</td>\n","      <td>0.924249</td>\n","      <td>0.924249</td>\n","      <td>0.727300</td>\n","      <td>0.729005</td>\n","      <td>0.727881</td>\n","      <td>0.710984</td>\n","      <td>0.698311</td>\n","      <td>0.724125</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.061341</td>\n","      <td>0.758828</td>\n","      <td>0.757264</td>\n","      <td>0.762395</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.724987</td>\n","      <td>0.722486</td>\n","      <td>0.729819</td>\n","      <td>0.709101</td>\n","      <td>0.692908</td>\n","      <td>0.726070</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.060718</td>\n","      <td>0.760703</td>\n","      <td>0.756910</td>\n","      <td>0.767480</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.727206</td>\n","      <td>0.721720</td>\n","      <td>0.736161</td>\n","      <td>0.708552</td>\n","      <td>0.683925</td>\n","      <td>0.735019</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.062026</td>\n","      <td>0.759456</td>\n","      <td>0.758210</td>\n","      <td>0.763547</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.725727</td>\n","      <td>0.723397</td>\n","      <td>0.731368</td>\n","      <td>0.708058</td>\n","      <td>0.687431</td>\n","      <td>0.729961</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000500</td>\n","      <td>0.059573</td>\n","      <td>0.759952</td>\n","      <td>0.750476</td>\n","      <td>0.772189</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.726435</td>\n","      <td>0.714134</td>\n","      <td>0.741939</td>\n","      <td>0.708295</td>\n","      <td>0.679785</td>\n","      <td>0.739300</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.060289</td>\n","      <td>0.759856</td>\n","      <td>0.751329</td>\n","      <td>0.770958</td>\n","      <td>0.922221</td>\n","      <td>0.922221</td>\n","      <td>0.922221</td>\n","      <td>0.726252</td>\n","      <td>0.715113</td>\n","      <td>0.740378</td>\n","      <td>0.708497</td>\n","      <td>0.681149</td>\n","      <td>0.738132</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.060871</td>\n","      <td>0.759067</td>\n","      <td>0.751761</td>\n","      <td>0.768782</td>\n","      <td>0.922453</td>\n","      <td>0.922453</td>\n","      <td>0.922453</td>\n","      <td>0.725298</td>\n","      <td>0.715691</td>\n","      <td>0.737703</td>\n","      <td>0.708021</td>\n","      <td>0.682936</td>\n","      <td>0.735019</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000600</td>\n","      <td>0.060915</td>\n","      <td>0.759125</td>\n","      <td>0.751853</td>\n","      <td>0.768792</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.725361</td>\n","      <td>0.715797</td>\n","      <td>0.737703</td>\n","      <td>0.708154</td>\n","      <td>0.683183</td>\n","      <td>0.735019</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7591254923039971, 'eval_macro_precision': 0.7518525172849293, 'eval_macro_recall': 0.768791592269466, 'eval_micro_f1': 0.9225107221513852, 'eval_micro_precision': 0.9225107221513852, 'eval_micro_recall': 0.9225107221513852, 'eval_macro_f1_no_o': 0.7253612124732433, 'eval_macro_precision_no_o': 0.7157974769433061, 'eval_macro_recall_no_o': 0.7377026035843307, 'eval_micro_f1_no_o': 0.708153701968135, 'eval_micro_precision_no_o': 0.6831826401446655, 'eval_micro_recall_no_o': 0.7350194552529183, 'eval_loss': 0.06091525146942233, 'eval_runtime': 26.377, 'eval_samples_per_second': 4.587, 'eval_steps_per_second': 1.175, 'epoch': 18.0}\n","Accuracy for fold  6 :  0.708153701968135  --  0.9225107221513852\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #7\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 36:05, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.028900</td>\n","      <td>0.104986</td>\n","      <td>0.747720</td>\n","      <td>0.719458</td>\n","      <td>0.788208</td>\n","      <td>0.912194</td>\n","      <td>0.912194</td>\n","      <td>0.912194</td>\n","      <td>0.712555</td>\n","      <td>0.676055</td>\n","      <td>0.763170</td>\n","      <td>0.680090</td>\n","      <td>0.613914</td>\n","      <td>0.762257</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.013000</td>\n","      <td>0.066607</td>\n","      <td>0.762737</td>\n","      <td>0.733350</td>\n","      <td>0.801781</td>\n","      <td>0.918454</td>\n","      <td>0.918454</td>\n","      <td>0.918454</td>\n","      <td>0.729806</td>\n","      <td>0.692501</td>\n","      <td>0.778267</td>\n","      <td>0.705675</td>\n","      <td>0.644974</td>\n","      <td>0.778988</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004400</td>\n","      <td>0.061141</td>\n","      <td>0.760387</td>\n","      <td>0.724691</td>\n","      <td>0.803920</td>\n","      <td>0.919961</td>\n","      <td>0.919961</td>\n","      <td>0.919961</td>\n","      <td>0.726995</td>\n","      <td>0.682912</td>\n","      <td>0.780150</td>\n","      <td>0.708288</td>\n","      <td>0.657124</td>\n","      <td>0.768093</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003000</td>\n","      <td>0.059268</td>\n","      <td>0.774685</td>\n","      <td>0.771564</td>\n","      <td>0.781358</td>\n","      <td>0.928364</td>\n","      <td>0.928364</td>\n","      <td>0.928364</td>\n","      <td>0.742928</td>\n","      <td>0.738779</td>\n","      <td>0.751217</td>\n","      <td>0.722095</td>\n","      <td>0.709617</td>\n","      <td>0.735019</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002400</td>\n","      <td>0.066045</td>\n","      <td>0.761024</td>\n","      <td>0.777168</td>\n","      <td>0.754623</td>\n","      <td>0.926510</td>\n","      <td>0.926510</td>\n","      <td>0.926510</td>\n","      <td>0.727184</td>\n","      <td>0.746254</td>\n","      <td>0.719482</td>\n","      <td>0.709829</td>\n","      <td>0.715869</td>\n","      <td>0.703891</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001500</td>\n","      <td>0.064930</td>\n","      <td>0.751642</td>\n","      <td>0.730538</td>\n","      <td>0.778025</td>\n","      <td>0.918686</td>\n","      <td>0.918686</td>\n","      <td>0.918686</td>\n","      <td>0.716944</td>\n","      <td>0.690700</td>\n","      <td>0.749315</td>\n","      <td>0.698454</td>\n","      <td>0.662823</td>\n","      <td>0.738132</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001200</td>\n","      <td>0.059179</td>\n","      <td>0.758400</td>\n","      <td>0.732060</td>\n","      <td>0.791931</td>\n","      <td>0.920424</td>\n","      <td>0.920424</td>\n","      <td>0.920424</td>\n","      <td>0.724559</td>\n","      <td>0.691614</td>\n","      <td>0.765834</td>\n","      <td>0.705691</td>\n","      <td>0.658685</td>\n","      <td>0.759922</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.058222</td>\n","      <td>0.758700</td>\n","      <td>0.726830</td>\n","      <td>0.797384</td>\n","      <td>0.918164</td>\n","      <td>0.918164</td>\n","      <td>0.918164</td>\n","      <td>0.725163</td>\n","      <td>0.685458</td>\n","      <td>0.772740</td>\n","      <td>0.702093</td>\n","      <td>0.649884</td>\n","      <td>0.763424</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.060114</td>\n","      <td>0.763961</td>\n","      <td>0.747122</td>\n","      <td>0.789113</td>\n","      <td>0.921583</td>\n","      <td>0.921583</td>\n","      <td>0.921583</td>\n","      <td>0.731028</td>\n","      <td>0.709484</td>\n","      <td>0.762228</td>\n","      <td>0.709854</td>\n","      <td>0.668385</td>\n","      <td>0.756809</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000700</td>\n","      <td>0.060664</td>\n","      <td>0.763957</td>\n","      <td>0.751771</td>\n","      <td>0.782124</td>\n","      <td>0.922395</td>\n","      <td>0.922395</td>\n","      <td>0.922395</td>\n","      <td>0.730941</td>\n","      <td>0.715199</td>\n","      <td>0.753632</td>\n","      <td>0.709272</td>\n","      <td>0.675105</td>\n","      <td>0.747082</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.060733</td>\n","      <td>0.771787</td>\n","      <td>0.765178</td>\n","      <td>0.781453</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.739702</td>\n","      <td>0.731145</td>\n","      <td>0.751816</td>\n","      <td>0.720196</td>\n","      <td>0.699963</td>\n","      <td>0.741634</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.062074</td>\n","      <td>0.770255</td>\n","      <td>0.765438</td>\n","      <td>0.777809</td>\n","      <td>0.926394</td>\n","      <td>0.926394</td>\n","      <td>0.926394</td>\n","      <td>0.737939</td>\n","      <td>0.731568</td>\n","      <td>0.747497</td>\n","      <td>0.717210</td>\n","      <td>0.699187</td>\n","      <td>0.736187</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.058907</td>\n","      <td>0.770863</td>\n","      <td>0.753592</td>\n","      <td>0.791160</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.738769</td>\n","      <td>0.717257</td>\n","      <td>0.763788</td>\n","      <td>0.717549</td>\n","      <td>0.686323</td>\n","      <td>0.751751</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.061393</td>\n","      <td>0.772979</td>\n","      <td>0.759237</td>\n","      <td>0.789672</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.741135</td>\n","      <td>0.723891</td>\n","      <td>0.761802</td>\n","      <td>0.720418</td>\n","      <td>0.692253</td>\n","      <td>0.750973</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000500</td>\n","      <td>0.061505</td>\n","      <td>0.775177</td>\n","      <td>0.771975</td>\n","      <td>0.781545</td>\n","      <td>0.928017</td>\n","      <td>0.928017</td>\n","      <td>0.928017</td>\n","      <td>0.743557</td>\n","      <td>0.739058</td>\n","      <td>0.751742</td>\n","      <td>0.723759</td>\n","      <td>0.705318</td>\n","      <td>0.743191</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000500</td>\n","      <td>0.061415</td>\n","      <td>0.770815</td>\n","      <td>0.763508</td>\n","      <td>0.781161</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.738524</td>\n","      <td>0.729086</td>\n","      <td>0.751498</td>\n","      <td>0.720829</td>\n","      <td>0.699086</td>\n","      <td>0.743969</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.061459</td>\n","      <td>0.770720</td>\n","      <td>0.759973</td>\n","      <td>0.784586</td>\n","      <td>0.926800</td>\n","      <td>0.926800</td>\n","      <td>0.926800</td>\n","      <td>0.738430</td>\n","      <td>0.724833</td>\n","      <td>0.755653</td>\n","      <td>0.720690</td>\n","      <td>0.695762</td>\n","      <td>0.747471</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.061459</td>\n","      <td>0.770613</td>\n","      <td>0.759790</td>\n","      <td>0.784577</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.738311</td>\n","      <td>0.724620</td>\n","      <td>0.755653</td>\n","      <td>0.720555</td>\n","      <td>0.695510</td>\n","      <td>0.747471</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7706125214277525, 'eval_macro_precision': 0.7597903911065992, 'eval_macro_recall': 0.7845767182180088, 'eval_micro_f1': 0.9267416251304046, 'eval_micro_precision': 0.9267416251304046, 'eval_micro_recall': 0.9267416251304046, 'eval_macro_f1_no_o': 0.7383108061205143, 'eval_macro_precision_no_o': 0.7246202092581867, 'eval_macro_recall_no_o': 0.7556532247365918, 'eval_micro_f1_no_o': 0.7205551387846961, 'eval_micro_precision_no_o': 0.6955104996379435, 'eval_micro_recall_no_o': 0.7474708171206226, 'eval_loss': 0.0614586081347091, 'eval_runtime': 7.7627, 'eval_samples_per_second': 15.587, 'eval_steps_per_second': 3.993, 'epoch': 18.0}\n","Accuracy for fold  7 :  0.7205551387846961  --  0.9267416251304046\n","--------------------------------\n","Testing process has finished.\n","Train run #8\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 28:02, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.028300</td>\n","      <td>0.098339</td>\n","      <td>0.759667</td>\n","      <td>0.792424</td>\n","      <td>0.742490</td>\n","      <td>0.930103</td>\n","      <td>0.930103</td>\n","      <td>0.930103</td>\n","      <td>0.725178</td>\n","      <td>0.764230</td>\n","      <td>0.704293</td>\n","      <td>0.713999</td>\n","      <td>0.736755</td>\n","      <td>0.692607</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.010000</td>\n","      <td>0.072780</td>\n","      <td>0.751608</td>\n","      <td>0.721076</td>\n","      <td>0.796348</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.716060</td>\n","      <td>0.679199</td>\n","      <td>0.769477</td>\n","      <td>0.705202</td>\n","      <td>0.677050</td>\n","      <td>0.735798</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004500</td>\n","      <td>0.065391</td>\n","      <td>0.755690</td>\n","      <td>0.727595</td>\n","      <td>0.793877</td>\n","      <td>0.920540</td>\n","      <td>0.920540</td>\n","      <td>0.920540</td>\n","      <td>0.721339</td>\n","      <td>0.686346</td>\n","      <td>0.768047</td>\n","      <td>0.704607</td>\n","      <td>0.657673</td>\n","      <td>0.758755</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002800</td>\n","      <td>0.062420</td>\n","      <td>0.763393</td>\n","      <td>0.738262</td>\n","      <td>0.794602</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.729807</td>\n","      <td>0.699304</td>\n","      <td>0.767384</td>\n","      <td>0.715113</td>\n","      <td>0.687747</td>\n","      <td>0.744747</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002100</td>\n","      <td>0.063081</td>\n","      <td>0.751650</td>\n","      <td>0.723232</td>\n","      <td>0.794161</td>\n","      <td>0.918801</td>\n","      <td>0.918801</td>\n","      <td>0.918801</td>\n","      <td>0.716687</td>\n","      <td>0.681070</td>\n","      <td>0.768674</td>\n","      <td>0.697741</td>\n","      <td>0.646941</td>\n","      <td>0.757198</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.058939</td>\n","      <td>0.758377</td>\n","      <td>0.750761</td>\n","      <td>0.774270</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.723993</td>\n","      <td>0.714127</td>\n","      <td>0.743504</td>\n","      <td>0.704156</td>\n","      <td>0.681471</td>\n","      <td>0.728405</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.057205</td>\n","      <td>0.760121</td>\n","      <td>0.734399</td>\n","      <td>0.793804</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.726313</td>\n","      <td>0.694419</td>\n","      <td>0.767451</td>\n","      <td>0.708052</td>\n","      <td>0.667011</td>\n","      <td>0.754475</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.056890</td>\n","      <td>0.754391</td>\n","      <td>0.720043</td>\n","      <td>0.796540</td>\n","      <td>0.919497</td>\n","      <td>0.919497</td>\n","      <td>0.919497</td>\n","      <td>0.719907</td>\n","      <td>0.677418</td>\n","      <td>0.771427</td>\n","      <td>0.702334</td>\n","      <td>0.652000</td>\n","      <td>0.761089</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.057837</td>\n","      <td>0.757381</td>\n","      <td>0.731061</td>\n","      <td>0.788138</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.723087</td>\n","      <td>0.690831</td>\n","      <td>0.760489</td>\n","      <td>0.707641</td>\n","      <td>0.673104</td>\n","      <td>0.745914</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000700</td>\n","      <td>0.058108</td>\n","      <td>0.763830</td>\n","      <td>0.747314</td>\n","      <td>0.786688</td>\n","      <td>0.928191</td>\n","      <td>0.928191</td>\n","      <td>0.928191</td>\n","      <td>0.730084</td>\n","      <td>0.710100</td>\n","      <td>0.757458</td>\n","      <td>0.716645</td>\n","      <td>0.699518</td>\n","      <td>0.734630</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.062532</td>\n","      <td>0.763559</td>\n","      <td>0.729285</td>\n","      <td>0.804358</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.730417</td>\n","      <td>0.688512</td>\n","      <td>0.779889</td>\n","      <td>0.711289</td>\n","      <td>0.669413</td>\n","      <td>0.758755</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.058427</td>\n","      <td>0.754231</td>\n","      <td>0.712572</td>\n","      <td>0.807823</td>\n","      <td>0.919033</td>\n","      <td>0.919033</td>\n","      <td>0.919033</td>\n","      <td>0.719748</td>\n","      <td>0.668310</td>\n","      <td>0.785010</td>\n","      <td>0.703651</td>\n","      <td>0.646159</td>\n","      <td>0.772374</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.060794</td>\n","      <td>0.761128</td>\n","      <td>0.731650</td>\n","      <td>0.797452</td>\n","      <td>0.923496</td>\n","      <td>0.923496</td>\n","      <td>0.923496</td>\n","      <td>0.727485</td>\n","      <td>0.691267</td>\n","      <td>0.771650</td>\n","      <td>0.713737</td>\n","      <td>0.673455</td>\n","      <td>0.759144</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.060703</td>\n","      <td>0.761688</td>\n","      <td>0.731704</td>\n","      <td>0.800298</td>\n","      <td>0.923206</td>\n","      <td>0.923206</td>\n","      <td>0.923206</td>\n","      <td>0.728105</td>\n","      <td>0.691284</td>\n","      <td>0.774948</td>\n","      <td>0.710916</td>\n","      <td>0.670576</td>\n","      <td>0.756420</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000500</td>\n","      <td>0.061000</td>\n","      <td>0.759957</td>\n","      <td>0.734885</td>\n","      <td>0.791577</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.726013</td>\n","      <td>0.695179</td>\n","      <td>0.764455</td>\n","      <td>0.711283</td>\n","      <td>0.675893</td>\n","      <td>0.750584</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.061274</td>\n","      <td>0.760932</td>\n","      <td>0.736291</td>\n","      <td>0.792029</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.727138</td>\n","      <td>0.696829</td>\n","      <td>0.764950</td>\n","      <td>0.712177</td>\n","      <td>0.677193</td>\n","      <td>0.750973</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.061391</td>\n","      <td>0.760847</td>\n","      <td>0.736593</td>\n","      <td>0.791460</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.727043</td>\n","      <td>0.697214</td>\n","      <td>0.764263</td>\n","      <td>0.712096</td>\n","      <td>0.677680</td>\n","      <td>0.750195</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.061390</td>\n","      <td>0.760799</td>\n","      <td>0.736515</td>\n","      <td>0.791460</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.726982</td>\n","      <td>0.697112</td>\n","      <td>0.764263</td>\n","      <td>0.711965</td>\n","      <td>0.677442</td>\n","      <td>0.750195</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7607987481629009, 'eval_macro_precision': 0.7365146540891193, 'eval_macro_recall': 0.7914603259976231, 'eval_micro_f1': 0.9241914918279819, 'eval_micro_precision': 0.9241914918279819, 'eval_micro_recall': 0.9241914918279819, 'eval_macro_f1_no_o': 0.7269816459902252, 'eval_macro_precision_no_o': 0.6971118123358807, 'eval_macro_recall_no_o': 0.7642629618251574, 'eval_micro_f1_no_o': 0.7119645494830132, 'eval_micro_precision_no_o': 0.6774420238931834, 'eval_micro_recall_no_o': 0.7501945525291829, 'eval_loss': 0.0613899573939209, 'eval_runtime': 26.3515, 'eval_samples_per_second': 4.592, 'eval_steps_per_second': 1.176, 'epoch': 18.0}\n","Accuracy for fold  8 :  0.7119645494830132  --  0.9241914918279819\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #9\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 41:53, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.028100</td>\n","      <td>0.099602</td>\n","      <td>0.756155</td>\n","      <td>0.718148</td>\n","      <td>0.801623</td>\n","      <td>0.917526</td>\n","      <td>0.917526</td>\n","      <td>0.917526</td>\n","      <td>0.722303</td>\n","      <td>0.675313</td>\n","      <td>0.777913</td>\n","      <td>0.702549</td>\n","      <td>0.648142</td>\n","      <td>0.766926</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.010100</td>\n","      <td>0.082050</td>\n","      <td>0.748853</td>\n","      <td>0.766511</td>\n","      <td>0.734055</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.713204</td>\n","      <td>0.733963</td>\n","      <td>0.695781</td>\n","      <td>0.693015</td>\n","      <td>0.696970</td>\n","      <td>0.689105</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004600</td>\n","      <td>0.071488</td>\n","      <td>0.750321</td>\n","      <td>0.744943</td>\n","      <td>0.758220</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.714819</td>\n","      <td>0.707805</td>\n","      <td>0.724767</td>\n","      <td>0.698900</td>\n","      <td>0.681583</td>\n","      <td>0.717121</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003100</td>\n","      <td>0.066806</td>\n","      <td>0.758663</td>\n","      <td>0.749046</td>\n","      <td>0.770934</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.724615</td>\n","      <td>0.712929</td>\n","      <td>0.739397</td>\n","      <td>0.706986</td>\n","      <td>0.695669</td>\n","      <td>0.718677</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002200</td>\n","      <td>0.057987</td>\n","      <td>0.749775</td>\n","      <td>0.716056</td>\n","      <td>0.799129</td>\n","      <td>0.908601</td>\n","      <td>0.908601</td>\n","      <td>0.908601</td>\n","      <td>0.715576</td>\n","      <td>0.672007</td>\n","      <td>0.777171</td>\n","      <td>0.682877</td>\n","      <td>0.606463</td>\n","      <td>0.781323</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.056891</td>\n","      <td>0.749863</td>\n","      <td>0.695058</td>\n","      <td>0.822481</td>\n","      <td>0.915962</td>\n","      <td>0.915962</td>\n","      <td>0.915962</td>\n","      <td>0.715083</td>\n","      <td>0.647918</td>\n","      <td>0.802905</td>\n","      <td>0.701226</td>\n","      <td>0.637580</td>\n","      <td>0.778988</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001100</td>\n","      <td>0.054571</td>\n","      <td>0.770158</td>\n","      <td>0.743832</td>\n","      <td>0.801187</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.737989</td>\n","      <td>0.705223</td>\n","      <td>0.776190</td>\n","      <td>0.713404</td>\n","      <td>0.668937</td>\n","      <td>0.764202</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.059836</td>\n","      <td>0.769825</td>\n","      <td>0.759738</td>\n","      <td>0.782783</td>\n","      <td>0.925177</td>\n","      <td>0.925177</td>\n","      <td>0.925177</td>\n","      <td>0.737443</td>\n","      <td>0.724475</td>\n","      <td>0.753742</td>\n","      <td>0.713219</td>\n","      <td>0.685571</td>\n","      <td>0.743191</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.057761</td>\n","      <td>0.774177</td>\n","      <td>0.754789</td>\n","      <td>0.796134</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.742552</td>\n","      <td>0.718495</td>\n","      <td>0.769580</td>\n","      <td>0.719155</td>\n","      <td>0.686351</td>\n","      <td>0.755253</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.057643</td>\n","      <td>0.763687</td>\n","      <td>0.736291</td>\n","      <td>0.795753</td>\n","      <td>0.922279</td>\n","      <td>0.922279</td>\n","      <td>0.922279</td>\n","      <td>0.730537</td>\n","      <td>0.696663</td>\n","      <td>0.769816</td>\n","      <td>0.708865</td>\n","      <td>0.667239</td>\n","      <td>0.756031</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.064790</td>\n","      <td>0.765116</td>\n","      <td>0.762946</td>\n","      <td>0.768799</td>\n","      <td>0.926104</td>\n","      <td>0.926104</td>\n","      <td>0.926104</td>\n","      <td>0.731948</td>\n","      <td>0.729026</td>\n","      <td>0.736633</td>\n","      <td>0.712339</td>\n","      <td>0.702764</td>\n","      <td>0.722179</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.055206</td>\n","      <td>0.766147</td>\n","      <td>0.738300</td>\n","      <td>0.803096</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.733500</td>\n","      <td>0.698790</td>\n","      <td>0.778769</td>\n","      <td>0.710983</td>\n","      <td>0.663520</td>\n","      <td>0.765759</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.061654</td>\n","      <td>0.767741</td>\n","      <td>0.756606</td>\n","      <td>0.780526</td>\n","      <td>0.926394</td>\n","      <td>0.926394</td>\n","      <td>0.926394</td>\n","      <td>0.734985</td>\n","      <td>0.721220</td>\n","      <td>0.750666</td>\n","      <td>0.716667</td>\n","      <td>0.698155</td>\n","      <td>0.736187</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.058905</td>\n","      <td>0.769986</td>\n","      <td>0.750154</td>\n","      <td>0.792207</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.737668</td>\n","      <td>0.713242</td>\n","      <td>0.764861</td>\n","      <td>0.718168</td>\n","      <td>0.688437</td>\n","      <td>0.750584</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.059589</td>\n","      <td>0.770905</td>\n","      <td>0.752823</td>\n","      <td>0.791130</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.738740</td>\n","      <td>0.716377</td>\n","      <td>0.763583</td>\n","      <td>0.719076</td>\n","      <td>0.689778</td>\n","      <td>0.750973</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.054856</td>\n","      <td>0.772623</td>\n","      <td>0.751365</td>\n","      <td>0.797803</td>\n","      <td>0.925756</td>\n","      <td>0.925756</td>\n","      <td>0.925756</td>\n","      <td>0.740731</td>\n","      <td>0.714215</td>\n","      <td>0.771788</td>\n","      <td>0.721865</td>\n","      <td>0.683351</td>\n","      <td>0.764981</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.054847</td>\n","      <td>0.772897</td>\n","      <td>0.751817</td>\n","      <td>0.797807</td>\n","      <td>0.925756</td>\n","      <td>0.925756</td>\n","      <td>0.925756</td>\n","      <td>0.741055</td>\n","      <td>0.714776</td>\n","      <td>0.771769</td>\n","      <td>0.721793</td>\n","      <td>0.683844</td>\n","      <td>0.764202</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000600</td>\n","      <td>0.054836</td>\n","      <td>0.772999</td>\n","      <td>0.751876</td>\n","      <td>0.797975</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.741170</td>\n","      <td>0.714833</td>\n","      <td>0.771965</td>\n","      <td>0.722028</td>\n","      <td>0.683954</td>\n","      <td>0.764591</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7729994323104566, 'eval_macro_precision': 0.7518756832657921, 'eval_macro_recall': 0.797974724510504, 'eval_micro_f1': 0.9258143039295236, 'eval_micro_precision': 0.9258143039295236, 'eval_micro_recall': 0.9258143039295236, 'eval_macro_f1_no_o': 0.7411697889828773, 'eval_macro_precision_no_o': 0.7148328744792606, 'eval_macro_recall_no_o': 0.7719652454256978, 'eval_micro_f1_no_o': 0.7220282932206504, 'eval_micro_precision_no_o': 0.683954054994779, 'eval_micro_recall_no_o': 0.7645914396887159, 'eval_loss': 0.054836042357793824, 'eval_runtime': 26.4828, 'eval_samples_per_second': 4.569, 'eval_steps_per_second': 1.171, 'epoch': 18.0}\n","Accuracy for fold  9 :  0.7220282932206504  --  0.9258143039295236\n","--------------------------------\n","Testing process has finished.\n","Train run #10\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 41:38, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.028900</td>\n","      <td>0.103690</td>\n","      <td>0.736194</td>\n","      <td>0.735661</td>\n","      <td>0.752738</td>\n","      <td>0.914628</td>\n","      <td>0.914628</td>\n","      <td>0.914628</td>\n","      <td>0.698795</td>\n","      <td>0.695833</td>\n","      <td>0.720370</td>\n","      <td>0.675184</td>\n","      <td>0.628055</td>\n","      <td>0.729961</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.010300</td>\n","      <td>0.075696</td>\n","      <td>0.756958</td>\n","      <td>0.725326</td>\n","      <td>0.795954</td>\n","      <td>0.916715</td>\n","      <td>0.916715</td>\n","      <td>0.916715</td>\n","      <td>0.723358</td>\n","      <td>0.684015</td>\n","      <td>0.771219</td>\n","      <td>0.699552</td>\n","      <td>0.648918</td>\n","      <td>0.758755</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004700</td>\n","      <td>0.062217</td>\n","      <td>0.771720</td>\n","      <td>0.766091</td>\n","      <td>0.782318</td>\n","      <td>0.926626</td>\n","      <td>0.926626</td>\n","      <td>0.926626</td>\n","      <td>0.739627</td>\n","      <td>0.731787</td>\n","      <td>0.753245</td>\n","      <td>0.722295</td>\n","      <td>0.692747</td>\n","      <td>0.754475</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003000</td>\n","      <td>0.063841</td>\n","      <td>0.763788</td>\n","      <td>0.767069</td>\n","      <td>0.762697</td>\n","      <td>0.924655</td>\n","      <td>0.924655</td>\n","      <td>0.924655</td>\n","      <td>0.730477</td>\n","      <td>0.733648</td>\n","      <td>0.729855</td>\n","      <td>0.707739</td>\n","      <td>0.692079</td>\n","      <td>0.724125</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002000</td>\n","      <td>0.059261</td>\n","      <td>0.775870</td>\n","      <td>0.768388</td>\n","      <td>0.787424</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.744484</td>\n","      <td>0.734544</td>\n","      <td>0.759156</td>\n","      <td>0.727137</td>\n","      <td>0.698709</td>\n","      <td>0.757977</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.057804</td>\n","      <td>0.774629</td>\n","      <td>0.750201</td>\n","      <td>0.803363</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.743156</td>\n","      <td>0.712817</td>\n","      <td>0.778478</td>\n","      <td>0.725178</td>\n","      <td>0.684029</td>\n","      <td>0.771595</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001100</td>\n","      <td>0.059286</td>\n","      <td>0.761305</td>\n","      <td>0.746997</td>\n","      <td>0.780910</td>\n","      <td>0.921352</td>\n","      <td>0.921352</td>\n","      <td>0.921352</td>\n","      <td>0.727828</td>\n","      <td>0.709201</td>\n","      <td>0.752590</td>\n","      <td>0.705432</td>\n","      <td>0.663580</td>\n","      <td>0.752918</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.058991</td>\n","      <td>0.776185</td>\n","      <td>0.766887</td>\n","      <td>0.788431</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.744938</td>\n","      <td>0.732863</td>\n","      <td>0.760433</td>\n","      <td>0.726730</td>\n","      <td>0.697958</td>\n","      <td>0.757977</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.056078</td>\n","      <td>0.770801</td>\n","      <td>0.765763</td>\n","      <td>0.781757</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.738508</td>\n","      <td>0.731238</td>\n","      <td>0.752659</td>\n","      <td>0.718738</td>\n","      <td>0.686879</td>\n","      <td>0.753696</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.060060</td>\n","      <td>0.779033</td>\n","      <td>0.783666</td>\n","      <td>0.778343</td>\n","      <td>0.930045</td>\n","      <td>0.930045</td>\n","      <td>0.930045</td>\n","      <td>0.747936</td>\n","      <td>0.752884</td>\n","      <td>0.747586</td>\n","      <td>0.730615</td>\n","      <td>0.719186</td>\n","      <td>0.742412</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000600</td>\n","      <td>0.060501</td>\n","      <td>0.759697</td>\n","      <td>0.721580</td>\n","      <td>0.807815</td>\n","      <td>0.918338</td>\n","      <td>0.918338</td>\n","      <td>0.918338</td>\n","      <td>0.726304</td>\n","      <td>0.679007</td>\n","      <td>0.785170</td>\n","      <td>0.704840</td>\n","      <td>0.647346</td>\n","      <td>0.773541</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.058502</td>\n","      <td>0.773319</td>\n","      <td>0.758775</td>\n","      <td>0.791226</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.741350</td>\n","      <td>0.723113</td>\n","      <td>0.763490</td>\n","      <td>0.725410</td>\n","      <td>0.695854</td>\n","      <td>0.757588</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.059194</td>\n","      <td>0.775989</td>\n","      <td>0.764960</td>\n","      <td>0.790094</td>\n","      <td>0.928712</td>\n","      <td>0.928712</td>\n","      <td>0.928712</td>\n","      <td>0.744445</td>\n","      <td>0.730546</td>\n","      <td>0.761919</td>\n","      <td>0.728502</td>\n","      <td>0.703919</td>\n","      <td>0.754864</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.059672</td>\n","      <td>0.775498</td>\n","      <td>0.767479</td>\n","      <td>0.786171</td>\n","      <td>0.928422</td>\n","      <td>0.928422</td>\n","      <td>0.928422</td>\n","      <td>0.743903</td>\n","      <td>0.733638</td>\n","      <td>0.757252</td>\n","      <td>0.726621</td>\n","      <td>0.704828</td>\n","      <td>0.749805</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.058810</td>\n","      <td>0.777603</td>\n","      <td>0.766158</td>\n","      <td>0.792406</td>\n","      <td>0.928886</td>\n","      <td>0.928886</td>\n","      <td>0.928886</td>\n","      <td>0.746304</td>\n","      <td>0.731790</td>\n","      <td>0.764720</td>\n","      <td>0.729856</td>\n","      <td>0.702411</td>\n","      <td>0.759533</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.058893</td>\n","      <td>0.777423</td>\n","      <td>0.765951</td>\n","      <td>0.792317</td>\n","      <td>0.928944</td>\n","      <td>0.928944</td>\n","      <td>0.928944</td>\n","      <td>0.746077</td>\n","      <td>0.731525</td>\n","      <td>0.764604</td>\n","      <td>0.729720</td>\n","      <td>0.702158</td>\n","      <td>0.759533</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.058903</td>\n","      <td>0.777162</td>\n","      <td>0.765965</td>\n","      <td>0.791804</td>\n","      <td>0.928944</td>\n","      <td>0.928944</td>\n","      <td>0.928944</td>\n","      <td>0.745772</td>\n","      <td>0.731553</td>\n","      <td>0.763995</td>\n","      <td>0.729619</td>\n","      <td>0.702304</td>\n","      <td>0.759144</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.058918</td>\n","      <td>0.777483</td>\n","      <td>0.766316</td>\n","      <td>0.792031</td>\n","      <td>0.928944</td>\n","      <td>0.928944</td>\n","      <td>0.928944</td>\n","      <td>0.746147</td>\n","      <td>0.731962</td>\n","      <td>0.764259</td>\n","      <td>0.729619</td>\n","      <td>0.702304</td>\n","      <td>0.759144</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7774832232498049, 'eval_macro_precision': 0.7663159120156193, 'eval_macro_recall': 0.7920310444212765, 'eval_micro_f1': 0.9289440129824968, 'eval_micro_precision': 0.9289440129824968, 'eval_micro_recall': 0.9289440129824968, 'eval_macro_f1_no_o': 0.7461469794795964, 'eval_macro_precision_no_o': 0.7319623744630969, 'eval_macro_recall_no_o': 0.7642591368606894, 'eval_micro_f1_no_o': 0.7296185489902768, 'eval_micro_precision_no_o': 0.7023038156947444, 'eval_micro_recall_no_o': 0.7591439688715953, 'eval_loss': 0.058917677837403176, 'eval_runtime': 26.3624, 'eval_samples_per_second': 4.59, 'eval_steps_per_second': 1.176, 'epoch': 18.0}\n","Accuracy for fold  10 :  0.7296185489902768  --  0.9289440129824968\n","--------------------------------\n","Testing process has finished.\n","Train run #11\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 42:23, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.028200</td>\n","      <td>0.108016</td>\n","      <td>0.718473</td>\n","      <td>0.691098</td>\n","      <td>0.766386</td>\n","      <td>0.916251</td>\n","      <td>0.916251</td>\n","      <td>0.916251</td>\n","      <td>0.678108</td>\n","      <td>0.644306</td>\n","      <td>0.735826</td>\n","      <td>0.680672</td>\n","      <td>0.641529</td>\n","      <td>0.724903</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009900</td>\n","      <td>0.089514</td>\n","      <td>0.738627</td>\n","      <td>0.719556</td>\n","      <td>0.777580</td>\n","      <td>0.909760</td>\n","      <td>0.909760</td>\n","      <td>0.909760</td>\n","      <td>0.702334</td>\n","      <td>0.677123</td>\n","      <td>0.750635</td>\n","      <td>0.672432</td>\n","      <td>0.615310</td>\n","      <td>0.741245</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004600</td>\n","      <td>0.086505</td>\n","      <td>0.719619</td>\n","      <td>0.766084</td>\n","      <td>0.696987</td>\n","      <td>0.914744</td>\n","      <td>0.914744</td>\n","      <td>0.914744</td>\n","      <td>0.679787</td>\n","      <td>0.734332</td>\n","      <td>0.653045</td>\n","      <td>0.660496</td>\n","      <td>0.668660</td>\n","      <td>0.652529</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002800</td>\n","      <td>0.063611</td>\n","      <td>0.759807</td>\n","      <td>0.796556</td>\n","      <td>0.731433</td>\n","      <td>0.928596</td>\n","      <td>0.928596</td>\n","      <td>0.928596</td>\n","      <td>0.725586</td>\n","      <td>0.769101</td>\n","      <td>0.691836</td>\n","      <td>0.714058</td>\n","      <td>0.731240</td>\n","      <td>0.697665</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002000</td>\n","      <td>0.063189</td>\n","      <td>0.757908</td>\n","      <td>0.764463</td>\n","      <td>0.756096</td>\n","      <td>0.927031</td>\n","      <td>0.927031</td>\n","      <td>0.927031</td>\n","      <td>0.723426</td>\n","      <td>0.731096</td>\n","      <td>0.721291</td>\n","      <td>0.711059</td>\n","      <td>0.711613</td>\n","      <td>0.710506</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.060490</td>\n","      <td>0.755745</td>\n","      <td>0.757042</td>\n","      <td>0.761012</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.720978</td>\n","      <td>0.722008</td>\n","      <td>0.727605</td>\n","      <td>0.712701</td>\n","      <td>0.700903</td>\n","      <td>0.724903</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001100</td>\n","      <td>0.058498</td>\n","      <td>0.758029</td>\n","      <td>0.748014</td>\n","      <td>0.779115</td>\n","      <td>0.923786</td>\n","      <td>0.923786</td>\n","      <td>0.923786</td>\n","      <td>0.723830</td>\n","      <td>0.710794</td>\n","      <td>0.749758</td>\n","      <td>0.710385</td>\n","      <td>0.679701</td>\n","      <td>0.743969</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.063515</td>\n","      <td>0.758906</td>\n","      <td>0.753967</td>\n","      <td>0.767883</td>\n","      <td>0.922395</td>\n","      <td>0.922395</td>\n","      <td>0.922395</td>\n","      <td>0.725045</td>\n","      <td>0.718131</td>\n","      <td>0.736654</td>\n","      <td>0.706058</td>\n","      <td>0.679626</td>\n","      <td>0.734630</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.064779</td>\n","      <td>0.762837</td>\n","      <td>0.766613</td>\n","      <td>0.761232</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.729452</td>\n","      <td>0.733389</td>\n","      <td>0.728043</td>\n","      <td>0.709665</td>\n","      <td>0.698305</td>\n","      <td>0.721401</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000700</td>\n","      <td>0.064058</td>\n","      <td>0.765314</td>\n","      <td>0.743064</td>\n","      <td>0.791896</td>\n","      <td>0.922627</td>\n","      <td>0.922627</td>\n","      <td>0.922627</td>\n","      <td>0.732523</td>\n","      <td>0.704756</td>\n","      <td>0.765305</td>\n","      <td>0.713031</td>\n","      <td>0.673117</td>\n","      <td>0.757977</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.062986</td>\n","      <td>0.767127</td>\n","      <td>0.762716</td>\n","      <td>0.773048</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.734365</td>\n","      <td>0.728451</td>\n","      <td>0.742032</td>\n","      <td>0.715287</td>\n","      <td>0.696936</td>\n","      <td>0.734630</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000600</td>\n","      <td>0.053808</td>\n","      <td>0.764636</td>\n","      <td>0.738505</td>\n","      <td>0.801020</td>\n","      <td>0.917352</td>\n","      <td>0.917352</td>\n","      <td>0.917352</td>\n","      <td>0.732248</td>\n","      <td>0.698706</td>\n","      <td>0.777641</td>\n","      <td>0.706214</td>\n","      <td>0.644809</td>\n","      <td>0.780545</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.058464</td>\n","      <td>0.768203</td>\n","      <td>0.759965</td>\n","      <td>0.780452</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.735881</td>\n","      <td>0.724823</td>\n","      <td>0.751590</td>\n","      <td>0.712593</td>\n","      <td>0.679859</td>\n","      <td>0.748638</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.061318</td>\n","      <td>0.771001</td>\n","      <td>0.767459</td>\n","      <td>0.777695</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.739010</td>\n","      <td>0.733798</td>\n","      <td>0.747885</td>\n","      <td>0.717151</td>\n","      <td>0.691863</td>\n","      <td>0.744358</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.061970</td>\n","      <td>0.769193</td>\n","      <td>0.768460</td>\n","      <td>0.772776</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.736912</td>\n","      <td>0.735150</td>\n","      <td>0.741987</td>\n","      <td>0.714932</td>\n","      <td>0.693489</td>\n","      <td>0.737743</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.062160</td>\n","      <td>0.769782</td>\n","      <td>0.768663</td>\n","      <td>0.773870</td>\n","      <td>0.924829</td>\n","      <td>0.924829</td>\n","      <td>0.924829</td>\n","      <td>0.737594</td>\n","      <td>0.735354</td>\n","      <td>0.743287</td>\n","      <td>0.715389</td>\n","      <td>0.693319</td>\n","      <td>0.738911</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.062081</td>\n","      <td>0.768967</td>\n","      <td>0.768648</td>\n","      <td>0.772142</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.736652</td>\n","      <td>0.735402</td>\n","      <td>0.741226</td>\n","      <td>0.714474</td>\n","      <td>0.693661</td>\n","      <td>0.736576</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.062096</td>\n","      <td>0.768967</td>\n","      <td>0.768648</td>\n","      <td>0.772142</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.736652</td>\n","      <td>0.735402</td>\n","      <td>0.741226</td>\n","      <td>0.714474</td>\n","      <td>0.693661</td>\n","      <td>0.736576</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7689667323721603, 'eval_macro_precision': 0.7686483863834799, 'eval_macro_recall': 0.772142108131126, 'eval_micro_f1': 0.9247131100034774, 'eval_micro_precision': 0.9247131100034774, 'eval_micro_recall': 0.9247131100034774, 'eval_macro_f1_no_o': 0.736652201952646, 'eval_macro_precision_no_o': 0.7354024634026429, 'eval_macro_recall_no_o': 0.7412256311924792, 'eval_micro_f1_no_o': 0.7144744291375733, 'eval_micro_precision_no_o': 0.69366068156834, 'eval_micro_recall_no_o': 0.7365758754863814, 'eval_loss': 0.062095801227544205, 'eval_runtime': 26.4966, 'eval_samples_per_second': 4.567, 'eval_steps_per_second': 1.17, 'epoch': 18.0}\n","Accuracy for fold  11 :  0.7144744291375733  --  0.9247131100034774\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #12\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 39:20, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.027900</td>\n","      <td>0.100469</td>\n","      <td>0.760561</td>\n","      <td>0.790173</td>\n","      <td>0.738448</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.726584</td>\n","      <td>0.761464</td>\n","      <td>0.700452</td>\n","      <td>0.704863</td>\n","      <td>0.713432</td>\n","      <td>0.696498</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.010500</td>\n","      <td>0.077564</td>\n","      <td>0.747329</td>\n","      <td>0.789174</td>\n","      <td>0.722536</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.710920</td>\n","      <td>0.760377</td>\n","      <td>0.681354</td>\n","      <td>0.693609</td>\n","      <td>0.710151</td>\n","      <td>0.677821</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004800</td>\n","      <td>0.080482</td>\n","      <td>0.745070</td>\n","      <td>0.776230</td>\n","      <td>0.720880</td>\n","      <td>0.921410</td>\n","      <td>0.921410</td>\n","      <td>0.921410</td>\n","      <td>0.709093</td>\n","      <td>0.746639</td>\n","      <td>0.679661</td>\n","      <td>0.683611</td>\n","      <td>0.715928</td>\n","      <td>0.654086</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002800</td>\n","      <td>0.071652</td>\n","      <td>0.759159</td>\n","      <td>0.778469</td>\n","      <td>0.743298</td>\n","      <td>0.923264</td>\n","      <td>0.923264</td>\n","      <td>0.923264</td>\n","      <td>0.725377</td>\n","      <td>0.748008</td>\n","      <td>0.706769</td>\n","      <td>0.701816</td>\n","      <td>0.704430</td>\n","      <td>0.699222</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002100</td>\n","      <td>0.076656</td>\n","      <td>0.757393</td>\n","      <td>0.759481</td>\n","      <td>0.757283</td>\n","      <td>0.922163</td>\n","      <td>0.922163</td>\n","      <td>0.922163</td>\n","      <td>0.723344</td>\n","      <td>0.725468</td>\n","      <td>0.723527</td>\n","      <td>0.699250</td>\n","      <td>0.691663</td>\n","      <td>0.707004</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.002200</td>\n","      <td>0.066582</td>\n","      <td>0.750819</td>\n","      <td>0.748519</td>\n","      <td>0.757175</td>\n","      <td>0.920888</td>\n","      <td>0.920888</td>\n","      <td>0.920888</td>\n","      <td>0.715733</td>\n","      <td>0.712138</td>\n","      <td>0.724048</td>\n","      <td>0.698209</td>\n","      <td>0.677148</td>\n","      <td>0.720623</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001300</td>\n","      <td>0.059570</td>\n","      <td>0.751948</td>\n","      <td>0.736218</td>\n","      <td>0.773012</td>\n","      <td>0.921989</td>\n","      <td>0.921989</td>\n","      <td>0.921989</td>\n","      <td>0.716601</td>\n","      <td>0.696807</td>\n","      <td>0.742592</td>\n","      <td>0.695314</td>\n","      <td>0.663485</td>\n","      <td>0.730350</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.056627</td>\n","      <td>0.756861</td>\n","      <td>0.748161</td>\n","      <td>0.770536</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.722412</td>\n","      <td>0.710821</td>\n","      <td>0.739784</td>\n","      <td>0.697166</td>\n","      <td>0.665253</td>\n","      <td>0.732296</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.062652</td>\n","      <td>0.759540</td>\n","      <td>0.773878</td>\n","      <td>0.748082</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.725359</td>\n","      <td>0.742419</td>\n","      <td>0.711658</td>\n","      <td>0.704469</td>\n","      <td>0.713033</td>\n","      <td>0.696109</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.061993</td>\n","      <td>0.761512</td>\n","      <td>0.767446</td>\n","      <td>0.757815</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.727743</td>\n","      <td>0.734265</td>\n","      <td>0.723829</td>\n","      <td>0.708997</td>\n","      <td>0.699205</td>\n","      <td>0.719066</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.063166</td>\n","      <td>0.758173</td>\n","      <td>0.777074</td>\n","      <td>0.742729</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.723858</td>\n","      <td>0.746144</td>\n","      <td>0.705605</td>\n","      <td>0.704336</td>\n","      <td>0.710328</td>\n","      <td>0.698444</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.063734</td>\n","      <td>0.759664</td>\n","      <td>0.778931</td>\n","      <td>0.743188</td>\n","      <td>0.926915</td>\n","      <td>0.926915</td>\n","      <td>0.926915</td>\n","      <td>0.725481</td>\n","      <td>0.748302</td>\n","      <td>0.705914</td>\n","      <td>0.707504</td>\n","      <td>0.716394</td>\n","      <td>0.698833</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.065233</td>\n","      <td>0.754368</td>\n","      <td>0.760254</td>\n","      <td>0.755239</td>\n","      <td>0.922279</td>\n","      <td>0.922279</td>\n","      <td>0.922279</td>\n","      <td>0.719585</td>\n","      <td>0.725768</td>\n","      <td>0.721279</td>\n","      <td>0.695669</td>\n","      <td>0.679659</td>\n","      <td>0.712451</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.064869</td>\n","      <td>0.756192</td>\n","      <td>0.761026</td>\n","      <td>0.757187</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.721678</td>\n","      <td>0.726655</td>\n","      <td>0.723494</td>\n","      <td>0.699620</td>\n","      <td>0.684015</td>\n","      <td>0.715953</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.065405</td>\n","      <td>0.754076</td>\n","      <td>0.760508</td>\n","      <td>0.752677</td>\n","      <td>0.922627</td>\n","      <td>0.922627</td>\n","      <td>0.922627</td>\n","      <td>0.719255</td>\n","      <td>0.726225</td>\n","      <td>0.718153</td>\n","      <td>0.696964</td>\n","      <td>0.684289</td>\n","      <td>0.710117</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.064791</td>\n","      <td>0.756584</td>\n","      <td>0.758158</td>\n","      <td>0.758325</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.722237</td>\n","      <td>0.723434</td>\n","      <td>0.724901</td>\n","      <td>0.700152</td>\n","      <td>0.685034</td>\n","      <td>0.715953</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.064598</td>\n","      <td>0.756062</td>\n","      <td>0.757502</td>\n","      <td>0.757554</td>\n","      <td>0.922743</td>\n","      <td>0.922743</td>\n","      <td>0.922743</td>\n","      <td>0.721609</td>\n","      <td>0.722689</td>\n","      <td>0.723945</td>\n","      <td>0.699562</td>\n","      <td>0.685330</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.064592</td>\n","      <td>0.756565</td>\n","      <td>0.758038</td>\n","      <td>0.757968</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.722184</td>\n","      <td>0.723313</td>\n","      <td>0.724406</td>\n","      <td>0.700210</td>\n","      <td>0.686216</td>\n","      <td>0.714786</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.756565018035917, 'eval_macro_precision': 0.758037672859433, 'eval_macro_recall': 0.7579682049722699, 'eval_micro_f1': 0.9229164251767706, 'eval_micro_precision': 0.9229164251767706, 'eval_micro_recall': 0.9229164251767706, 'eval_macro_f1_no_o': 0.7221839248444617, 'eval_macro_precision_no_o': 0.723313051392647, 'eval_macro_recall_no_o': 0.7244056552788712, 'eval_micro_f1_no_o': 0.7002096436058699, 'eval_micro_precision_no_o': 0.6862159133358237, 'eval_micro_recall_no_o': 0.7147859922178988, 'eval_loss': 0.06459169971469597, 'eval_runtime': 21.2227, 'eval_samples_per_second': 5.701, 'eval_steps_per_second': 1.461, 'epoch': 18.0}\n","Accuracy for fold  12 :  0.7002096436058699  --  0.9229164251767706\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #13\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 39:51, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.027600</td>\n","      <td>0.106097</td>\n","      <td>0.739181</td>\n","      <td>0.775009</td>\n","      <td>0.720802</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.701554</td>\n","      <td>0.743871</td>\n","      <td>0.679593</td>\n","      <td>0.683053</td>\n","      <td>0.696162</td>\n","      <td>0.670428</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009900</td>\n","      <td>0.079634</td>\n","      <td>0.740465</td>\n","      <td>0.766758</td>\n","      <td>0.739036</td>\n","      <td>0.915614</td>\n","      <td>0.915614</td>\n","      <td>0.915614</td>\n","      <td>0.704040</td>\n","      <td>0.733270</td>\n","      <td>0.703793</td>\n","      <td>0.681726</td>\n","      <td>0.650300</td>\n","      <td>0.716342</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.005000</td>\n","      <td>0.069345</td>\n","      <td>0.742290</td>\n","      <td>0.716381</td>\n","      <td>0.771977</td>\n","      <td>0.919439</td>\n","      <td>0.919439</td>\n","      <td>0.919439</td>\n","      <td>0.705719</td>\n","      <td>0.674424</td>\n","      <td>0.741408</td>\n","      <td>0.688168</td>\n","      <td>0.664133</td>\n","      <td>0.714008</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002900</td>\n","      <td>0.062780</td>\n","      <td>0.730588</td>\n","      <td>0.726597</td>\n","      <td>0.753760</td>\n","      <td>0.910803</td>\n","      <td>0.910803</td>\n","      <td>0.910803</td>\n","      <td>0.692742</td>\n","      <td>0.685271</td>\n","      <td>0.722493</td>\n","      <td>0.670922</td>\n","      <td>0.616287</td>\n","      <td>0.736187</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002100</td>\n","      <td>0.058411</td>\n","      <td>0.755816</td>\n","      <td>0.736175</td>\n","      <td>0.782809</td>\n","      <td>0.920019</td>\n","      <td>0.920019</td>\n","      <td>0.920019</td>\n","      <td>0.721389</td>\n","      <td>0.696263</td>\n","      <td>0.755033</td>\n","      <td>0.698229</td>\n","      <td>0.651822</td>\n","      <td>0.751751</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.060419</td>\n","      <td>0.762268</td>\n","      <td>0.748467</td>\n","      <td>0.779056</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.728582</td>\n","      <td>0.711246</td>\n","      <td>0.749383</td>\n","      <td>0.709064</td>\n","      <td>0.680874</td>\n","      <td>0.739689</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001100</td>\n","      <td>0.061905</td>\n","      <td>0.763975</td>\n","      <td>0.761174</td>\n","      <td>0.768699</td>\n","      <td>0.925930</td>\n","      <td>0.925930</td>\n","      <td>0.925930</td>\n","      <td>0.730441</td>\n","      <td>0.726583</td>\n","      <td>0.736539</td>\n","      <td>0.707071</td>\n","      <td>0.692940</td>\n","      <td>0.721790</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.061535</td>\n","      <td>0.760284</td>\n","      <td>0.756827</td>\n","      <td>0.768077</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.726278</td>\n","      <td>0.721327</td>\n","      <td>0.736279</td>\n","      <td>0.705239</td>\n","      <td>0.683845</td>\n","      <td>0.728016</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.058991</td>\n","      <td>0.764819</td>\n","      <td>0.766459</td>\n","      <td>0.765585</td>\n","      <td>0.926568</td>\n","      <td>0.926568</td>\n","      <td>0.926568</td>\n","      <td>0.731486</td>\n","      <td>0.732882</td>\n","      <td>0.732895</td>\n","      <td>0.712648</td>\n","      <td>0.700075</td>\n","      <td>0.725681</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.060867</td>\n","      <td>0.764920</td>\n","      <td>0.761265</td>\n","      <td>0.771659</td>\n","      <td>0.925119</td>\n","      <td>0.925119</td>\n","      <td>0.925119</td>\n","      <td>0.731724</td>\n","      <td>0.726625</td>\n","      <td>0.740413</td>\n","      <td>0.709885</td>\n","      <td>0.690187</td>\n","      <td>0.730739</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.059540</td>\n","      <td>0.764855</td>\n","      <td>0.761569</td>\n","      <td>0.770097</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.731588</td>\n","      <td>0.727158</td>\n","      <td>0.738295</td>\n","      <td>0.710366</td>\n","      <td>0.696042</td>\n","      <td>0.725292</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.059698</td>\n","      <td>0.765212</td>\n","      <td>0.764174</td>\n","      <td>0.768136</td>\n","      <td>0.926973</td>\n","      <td>0.926973</td>\n","      <td>0.926973</td>\n","      <td>0.731859</td>\n","      <td>0.730146</td>\n","      <td>0.735769</td>\n","      <td>0.712292</td>\n","      <td>0.700113</td>\n","      <td>0.724903</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.059290</td>\n","      <td>0.765661</td>\n","      <td>0.766245</td>\n","      <td>0.767108</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.732399</td>\n","      <td>0.732616</td>\n","      <td>0.734547</td>\n","      <td>0.711715</td>\n","      <td>0.700452</td>\n","      <td>0.723346</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.061812</td>\n","      <td>0.760673</td>\n","      <td>0.754614</td>\n","      <td>0.770308</td>\n","      <td>0.924655</td>\n","      <td>0.924655</td>\n","      <td>0.924655</td>\n","      <td>0.726721</td>\n","      <td>0.718712</td>\n","      <td>0.738893</td>\n","      <td>0.706215</td>\n","      <td>0.684307</td>\n","      <td>0.729572</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.061635</td>\n","      <td>0.762128</td>\n","      <td>0.756730</td>\n","      <td>0.770492</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.728400</td>\n","      <td>0.721212</td>\n","      <td>0.739040</td>\n","      <td>0.707791</td>\n","      <td>0.686928</td>\n","      <td>0.729961</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.061714</td>\n","      <td>0.763162</td>\n","      <td>0.758286</td>\n","      <td>0.770877</td>\n","      <td>0.925467</td>\n","      <td>0.925467</td>\n","      <td>0.925467</td>\n","      <td>0.729583</td>\n","      <td>0.723036</td>\n","      <td>0.739431</td>\n","      <td>0.709348</td>\n","      <td>0.689174</td>\n","      <td>0.730739</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.061776</td>\n","      <td>0.761581</td>\n","      <td>0.756978</td>\n","      <td>0.768908</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.727742</td>\n","      <td>0.721576</td>\n","      <td>0.737077</td>\n","      <td>0.707797</td>\n","      <td>0.689020</td>\n","      <td>0.727626</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.061781</td>\n","      <td>0.761768</td>\n","      <td>0.757195</td>\n","      <td>0.769006</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.727965</td>\n","      <td>0.721839</td>\n","      <td>0.737193</td>\n","      <td>0.707931</td>\n","      <td>0.689274</td>\n","      <td>0.727626</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7617680356495039, 'eval_macro_precision': 0.7571945887832559, 'eval_macro_recall': 0.7690064663006583, 'eval_micro_f1': 0.925292685754028, 'eval_micro_precision': 0.925292685754028, 'eval_micro_recall': 0.925292685754028, 'eval_macro_f1_no_o': 0.7279651890105346, 'eval_macro_precision_no_o': 0.7218390024583506, 'eval_macro_recall_no_o': 0.7371928250489428, 'eval_micro_f1_no_o': 0.7079310997539278, 'eval_micro_precision_no_o': 0.6892738665683745, 'eval_micro_recall_no_o': 0.7276264591439688, 'eval_loss': 0.0617806099021549, 'eval_runtime': 21.707, 'eval_samples_per_second': 5.574, 'eval_steps_per_second': 1.428, 'epoch': 18.0}\n","Accuracy for fold  13 :  0.7079310997539278  --  0.925292685754028\n","--------------------------------\n","Testing process has finished.\n","Train run #14\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 39:36, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.027100</td>\n","      <td>0.098625</td>\n","      <td>0.756587</td>\n","      <td>0.737151</td>\n","      <td>0.781942</td>\n","      <td>0.925467</td>\n","      <td>0.925467</td>\n","      <td>0.925467</td>\n","      <td>0.721939</td>\n","      <td>0.698293</td>\n","      <td>0.752477</td>\n","      <td>0.711195</td>\n","      <td>0.688525</td>\n","      <td>0.735409</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009500</td>\n","      <td>0.074449</td>\n","      <td>0.764973</td>\n","      <td>0.775390</td>\n","      <td>0.757392</td>\n","      <td>0.926915</td>\n","      <td>0.926915</td>\n","      <td>0.926915</td>\n","      <td>0.731642</td>\n","      <td>0.743630</td>\n","      <td>0.722962</td>\n","      <td>0.711025</td>\n","      <td>0.706923</td>\n","      <td>0.715175</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004300</td>\n","      <td>0.066923</td>\n","      <td>0.749775</td>\n","      <td>0.751909</td>\n","      <td>0.753295</td>\n","      <td>0.922627</td>\n","      <td>0.922627</td>\n","      <td>0.922627</td>\n","      <td>0.714238</td>\n","      <td>0.715721</td>\n","      <td>0.719340</td>\n","      <td>0.701240</td>\n","      <td>0.678052</td>\n","      <td>0.726070</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002700</td>\n","      <td>0.065613</td>\n","      <td>0.742097</td>\n","      <td>0.720892</td>\n","      <td>0.773321</td>\n","      <td>0.918396</td>\n","      <td>0.918396</td>\n","      <td>0.918396</td>\n","      <td>0.705473</td>\n","      <td>0.678426</td>\n","      <td>0.744145</td>\n","      <td>0.691767</td>\n","      <td>0.644079</td>\n","      <td>0.747082</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002000</td>\n","      <td>0.060939</td>\n","      <td>0.757585</td>\n","      <td>0.741138</td>\n","      <td>0.778350</td>\n","      <td>0.924076</td>\n","      <td>0.924076</td>\n","      <td>0.924076</td>\n","      <td>0.723125</td>\n","      <td>0.702522</td>\n","      <td>0.748741</td>\n","      <td>0.706711</td>\n","      <td>0.674929</td>\n","      <td>0.741634</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.060799</td>\n","      <td>0.761564</td>\n","      <td>0.766968</td>\n","      <td>0.760928</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.727812</td>\n","      <td>0.733343</td>\n","      <td>0.727836</td>\n","      <td>0.707197</td>\n","      <td>0.688930</td>\n","      <td>0.726459</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001100</td>\n","      <td>0.061091</td>\n","      <td>0.758556</td>\n","      <td>0.761606</td>\n","      <td>0.760734</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.724430</td>\n","      <td>0.726846</td>\n","      <td>0.728098</td>\n","      <td>0.701833</td>\n","      <td>0.675793</td>\n","      <td>0.729961</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.063767</td>\n","      <td>0.754740</td>\n","      <td>0.755489</td>\n","      <td>0.763257</td>\n","      <td>0.919903</td>\n","      <td>0.919903</td>\n","      <td>0.919903</td>\n","      <td>0.720107</td>\n","      <td>0.719171</td>\n","      <td>0.731813</td>\n","      <td>0.693265</td>\n","      <td>0.654457</td>\n","      <td>0.736965</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.059932</td>\n","      <td>0.765328</td>\n","      <td>0.765066</td>\n","      <td>0.770522</td>\n","      <td>0.924655</td>\n","      <td>0.924655</td>\n","      <td>0.924655</td>\n","      <td>0.732125</td>\n","      <td>0.730535</td>\n","      <td>0.739449</td>\n","      <td>0.708248</td>\n","      <td>0.679043</td>\n","      <td>0.740078</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.058821</td>\n","      <td>0.765319</td>\n","      <td>0.757969</td>\n","      <td>0.779245</td>\n","      <td>0.922974</td>\n","      <td>0.922974</td>\n","      <td>0.922974</td>\n","      <td>0.732337</td>\n","      <td>0.722036</td>\n","      <td>0.750272</td>\n","      <td>0.708387</td>\n","      <td>0.670372</td>\n","      <td>0.750973</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.058492</td>\n","      <td>0.763990</td>\n","      <td>0.749614</td>\n","      <td>0.785111</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.730859</td>\n","      <td>0.712094</td>\n","      <td>0.757446</td>\n","      <td>0.705968</td>\n","      <td>0.663021</td>\n","      <td>0.754864</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.058627</td>\n","      <td>0.767251</td>\n","      <td>0.762304</td>\n","      <td>0.777401</td>\n","      <td>0.925119</td>\n","      <td>0.925119</td>\n","      <td>0.925119</td>\n","      <td>0.734399</td>\n","      <td>0.727292</td>\n","      <td>0.747554</td>\n","      <td>0.712639</td>\n","      <td>0.682206</td>\n","      <td>0.745914</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.058477</td>\n","      <td>0.769586</td>\n","      <td>0.766323</td>\n","      <td>0.777642</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.736929</td>\n","      <td>0.732080</td>\n","      <td>0.747358</td>\n","      <td>0.714339</td>\n","      <td>0.689993</td>\n","      <td>0.740467</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.059115</td>\n","      <td>0.765001</td>\n","      <td>0.756567</td>\n","      <td>0.779443</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.731883</td>\n","      <td>0.720385</td>\n","      <td>0.750357</td>\n","      <td>0.707375</td>\n","      <td>0.670736</td>\n","      <td>0.748249</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.059338</td>\n","      <td>0.765191</td>\n","      <td>0.756983</td>\n","      <td>0.779223</td>\n","      <td>0.923496</td>\n","      <td>0.923496</td>\n","      <td>0.923496</td>\n","      <td>0.732081</td>\n","      <td>0.720858</td>\n","      <td>0.750065</td>\n","      <td>0.707636</td>\n","      <td>0.671204</td>\n","      <td>0.748249</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.059511</td>\n","      <td>0.764578</td>\n","      <td>0.756359</td>\n","      <td>0.778608</td>\n","      <td>0.923206</td>\n","      <td>0.923206</td>\n","      <td>0.923206</td>\n","      <td>0.731384</td>\n","      <td>0.720132</td>\n","      <td>0.749382</td>\n","      <td>0.706510</td>\n","      <td>0.669805</td>\n","      <td>0.747471</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.059330</td>\n","      <td>0.764773</td>\n","      <td>0.756849</td>\n","      <td>0.778127</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.731557</td>\n","      <td>0.720722</td>\n","      <td>0.748696</td>\n","      <td>0.707465</td>\n","      <td>0.672154</td>\n","      <td>0.746693</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000400</td>\n","      <td>0.059329</td>\n","      <td>0.764773</td>\n","      <td>0.756849</td>\n","      <td>0.778127</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.731557</td>\n","      <td>0.720722</td>\n","      <td>0.748696</td>\n","      <td>0.707465</td>\n","      <td>0.672154</td>\n","      <td>0.746693</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7647729024620034, 'eval_macro_precision': 0.756848846791892, 'eval_macro_recall': 0.7781267760108863, 'eval_micro_f1': 0.9237278312275414, 'eval_micro_precision': 0.9237278312275414, 'eval_micro_recall': 0.9237278312275414, 'eval_macro_f1_no_o': 0.7315569935249523, 'eval_macro_precision_no_o': 0.7207221081868553, 'eval_macro_recall_no_o': 0.7486958032848338, 'eval_micro_f1_no_o': 0.7074654377880184, 'eval_micro_precision_no_o': 0.67215411558669, 'eval_micro_recall_no_o': 0.7466926070038911, 'eval_loss': 0.059329186256103984, 'eval_runtime': 21.114, 'eval_samples_per_second': 5.731, 'eval_steps_per_second': 1.468, 'epoch': 18.0}\n","Accuracy for fold  14 :  0.7074654377880184  --  0.9237278312275414\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #15\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:17, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026800</td>\n","      <td>0.096770</td>\n","      <td>0.764420</td>\n","      <td>0.768106</td>\n","      <td>0.767587</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.730964</td>\n","      <td>0.734250</td>\n","      <td>0.735661</td>\n","      <td>0.712380</td>\n","      <td>0.688703</td>\n","      <td>0.737743</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.016600</td>\n","      <td>0.080856</td>\n","      <td>0.739421</td>\n","      <td>0.778656</td>\n","      <td>0.715214</td>\n","      <td>0.923206</td>\n","      <td>0.923206</td>\n","      <td>0.923206</td>\n","      <td>0.702059</td>\n","      <td>0.747953</td>\n","      <td>0.673697</td>\n","      <td>0.691286</td>\n","      <td>0.694270</td>\n","      <td>0.688327</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004800</td>\n","      <td>0.071177</td>\n","      <td>0.749778</td>\n","      <td>0.761119</td>\n","      <td>0.742218</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.714233</td>\n","      <td>0.727447</td>\n","      <td>0.705429</td>\n","      <td>0.693370</td>\n","      <td>0.692965</td>\n","      <td>0.693774</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003100</td>\n","      <td>0.065937</td>\n","      <td>0.763885</td>\n","      <td>0.800437</td>\n","      <td>0.737083</td>\n","      <td>0.928828</td>\n","      <td>0.928828</td>\n","      <td>0.928828</td>\n","      <td>0.730318</td>\n","      <td>0.773668</td>\n","      <td>0.698337</td>\n","      <td>0.714172</td>\n","      <td>0.733197</td>\n","      <td>0.696109</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002000</td>\n","      <td>0.053813</td>\n","      <td>0.764484</td>\n","      <td>0.749599</td>\n","      <td>0.783992</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.731449</td>\n","      <td>0.712364</td>\n","      <td>0.755890</td>\n","      <td>0.708150</td>\n","      <td>0.670257</td>\n","      <td>0.750584</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.059294</td>\n","      <td>0.749008</td>\n","      <td>0.738325</td>\n","      <td>0.772672</td>\n","      <td>0.915904</td>\n","      <td>0.915904</td>\n","      <td>0.915904</td>\n","      <td>0.713858</td>\n","      <td>0.699080</td>\n","      <td>0.743716</td>\n","      <td>0.686476</td>\n","      <td>0.638954</td>\n","      <td>0.741634</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.059939</td>\n","      <td>0.753133</td>\n","      <td>0.766786</td>\n","      <td>0.748119</td>\n","      <td>0.923902</td>\n","      <td>0.923902</td>\n","      <td>0.923902</td>\n","      <td>0.718003</td>\n","      <td>0.733187</td>\n","      <td>0.712892</td>\n","      <td>0.702180</td>\n","      <td>0.684658</td>\n","      <td>0.720623</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.058841</td>\n","      <td>0.765481</td>\n","      <td>0.769028</td>\n","      <td>0.762483</td>\n","      <td>0.929292</td>\n","      <td>0.929292</td>\n","      <td>0.929292</td>\n","      <td>0.731930</td>\n","      <td>0.736315</td>\n","      <td>0.728186</td>\n","      <td>0.712856</td>\n","      <td>0.719208</td>\n","      <td>0.706615</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.063410</td>\n","      <td>0.768751</td>\n","      <td>0.787205</td>\n","      <td>0.752813</td>\n","      <td>0.930625</td>\n","      <td>0.930625</td>\n","      <td>0.930625</td>\n","      <td>0.735691</td>\n","      <td>0.757749</td>\n","      <td>0.716564</td>\n","      <td>0.717430</td>\n","      <td>0.731500</td>\n","      <td>0.703891</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.056719</td>\n","      <td>0.760751</td>\n","      <td>0.743098</td>\n","      <td>0.782953</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.727046</td>\n","      <td>0.704919</td>\n","      <td>0.754451</td>\n","      <td>0.706187</td>\n","      <td>0.672056</td>\n","      <td>0.743969</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.056661</td>\n","      <td>0.759993</td>\n","      <td>0.740141</td>\n","      <td>0.783471</td>\n","      <td>0.923902</td>\n","      <td>0.923902</td>\n","      <td>0.923902</td>\n","      <td>0.726072</td>\n","      <td>0.701628</td>\n","      <td>0.754727</td>\n","      <td>0.708993</td>\n","      <td>0.679757</td>\n","      <td>0.740856</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.057523</td>\n","      <td>0.758585</td>\n","      <td>0.744011</td>\n","      <td>0.774242</td>\n","      <td>0.924249</td>\n","      <td>0.924249</td>\n","      <td>0.924249</td>\n","      <td>0.724424</td>\n","      <td>0.706609</td>\n","      <td>0.743494</td>\n","      <td>0.707017</td>\n","      <td>0.687891</td>\n","      <td>0.727237</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.057774</td>\n","      <td>0.762798</td>\n","      <td>0.751096</td>\n","      <td>0.775505</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.729350</td>\n","      <td>0.714886</td>\n","      <td>0.744978</td>\n","      <td>0.710422</td>\n","      <td>0.691204</td>\n","      <td>0.730739</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.058431</td>\n","      <td>0.767236</td>\n","      <td>0.761464</td>\n","      <td>0.773952</td>\n","      <td>0.926510</td>\n","      <td>0.926510</td>\n","      <td>0.926510</td>\n","      <td>0.734359</td>\n","      <td>0.727046</td>\n","      <td>0.742770</td>\n","      <td>0.714585</td>\n","      <td>0.700561</td>\n","      <td>0.729183</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.058502</td>\n","      <td>0.765650</td>\n","      <td>0.756694</td>\n","      <td>0.775457</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.732595</td>\n","      <td>0.721402</td>\n","      <td>0.744775</td>\n","      <td>0.713175</td>\n","      <td>0.695379</td>\n","      <td>0.731907</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.058149</td>\n","      <td>0.766923</td>\n","      <td>0.757665</td>\n","      <td>0.777314</td>\n","      <td>0.925930</td>\n","      <td>0.925930</td>\n","      <td>0.925930</td>\n","      <td>0.734049</td>\n","      <td>0.722413</td>\n","      <td>0.746998</td>\n","      <td>0.714799</td>\n","      <td>0.694965</td>\n","      <td>0.735798</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.058412</td>\n","      <td>0.767660</td>\n","      <td>0.759649</td>\n","      <td>0.776521</td>\n","      <td>0.926452</td>\n","      <td>0.926452</td>\n","      <td>0.926452</td>\n","      <td>0.734859</td>\n","      <td>0.724811</td>\n","      <td>0.745892</td>\n","      <td>0.715398</td>\n","      <td>0.698554</td>\n","      <td>0.733074</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.058410</td>\n","      <td>0.767623</td>\n","      <td>0.759561</td>\n","      <td>0.776521</td>\n","      <td>0.926452</td>\n","      <td>0.926452</td>\n","      <td>0.926452</td>\n","      <td>0.734816</td>\n","      <td>0.724709</td>\n","      <td>0.745892</td>\n","      <td>0.715398</td>\n","      <td>0.698554</td>\n","      <td>0.733074</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7676233772236319, 'eval_macro_precision': 0.7595610191732582, 'eval_macro_recall': 0.7765213303222434, 'eval_micro_f1': 0.9264518372551293, 'eval_micro_precision': 0.9264518372551293, 'eval_micro_recall': 0.9264518372551293, 'eval_macro_f1_no_o': 0.7348159901837233, 'eval_macro_precision_no_o': 0.7247087116477277, 'eval_macro_recall_no_o': 0.7458920650726726, 'eval_micro_f1_no_o': 0.715397759635466, 'eval_micro_precision_no_o': 0.6985539488320356, 'eval_micro_recall_no_o': 0.7330739299610894, 'eval_loss': 0.058410027304128836, 'eval_runtime': 20.8633, 'eval_samples_per_second': 5.8, 'eval_steps_per_second': 1.486, 'epoch': 18.0}\n","Accuracy for fold  15 :  0.715397759635466  --  0.9264518372551293\n","--------------------------------\n","Testing process has finished.\n","Train run #16\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 39:40, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026600</td>\n","      <td>0.102822</td>\n","      <td>0.731356</td>\n","      <td>0.715567</td>\n","      <td>0.756215</td>\n","      <td>0.916309</td>\n","      <td>0.916309</td>\n","      <td>0.916309</td>\n","      <td>0.692956</td>\n","      <td>0.673226</td>\n","      <td>0.723246</td>\n","      <td>0.670015</td>\n","      <td>0.641839</td>\n","      <td>0.700778</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009400</td>\n","      <td>0.075109</td>\n","      <td>0.747057</td>\n","      <td>0.736325</td>\n","      <td>0.763425</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.711079</td>\n","      <td>0.697226</td>\n","      <td>0.731487</td>\n","      <td>0.692193</td>\n","      <td>0.662633</td>\n","      <td>0.724514</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004300</td>\n","      <td>0.070670</td>\n","      <td>0.757721</td>\n","      <td>0.779019</td>\n","      <td>0.739654</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.723113</td>\n","      <td>0.748429</td>\n","      <td>0.701563</td>\n","      <td>0.701622</td>\n","      <td>0.713768</td>\n","      <td>0.689883</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002700</td>\n","      <td>0.065280</td>\n","      <td>0.758081</td>\n","      <td>0.773972</td>\n","      <td>0.744896</td>\n","      <td>0.926452</td>\n","      <td>0.926452</td>\n","      <td>0.926452</td>\n","      <td>0.723587</td>\n","      <td>0.742345</td>\n","      <td>0.707986</td>\n","      <td>0.703922</td>\n","      <td>0.709486</td>\n","      <td>0.698444</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.063388</td>\n","      <td>0.762204</td>\n","      <td>0.770606</td>\n","      <td>0.755338</td>\n","      <td>0.928017</td>\n","      <td>0.928017</td>\n","      <td>0.928017</td>\n","      <td>0.728228</td>\n","      <td>0.738058</td>\n","      <td>0.720191</td>\n","      <td>0.710419</td>\n","      <td>0.711111</td>\n","      <td>0.709728</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.060220</td>\n","      <td>0.762905</td>\n","      <td>0.757531</td>\n","      <td>0.770695</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.729215</td>\n","      <td>0.722076</td>\n","      <td>0.739163</td>\n","      <td>0.707948</td>\n","      <td>0.687569</td>\n","      <td>0.729572</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.064437</td>\n","      <td>0.760525</td>\n","      <td>0.771567</td>\n","      <td>0.754047</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.726696</td>\n","      <td>0.738983</td>\n","      <td>0.719729</td>\n","      <td>0.700076</td>\n","      <td>0.685960</td>\n","      <td>0.714786</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.059914</td>\n","      <td>0.764018</td>\n","      <td>0.762280</td>\n","      <td>0.769039</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.730878</td>\n","      <td>0.727703</td>\n","      <td>0.737867</td>\n","      <td>0.705068</td>\n","      <td>0.678790</td>\n","      <td>0.733463</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.060972</td>\n","      <td>0.765107</td>\n","      <td>0.765796</td>\n","      <td>0.766285</td>\n","      <td>0.925756</td>\n","      <td>0.925756</td>\n","      <td>0.925756</td>\n","      <td>0.731870</td>\n","      <td>0.732111</td>\n","      <td>0.733802</td>\n","      <td>0.709271</td>\n","      <td>0.695734</td>\n","      <td>0.723346</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000700</td>\n","      <td>0.063983</td>\n","      <td>0.762502</td>\n","      <td>0.769177</td>\n","      <td>0.758360</td>\n","      <td>0.926568</td>\n","      <td>0.926568</td>\n","      <td>0.926568</td>\n","      <td>0.728789</td>\n","      <td>0.736383</td>\n","      <td>0.724148</td>\n","      <td>0.709952</td>\n","      <td>0.705182</td>\n","      <td>0.714786</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.067471</td>\n","      <td>0.767806</td>\n","      <td>0.782169</td>\n","      <td>0.755553</td>\n","      <td>0.929292</td>\n","      <td>0.929292</td>\n","      <td>0.929292</td>\n","      <td>0.734805</td>\n","      <td>0.751899</td>\n","      <td>0.720170</td>\n","      <td>0.717605</td>\n","      <td>0.726475</td>\n","      <td>0.708949</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.057924</td>\n","      <td>0.766916</td>\n","      <td>0.767267</td>\n","      <td>0.768340</td>\n","      <td>0.926568</td>\n","      <td>0.926568</td>\n","      <td>0.926568</td>\n","      <td>0.733952</td>\n","      <td>0.733782</td>\n","      <td>0.736188</td>\n","      <td>0.713823</td>\n","      <td>0.699813</td>\n","      <td>0.728405</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.058371</td>\n","      <td>0.769163</td>\n","      <td>0.770779</td>\n","      <td>0.769446</td>\n","      <td>0.927031</td>\n","      <td>0.927031</td>\n","      <td>0.927031</td>\n","      <td>0.736484</td>\n","      <td>0.737767</td>\n","      <td>0.737410</td>\n","      <td>0.714041</td>\n","      <td>0.699515</td>\n","      <td>0.729183</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.059473</td>\n","      <td>0.769550</td>\n","      <td>0.773101</td>\n","      <td>0.767914</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.736920</td>\n","      <td>0.740572</td>\n","      <td>0.735498</td>\n","      <td>0.714859</td>\n","      <td>0.702896</td>\n","      <td>0.727237</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.060210</td>\n","      <td>0.769330</td>\n","      <td>0.775112</td>\n","      <td>0.765415</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.736641</td>\n","      <td>0.743023</td>\n","      <td>0.732435</td>\n","      <td>0.716097</td>\n","      <td>0.707132</td>\n","      <td>0.725292</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.059363</td>\n","      <td>0.768278</td>\n","      <td>0.771610</td>\n","      <td>0.766907</td>\n","      <td>0.927147</td>\n","      <td>0.927147</td>\n","      <td>0.927147</td>\n","      <td>0.735425</td>\n","      <td>0.738789</td>\n","      <td>0.734347</td>\n","      <td>0.713276</td>\n","      <td>0.700563</td>\n","      <td>0.726459</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.059649</td>\n","      <td>0.769526</td>\n","      <td>0.773210</td>\n","      <td>0.767653</td>\n","      <td>0.927611</td>\n","      <td>0.927611</td>\n","      <td>0.927611</td>\n","      <td>0.736851</td>\n","      <td>0.740665</td>\n","      <td>0.735149</td>\n","      <td>0.714996</td>\n","      <td>0.703160</td>\n","      <td>0.727237</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000400</td>\n","      <td>0.059501</td>\n","      <td>0.768927</td>\n","      <td>0.772665</td>\n","      <td>0.767042</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.736165</td>\n","      <td>0.740029</td>\n","      <td>0.734458</td>\n","      <td>0.713958</td>\n","      <td>0.701880</td>\n","      <td>0.726459</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7689272465263289, 'eval_macro_precision': 0.7726649463042449, 'eval_macro_recall': 0.7670421665636887, 'eval_micro_f1': 0.9273791584560102, 'eval_micro_precision': 0.9273791584560102, 'eval_micro_recall': 0.9273791584560102, 'eval_macro_f1_no_o': 0.7361646029247502, 'eval_macro_precision_no_o': 0.7400290711313903, 'eval_macro_recall_no_o': 0.7344584833463684, 'eval_micro_f1_no_o': 0.7139579349904398, 'eval_micro_precision_no_o': 0.7018796992481203, 'eval_micro_recall_no_o': 0.7264591439688716, 'eval_loss': 0.05950088466874119, 'eval_runtime': 21.5457, 'eval_samples_per_second': 5.616, 'eval_steps_per_second': 1.439, 'epoch': 18.0}\n","Accuracy for fold  16 :  0.7139579349904398  --  0.9273791584560102\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #17\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:42, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026700</td>\n","      <td>0.096353</td>\n","      <td>0.761239</td>\n","      <td>0.758296</td>\n","      <td>0.770791</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.727199</td>\n","      <td>0.722845</td>\n","      <td>0.739252</td>\n","      <td>0.710516</td>\n","      <td>0.688962</td>\n","      <td>0.733463</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009500</td>\n","      <td>0.073462</td>\n","      <td>0.753760</td>\n","      <td>0.734868</td>\n","      <td>0.777167</td>\n","      <td>0.920366</td>\n","      <td>0.920366</td>\n","      <td>0.920366</td>\n","      <td>0.719161</td>\n","      <td>0.695245</td>\n","      <td>0.748302</td>\n","      <td>0.703068</td>\n","      <td>0.662423</td>\n","      <td>0.749027</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004400</td>\n","      <td>0.068631</td>\n","      <td>0.754848</td>\n","      <td>0.781033</td>\n","      <td>0.732890</td>\n","      <td>0.928364</td>\n","      <td>0.928364</td>\n","      <td>0.928364</td>\n","      <td>0.719813</td>\n","      <td>0.751561</td>\n","      <td>0.692979</td>\n","      <td>0.707605</td>\n","      <td>0.741056</td>\n","      <td>0.677043</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002800</td>\n","      <td>0.065735</td>\n","      <td>0.736930</td>\n","      <td>0.719101</td>\n","      <td>0.758272</td>\n","      <td>0.918975</td>\n","      <td>0.918975</td>\n","      <td>0.918975</td>\n","      <td>0.699459</td>\n","      <td>0.677619</td>\n","      <td>0.725385</td>\n","      <td>0.684685</td>\n","      <td>0.661349</td>\n","      <td>0.709728</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002000</td>\n","      <td>0.064442</td>\n","      <td>0.753478</td>\n","      <td>0.764619</td>\n","      <td>0.751324</td>\n","      <td>0.920134</td>\n","      <td>0.920134</td>\n","      <td>0.920134</td>\n","      <td>0.718898</td>\n","      <td>0.731386</td>\n","      <td>0.716892</td>\n","      <td>0.691764</td>\n","      <td>0.679685</td>\n","      <td>0.704280</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001800</td>\n","      <td>0.074857</td>\n","      <td>0.742227</td>\n","      <td>0.776732</td>\n","      <td>0.716471</td>\n","      <td>0.920192</td>\n","      <td>0.920192</td>\n","      <td>0.920192</td>\n","      <td>0.705769</td>\n","      <td>0.746722</td>\n","      <td>0.675016</td>\n","      <td>0.680104</td>\n","      <td>0.698075</td>\n","      <td>0.663035</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.066839</td>\n","      <td>0.754701</td>\n","      <td>0.768912</td>\n","      <td>0.749724</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.720071</td>\n","      <td>0.736089</td>\n","      <td>0.714821</td>\n","      <td>0.699351</td>\n","      <td>0.686003</td>\n","      <td>0.713230</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.068877</td>\n","      <td>0.759151</td>\n","      <td>0.773005</td>\n","      <td>0.753136</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.725146</td>\n","      <td>0.740815</td>\n","      <td>0.718620</td>\n","      <td>0.704398</td>\n","      <td>0.692481</td>\n","      <td>0.716732</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.069117</td>\n","      <td>0.754229</td>\n","      <td>0.768749</td>\n","      <td>0.746500</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.719480</td>\n","      <td>0.736014</td>\n","      <td>0.710867</td>\n","      <td>0.698504</td>\n","      <td>0.688729</td>\n","      <td>0.708560</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.067955</td>\n","      <td>0.754569</td>\n","      <td>0.764977</td>\n","      <td>0.750489</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.719936</td>\n","      <td>0.731457</td>\n","      <td>0.715794</td>\n","      <td>0.697506</td>\n","      <td>0.682818</td>\n","      <td>0.712840</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.067773</td>\n","      <td>0.754838</td>\n","      <td>0.760595</td>\n","      <td>0.755396</td>\n","      <td>0.921410</td>\n","      <td>0.921410</td>\n","      <td>0.921410</td>\n","      <td>0.720295</td>\n","      <td>0.726122</td>\n","      <td>0.721824</td>\n","      <td>0.697227</td>\n","      <td>0.676675</td>\n","      <td>0.719066</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.064590</td>\n","      <td>0.757039</td>\n","      <td>0.756418</td>\n","      <td>0.763039</td>\n","      <td>0.921236</td>\n","      <td>0.921236</td>\n","      <td>0.921236</td>\n","      <td>0.722897</td>\n","      <td>0.720981</td>\n","      <td>0.731070</td>\n","      <td>0.699907</td>\n","      <td>0.672890</td>\n","      <td>0.729183</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.065437</td>\n","      <td>0.757743</td>\n","      <td>0.759103</td>\n","      <td>0.761331</td>\n","      <td>0.922395</td>\n","      <td>0.922395</td>\n","      <td>0.922395</td>\n","      <td>0.723596</td>\n","      <td>0.724182</td>\n","      <td>0.728772</td>\n","      <td>0.701748</td>\n","      <td>0.678662</td>\n","      <td>0.726459</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.067873</td>\n","      <td>0.759268</td>\n","      <td>0.768805</td>\n","      <td>0.756094</td>\n","      <td>0.924307</td>\n","      <td>0.924307</td>\n","      <td>0.924307</td>\n","      <td>0.725204</td>\n","      <td>0.735663</td>\n","      <td>0.722162</td>\n","      <td>0.705569</td>\n","      <td>0.689706</td>\n","      <td>0.722179</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.069725</td>\n","      <td>0.759374</td>\n","      <td>0.773838</td>\n","      <td>0.751261</td>\n","      <td>0.924597</td>\n","      <td>0.924597</td>\n","      <td>0.924597</td>\n","      <td>0.725341</td>\n","      <td>0.741782</td>\n","      <td>0.716308</td>\n","      <td>0.705882</td>\n","      <td>0.695357</td>\n","      <td>0.716732</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.069258</td>\n","      <td>0.758429</td>\n","      <td>0.769575</td>\n","      <td>0.753312</td>\n","      <td>0.924134</td>\n","      <td>0.924134</td>\n","      <td>0.924134</td>\n","      <td>0.724272</td>\n","      <td>0.736693</td>\n","      <td>0.718882</td>\n","      <td>0.705299</td>\n","      <td>0.691330</td>\n","      <td>0.719844</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.069417</td>\n","      <td>0.758723</td>\n","      <td>0.771308</td>\n","      <td>0.752594</td>\n","      <td>0.924423</td>\n","      <td>0.924423</td>\n","      <td>0.924423</td>\n","      <td>0.724585</td>\n","      <td>0.738734</td>\n","      <td>0.717965</td>\n","      <td>0.705748</td>\n","      <td>0.692913</td>\n","      <td>0.719066</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.069501</td>\n","      <td>0.758260</td>\n","      <td>0.770765</td>\n","      <td>0.752130</td>\n","      <td>0.924307</td>\n","      <td>0.924307</td>\n","      <td>0.924307</td>\n","      <td>0.724056</td>\n","      <td>0.738123</td>\n","      <td>0.717424</td>\n","      <td>0.705253</td>\n","      <td>0.692683</td>\n","      <td>0.718288</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.758259690282218, 'eval_macro_precision': 0.7707648222242078, 'eval_macro_recall': 0.7521298763315665, 'eval_micro_f1': 0.9243074069780921, 'eval_micro_precision': 0.924307406978092, 'eval_micro_recall': 0.924307406978092, 'eval_macro_f1_no_o': 0.7240560208805588, 'eval_macro_precision_no_o': 0.7381225117097675, 'eval_macro_recall_no_o': 0.717424018527752, 'eval_micro_f1_no_o': 0.7052531041069723, 'eval_micro_precision_no_o': 0.6926829268292682, 'eval_micro_recall_no_o': 0.7182879377431907, 'eval_loss': 0.06950108923098194, 'eval_runtime': 21.4334, 'eval_samples_per_second': 5.645, 'eval_steps_per_second': 1.446, 'epoch': 18.0}\n","Accuracy for fold  17 :  0.7052531041069723  --  0.9243074069780921\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #18\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:31, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026600</td>\n","      <td>0.104091</td>\n","      <td>0.728523</td>\n","      <td>0.734407</td>\n","      <td>0.731727</td>\n","      <td>0.924307</td>\n","      <td>0.924307</td>\n","      <td>0.924307</td>\n","      <td>0.689087</td>\n","      <td>0.696154</td>\n","      <td>0.692622</td>\n","      <td>0.689006</td>\n","      <td>0.694039</td>\n","      <td>0.684047</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009600</td>\n","      <td>0.075926</td>\n","      <td>0.746055</td>\n","      <td>0.741125</td>\n","      <td>0.751597</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.709692</td>\n","      <td>0.703875</td>\n","      <td>0.716223</td>\n","      <td>0.699922</td>\n","      <td>0.698296</td>\n","      <td>0.701556</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004300</td>\n","      <td>0.064289</td>\n","      <td>0.751229</td>\n","      <td>0.754635</td>\n","      <td>0.753302</td>\n","      <td>0.924423</td>\n","      <td>0.924423</td>\n","      <td>0.924423</td>\n","      <td>0.715608</td>\n","      <td>0.718590</td>\n","      <td>0.719008</td>\n","      <td>0.702012</td>\n","      <td>0.679156</td>\n","      <td>0.726459</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002900</td>\n","      <td>0.067989</td>\n","      <td>0.749323</td>\n","      <td>0.756495</td>\n","      <td>0.747488</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.713621</td>\n","      <td>0.721409</td>\n","      <td>0.712054</td>\n","      <td>0.697426</td>\n","      <td>0.683738</td>\n","      <td>0.711673</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002100</td>\n","      <td>0.066355</td>\n","      <td>0.737972</td>\n","      <td>0.731814</td>\n","      <td>0.753272</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.700502</td>\n","      <td>0.692181</td>\n","      <td>0.719471</td>\n","      <td>0.697848</td>\n","      <td>0.672072</td>\n","      <td>0.725681</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.062890</td>\n","      <td>0.762192</td>\n","      <td>0.764370</td>\n","      <td>0.763180</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.728576</td>\n","      <td>0.730421</td>\n","      <td>0.730418</td>\n","      <td>0.707178</td>\n","      <td>0.690653</td>\n","      <td>0.724514</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.063068</td>\n","      <td>0.755832</td>\n","      <td>0.750390</td>\n","      <td>0.766307</td>\n","      <td>0.921641</td>\n","      <td>0.921641</td>\n","      <td>0.921641</td>\n","      <td>0.721369</td>\n","      <td>0.713681</td>\n","      <td>0.734906</td>\n","      <td>0.699870</td>\n","      <td>0.669868</td>\n","      <td>0.732685</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.059399</td>\n","      <td>0.754838</td>\n","      <td>0.737662</td>\n","      <td>0.778810</td>\n","      <td>0.920598</td>\n","      <td>0.920598</td>\n","      <td>0.920598</td>\n","      <td>0.720311</td>\n","      <td>0.698201</td>\n","      <td>0.750299</td>\n","      <td>0.702722</td>\n","      <td>0.658503</td>\n","      <td>0.753307</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.062358</td>\n","      <td>0.756049</td>\n","      <td>0.753977</td>\n","      <td>0.764269</td>\n","      <td>0.924423</td>\n","      <td>0.924423</td>\n","      <td>0.924423</td>\n","      <td>0.721270</td>\n","      <td>0.717748</td>\n","      <td>0.731950</td>\n","      <td>0.704252</td>\n","      <td>0.678945</td>\n","      <td>0.731518</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.061364</td>\n","      <td>0.766943</td>\n","      <td>0.760903</td>\n","      <td>0.780119</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.734339</td>\n","      <td>0.725739</td>\n","      <td>0.751236</td>\n","      <td>0.710094</td>\n","      <td>0.675325</td>\n","      <td>0.748638</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.067115</td>\n","      <td>0.756360</td>\n","      <td>0.765407</td>\n","      <td>0.753137</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.721871</td>\n","      <td>0.731530</td>\n","      <td>0.718996</td>\n","      <td>0.698227</td>\n","      <td>0.677526</td>\n","      <td>0.720233</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.066173</td>\n","      <td>0.757360</td>\n","      <td>0.770600</td>\n","      <td>0.749062</td>\n","      <td>0.923786</td>\n","      <td>0.923786</td>\n","      <td>0.923786</td>\n","      <td>0.722962</td>\n","      <td>0.737853</td>\n","      <td>0.713833</td>\n","      <td>0.700630</td>\n","      <td>0.687383</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.065149</td>\n","      <td>0.760106</td>\n","      <td>0.763888</td>\n","      <td>0.760354</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.726184</td>\n","      <td>0.729690</td>\n","      <td>0.727370</td>\n","      <td>0.701357</td>\n","      <td>0.680322</td>\n","      <td>0.723735</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.065592</td>\n","      <td>0.759888</td>\n","      <td>0.764973</td>\n","      <td>0.758806</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.725928</td>\n","      <td>0.731009</td>\n","      <td>0.725508</td>\n","      <td>0.700793</td>\n","      <td>0.680984</td>\n","      <td>0.721790</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.066030</td>\n","      <td>0.759158</td>\n","      <td>0.764743</td>\n","      <td>0.757763</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.725052</td>\n","      <td>0.730740</td>\n","      <td>0.724246</td>\n","      <td>0.700189</td>\n","      <td>0.680882</td>\n","      <td>0.720623</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.065931</td>\n","      <td>0.759707</td>\n","      <td>0.764577</td>\n","      <td>0.758997</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.725700</td>\n","      <td>0.730525</td>\n","      <td>0.725720</td>\n","      <td>0.700661</td>\n","      <td>0.680734</td>\n","      <td>0.721790</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.064431</td>\n","      <td>0.759722</td>\n","      <td>0.761736</td>\n","      <td>0.762362</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.725741</td>\n","      <td>0.727038</td>\n","      <td>0.729860</td>\n","      <td>0.700188</td>\n","      <td>0.676087</td>\n","      <td>0.726070</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000600</td>\n","      <td>0.064446</td>\n","      <td>0.759148</td>\n","      <td>0.761583</td>\n","      <td>0.761434</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.725077</td>\n","      <td>0.726872</td>\n","      <td>0.728778</td>\n","      <td>0.699944</td>\n","      <td>0.675970</td>\n","      <td>0.725681</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7591475315531172, 'eval_macro_precision': 0.7615833718782542, 'eval_macro_recall': 0.7614339461115628, 'eval_micro_f1': 0.9228584676017155, 'eval_micro_precision': 0.9228584676017155, 'eval_micro_recall': 0.9228584676017155, 'eval_macro_f1_no_o': 0.7250771717234931, 'eval_macro_precision_no_o': 0.7268719883595155, 'eval_macro_recall_no_o': 0.7287781763928461, 'eval_micro_f1_no_o': 0.699943704259711, 'eval_micro_precision_no_o': 0.6759695541862993, 'eval_micro_recall_no_o': 0.72568093385214, 'eval_loss': 0.06444566399545129, 'eval_runtime': 21.5354, 'eval_samples_per_second': 5.619, 'eval_steps_per_second': 1.439, 'epoch': 18.0}\n","Accuracy for fold  18 :  0.699943704259711  --  0.9228584676017155\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #19\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:35, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026800</td>\n","      <td>0.094453</td>\n","      <td>0.765312</td>\n","      <td>0.772383</td>\n","      <td>0.764591</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.731801</td>\n","      <td>0.738978</td>\n","      <td>0.732019</td>\n","      <td>0.712170</td>\n","      <td>0.687296</td>\n","      <td>0.738911</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009600</td>\n","      <td>0.081751</td>\n","      <td>0.749122</td>\n","      <td>0.777611</td>\n","      <td>0.724617</td>\n","      <td>0.928538</td>\n","      <td>0.928538</td>\n","      <td>0.928538</td>\n","      <td>0.712801</td>\n","      <td>0.746912</td>\n","      <td>0.683329</td>\n","      <td>0.700141</td>\n","      <td>0.723537</td>\n","      <td>0.678210</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004200</td>\n","      <td>0.066160</td>\n","      <td>0.756711</td>\n","      <td>0.753416</td>\n","      <td>0.764958</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.721679</td>\n","      <td>0.717483</td>\n","      <td>0.731653</td>\n","      <td>0.702921</td>\n","      <td>0.694381</td>\n","      <td>0.711673</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002800</td>\n","      <td>0.063296</td>\n","      <td>0.756914</td>\n","      <td>0.769799</td>\n","      <td>0.749414</td>\n","      <td>0.927901</td>\n","      <td>0.927901</td>\n","      <td>0.927901</td>\n","      <td>0.721976</td>\n","      <td>0.736921</td>\n","      <td>0.713314</td>\n","      <td>0.707913</td>\n","      <td>0.705723</td>\n","      <td>0.710117</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002400</td>\n","      <td>0.056937</td>\n","      <td>0.772831</td>\n","      <td>0.777517</td>\n","      <td>0.769654</td>\n","      <td>0.928712</td>\n","      <td>0.928712</td>\n","      <td>0.928712</td>\n","      <td>0.740641</td>\n","      <td>0.745899</td>\n","      <td>0.737143</td>\n","      <td>0.717652</td>\n","      <td>0.712423</td>\n","      <td>0.722957</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.058742</td>\n","      <td>0.764410</td>\n","      <td>0.762828</td>\n","      <td>0.770020</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.730880</td>\n","      <td>0.728131</td>\n","      <td>0.738318</td>\n","      <td>0.708278</td>\n","      <td>0.687157</td>\n","      <td>0.730739</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001500</td>\n","      <td>0.059005</td>\n","      <td>0.757020</td>\n","      <td>0.736327</td>\n","      <td>0.785576</td>\n","      <td>0.918859</td>\n","      <td>0.918859</td>\n","      <td>0.918859</td>\n","      <td>0.722922</td>\n","      <td>0.696513</td>\n","      <td>0.758442</td>\n","      <td>0.695527</td>\n","      <td>0.648285</td>\n","      <td>0.750195</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.062444</td>\n","      <td>0.752112</td>\n","      <td>0.744461</td>\n","      <td>0.765930</td>\n","      <td>0.920019</td>\n","      <td>0.920019</td>\n","      <td>0.920019</td>\n","      <td>0.717097</td>\n","      <td>0.706549</td>\n","      <td>0.734806</td>\n","      <td>0.694163</td>\n","      <td>0.658861</td>\n","      <td>0.733463</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.062739</td>\n","      <td>0.755257</td>\n","      <td>0.769577</td>\n","      <td>0.747741</td>\n","      <td>0.925351</td>\n","      <td>0.925351</td>\n","      <td>0.925351</td>\n","      <td>0.720309</td>\n","      <td>0.736785</td>\n","      <td>0.711770</td>\n","      <td>0.701274</td>\n","      <td>0.695636</td>\n","      <td>0.707004</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.062149</td>\n","      <td>0.757943</td>\n","      <td>0.754799</td>\n","      <td>0.765519</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.723499</td>\n","      <td>0.718828</td>\n","      <td>0.733328</td>\n","      <td>0.700996</td>\n","      <td>0.677935</td>\n","      <td>0.725681</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.063158</td>\n","      <td>0.750727</td>\n","      <td>0.747768</td>\n","      <td>0.757824</td>\n","      <td>0.923496</td>\n","      <td>0.923496</td>\n","      <td>0.923496</td>\n","      <td>0.715175</td>\n","      <td>0.710944</td>\n","      <td>0.724226</td>\n","      <td>0.699110</td>\n","      <td>0.680930</td>\n","      <td>0.718288</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.061241</td>\n","      <td>0.746904</td>\n","      <td>0.722805</td>\n","      <td>0.777827</td>\n","      <td>0.918280</td>\n","      <td>0.918280</td>\n","      <td>0.918280</td>\n","      <td>0.711126</td>\n","      <td>0.680893</td>\n","      <td>0.749266</td>\n","      <td>0.690830</td>\n","      <td>0.646540</td>\n","      <td>0.741634</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.061021</td>\n","      <td>0.752558</td>\n","      <td>0.740675</td>\n","      <td>0.769767</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.717424</td>\n","      <td>0.702024</td>\n","      <td>0.739011</td>\n","      <td>0.698301</td>\n","      <td>0.664441</td>\n","      <td>0.735798</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.061347</td>\n","      <td>0.753139</td>\n","      <td>0.741482</td>\n","      <td>0.770219</td>\n","      <td>0.922221</td>\n","      <td>0.922221</td>\n","      <td>0.922221</td>\n","      <td>0.718049</td>\n","      <td>0.702973</td>\n","      <td>0.739425</td>\n","      <td>0.698982</td>\n","      <td>0.666314</td>\n","      <td>0.735019</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.061798</td>\n","      <td>0.750616</td>\n","      <td>0.745488</td>\n","      <td>0.760438</td>\n","      <td>0.922974</td>\n","      <td>0.922974</td>\n","      <td>0.922974</td>\n","      <td>0.715020</td>\n","      <td>0.707860</td>\n","      <td>0.727639</td>\n","      <td>0.698430</td>\n","      <td>0.671819</td>\n","      <td>0.727237</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.062028</td>\n","      <td>0.751832</td>\n","      <td>0.748821</td>\n","      <td>0.759483</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.716412</td>\n","      <td>0.711802</td>\n","      <td>0.726423</td>\n","      <td>0.699138</td>\n","      <td>0.674133</td>\n","      <td>0.726070</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.062050</td>\n","      <td>0.751435</td>\n","      <td>0.748333</td>\n","      <td>0.759285</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.715944</td>\n","      <td>0.711232</td>\n","      <td>0.726181</td>\n","      <td>0.698895</td>\n","      <td>0.674015</td>\n","      <td>0.725681</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000600</td>\n","      <td>0.062047</td>\n","      <td>0.751983</td>\n","      <td>0.749516</td>\n","      <td>0.759137</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.716582</td>\n","      <td>0.712633</td>\n","      <td>0.725985</td>\n","      <td>0.699044</td>\n","      <td>0.674629</td>\n","      <td>0.725292</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7519826415807248, 'eval_macro_precision': 0.7495156790061636, 'eval_macro_recall': 0.7591369720061498, 'eval_micro_f1': 0.9233800857772112, 'eval_micro_precision': 0.9233800857772111, 'eval_micro_recall': 0.9233800857772111, 'eval_macro_f1_no_o': 0.7165818621926509, 'eval_macro_precision_no_o': 0.7126333808495872, 'eval_macro_recall_no_o': 0.7259848710452207, 'eval_micro_f1_no_o': 0.6990436902306394, 'eval_micro_precision_no_o': 0.6746290264205573, 'eval_micro_recall_no_o': 0.7252918287937743, 'eval_loss': 0.062046617925434554, 'eval_runtime': 21.3991, 'eval_samples_per_second': 5.654, 'eval_steps_per_second': 1.449, 'epoch': 18.0}\n","Accuracy for fold  19 :  0.6990436902306394  --  0.9233800857772112\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #20\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:37, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026200</td>\n","      <td>0.096788</td>\n","      <td>0.750442</td>\n","      <td>0.742831</td>\n","      <td>0.762134</td>\n","      <td>0.926104</td>\n","      <td>0.926104</td>\n","      <td>0.926104</td>\n","      <td>0.714484</td>\n","      <td>0.704979</td>\n","      <td>0.728743</td>\n","      <td>0.702836</td>\n","      <td>0.688036</td>\n","      <td>0.718288</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009500</td>\n","      <td>0.071885</td>\n","      <td>0.759929</td>\n","      <td>0.752635</td>\n","      <td>0.769139</td>\n","      <td>0.928306</td>\n","      <td>0.928306</td>\n","      <td>0.928306</td>\n","      <td>0.725430</td>\n","      <td>0.716507</td>\n","      <td>0.736587</td>\n","      <td>0.711409</td>\n","      <td>0.701323</td>\n","      <td>0.721790</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004600</td>\n","      <td>0.066248</td>\n","      <td>0.768396</td>\n","      <td>0.792156</td>\n","      <td>0.753850</td>\n","      <td>0.929813</td>\n","      <td>0.929813</td>\n","      <td>0.929813</td>\n","      <td>0.735318</td>\n","      <td>0.763191</td>\n","      <td>0.718194</td>\n","      <td>0.716745</td>\n","      <td>0.720692</td>\n","      <td>0.712840</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002700</td>\n","      <td>0.061956</td>\n","      <td>0.768617</td>\n","      <td>0.780713</td>\n","      <td>0.760212</td>\n","      <td>0.929929</td>\n","      <td>0.929929</td>\n","      <td>0.929929</td>\n","      <td>0.735615</td>\n","      <td>0.749897</td>\n","      <td>0.725638</td>\n","      <td>0.718732</td>\n","      <td>0.723119</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002000</td>\n","      <td>0.062897</td>\n","      <td>0.756073</td>\n","      <td>0.779366</td>\n","      <td>0.743026</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.721153</td>\n","      <td>0.748400</td>\n","      <td>0.705861</td>\n","      <td>0.706456</td>\n","      <td>0.708252</td>\n","      <td>0.704669</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.059993</td>\n","      <td>0.762496</td>\n","      <td>0.774190</td>\n","      <td>0.756285</td>\n","      <td>0.928480</td>\n","      <td>0.928480</td>\n","      <td>0.928480</td>\n","      <td>0.728522</td>\n","      <td>0.741928</td>\n","      <td>0.721511</td>\n","      <td>0.714258</td>\n","      <td>0.708381</td>\n","      <td>0.720233</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001100</td>\n","      <td>0.058323</td>\n","      <td>0.753946</td>\n","      <td>0.759112</td>\n","      <td>0.760462</td>\n","      <td>0.924887</td>\n","      <td>0.924887</td>\n","      <td>0.924887</td>\n","      <td>0.718656</td>\n","      <td>0.723600</td>\n","      <td>0.727326</td>\n","      <td>0.701781</td>\n","      <td>0.677034</td>\n","      <td>0.728405</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.057591</td>\n","      <td>0.761144</td>\n","      <td>0.761431</td>\n","      <td>0.767870</td>\n","      <td>0.926684</td>\n","      <td>0.926684</td>\n","      <td>0.926684</td>\n","      <td>0.726985</td>\n","      <td>0.726400</td>\n","      <td>0.735743</td>\n","      <td>0.709763</td>\n","      <td>0.688231</td>\n","      <td>0.732685</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.059995</td>\n","      <td>0.762018</td>\n","      <td>0.779270</td>\n","      <td>0.750953</td>\n","      <td>0.928886</td>\n","      <td>0.928886</td>\n","      <td>0.928886</td>\n","      <td>0.727824</td>\n","      <td>0.747825</td>\n","      <td>0.715041</td>\n","      <td>0.711214</td>\n","      <td>0.708060</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000600</td>\n","      <td>0.058029</td>\n","      <td>0.766888</td>\n","      <td>0.770858</td>\n","      <td>0.766554</td>\n","      <td>0.928886</td>\n","      <td>0.928886</td>\n","      <td>0.928886</td>\n","      <td>0.733533</td>\n","      <td>0.737562</td>\n","      <td>0.733741</td>\n","      <td>0.716327</td>\n","      <td>0.701754</td>\n","      <td>0.731518</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.058935</td>\n","      <td>0.759073</td>\n","      <td>0.773266</td>\n","      <td>0.752710</td>\n","      <td>0.928133</td>\n","      <td>0.928133</td>\n","      <td>0.928133</td>\n","      <td>0.724461</td>\n","      <td>0.740740</td>\n","      <td>0.717318</td>\n","      <td>0.710075</td>\n","      <td>0.703167</td>\n","      <td>0.717121</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.059261</td>\n","      <td>0.757509</td>\n","      <td>0.762675</td>\n","      <td>0.757454</td>\n","      <td>0.928306</td>\n","      <td>0.928306</td>\n","      <td>0.928306</td>\n","      <td>0.722571</td>\n","      <td>0.728240</td>\n","      <td>0.722864</td>\n","      <td>0.709702</td>\n","      <td>0.700949</td>\n","      <td>0.718677</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.060394</td>\n","      <td>0.757292</td>\n","      <td>0.763181</td>\n","      <td>0.755878</td>\n","      <td>0.929350</td>\n","      <td>0.929350</td>\n","      <td>0.929350</td>\n","      <td>0.722180</td>\n","      <td>0.728897</td>\n","      <td>0.720685</td>\n","      <td>0.710139</td>\n","      <td>0.706313</td>\n","      <td>0.714008</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.060624</td>\n","      <td>0.753946</td>\n","      <td>0.752323</td>\n","      <td>0.759628</td>\n","      <td>0.928017</td>\n","      <td>0.928017</td>\n","      <td>0.928017</td>\n","      <td>0.718390</td>\n","      <td>0.716172</td>\n","      <td>0.725343</td>\n","      <td>0.706674</td>\n","      <td>0.698745</td>\n","      <td>0.714786</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.060798</td>\n","      <td>0.751758</td>\n","      <td>0.742194</td>\n","      <td>0.764741</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.716083</td>\n","      <td>0.704110</td>\n","      <td>0.732035</td>\n","      <td>0.703234</td>\n","      <td>0.684211</td>\n","      <td>0.723346</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.060119</td>\n","      <td>0.757976</td>\n","      <td>0.756789</td>\n","      <td>0.763485</td>\n","      <td>0.928538</td>\n","      <td>0.928538</td>\n","      <td>0.928538</td>\n","      <td>0.723107</td>\n","      <td>0.721286</td>\n","      <td>0.729968</td>\n","      <td>0.711631</td>\n","      <td>0.701019</td>\n","      <td>0.722568</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.060120</td>\n","      <td>0.757278</td>\n","      <td>0.756266</td>\n","      <td>0.762725</td>\n","      <td>0.928596</td>\n","      <td>0.928596</td>\n","      <td>0.928596</td>\n","      <td>0.722275</td>\n","      <td>0.720675</td>\n","      <td>0.729047</td>\n","      <td>0.711273</td>\n","      <td>0.701058</td>\n","      <td>0.721790</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.060119</td>\n","      <td>0.757340</td>\n","      <td>0.756366</td>\n","      <td>0.762735</td>\n","      <td>0.928654</td>\n","      <td>0.928654</td>\n","      <td>0.928654</td>\n","      <td>0.722341</td>\n","      <td>0.720792</td>\n","      <td>0.729047</td>\n","      <td>0.711409</td>\n","      <td>0.701323</td>\n","      <td>0.721790</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7573396251606864, 'eval_macro_precision': 0.7563662778130175, 'eval_macro_recall': 0.762734688008013, 'eval_micro_f1': 0.9286542251072215, 'eval_micro_precision': 0.9286542251072215, 'eval_micro_recall': 0.9286542251072215, 'eval_macro_f1_no_o': 0.7223412162110625, 'eval_macro_precision_no_o': 0.7207918026329958, 'eval_macro_recall_no_o': 0.7290471841342917, 'eval_micro_f1_no_o': 0.7114093959731544, 'eval_micro_precision_no_o': 0.7013232514177694, 'eval_micro_recall_no_o': 0.7217898832684825, 'eval_loss': 0.060119419237159216, 'eval_runtime': 21.6515, 'eval_samples_per_second': 5.589, 'eval_steps_per_second': 1.432, 'epoch': 18.0}\n","Accuracy for fold  20 :  0.7114093959731544  --  0.9286542251072215\n","--------------------------------\n","Testing process has finished.\n","Train run #21\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:19, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026100</td>\n","      <td>0.092739</td>\n","      <td>0.758251</td>\n","      <td>0.753661</td>\n","      <td>0.765964</td>\n","      <td>0.930103</td>\n","      <td>0.930103</td>\n","      <td>0.930103</td>\n","      <td>0.723295</td>\n","      <td>0.717554</td>\n","      <td>0.732678</td>\n","      <td>0.717083</td>\n","      <td>0.707576</td>\n","      <td>0.726848</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.010200</td>\n","      <td>0.070156</td>\n","      <td>0.741559</td>\n","      <td>0.735195</td>\n","      <td>0.750051</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.704584</td>\n","      <td>0.696485</td>\n","      <td>0.715158</td>\n","      <td>0.697073</td>\n","      <td>0.681278</td>\n","      <td>0.713619</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004300</td>\n","      <td>0.064520</td>\n","      <td>0.741647</td>\n","      <td>0.715087</td>\n","      <td>0.788153</td>\n","      <td>0.910803</td>\n","      <td>0.910803</td>\n","      <td>0.910803</td>\n","      <td>0.705967</td>\n","      <td>0.671374</td>\n","      <td>0.763674</td>\n","      <td>0.687327</td>\n","      <td>0.619152</td>\n","      <td>0.772374</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003100</td>\n","      <td>0.080495</td>\n","      <td>0.734619</td>\n","      <td>0.799986</td>\n","      <td>0.684773</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.696725</td>\n","      <td>0.775560</td>\n","      <td>0.635912</td>\n","      <td>0.684290</td>\n","      <td>0.764282</td>\n","      <td>0.619455</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.058257</td>\n","      <td>0.749721</td>\n","      <td>0.750839</td>\n","      <td>0.751429</td>\n","      <td>0.926452</td>\n","      <td>0.926452</td>\n","      <td>0.926452</td>\n","      <td>0.713746</td>\n","      <td>0.714736</td>\n","      <td>0.716050</td>\n","      <td>0.705792</td>\n","      <td>0.698135</td>\n","      <td>0.713619</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.058349</td>\n","      <td>0.750909</td>\n","      <td>0.757370</td>\n","      <td>0.748360</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.715063</td>\n","      <td>0.722426</td>\n","      <td>0.712265</td>\n","      <td>0.706497</td>\n","      <td>0.702152</td>\n","      <td>0.710895</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.059083</td>\n","      <td>0.743944</td>\n","      <td>0.743768</td>\n","      <td>0.750202</td>\n","      <td>0.923902</td>\n","      <td>0.923902</td>\n","      <td>0.923902</td>\n","      <td>0.707174</td>\n","      <td>0.706391</td>\n","      <td>0.715050</td>\n","      <td>0.697045</td>\n","      <td>0.683364</td>\n","      <td>0.711284</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.056571</td>\n","      <td>0.750675</td>\n","      <td>0.748244</td>\n","      <td>0.756649</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.714871</td>\n","      <td>0.711355</td>\n","      <td>0.722514</td>\n","      <td>0.699601</td>\n","      <td>0.683624</td>\n","      <td>0.716342</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.058107</td>\n","      <td>0.747729</td>\n","      <td>0.732695</td>\n","      <td>0.768881</td>\n","      <td>0.919845</td>\n","      <td>0.919845</td>\n","      <td>0.919845</td>\n","      <td>0.711885</td>\n","      <td>0.692598</td>\n","      <td>0.738272</td>\n","      <td>0.690996</td>\n","      <td>0.653486</td>\n","      <td>0.733074</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000700</td>\n","      <td>0.057431</td>\n","      <td>0.758473</td>\n","      <td>0.752512</td>\n","      <td>0.768002</td>\n","      <td>0.924018</td>\n","      <td>0.924018</td>\n","      <td>0.924018</td>\n","      <td>0.724183</td>\n","      <td>0.716141</td>\n","      <td>0.736372</td>\n","      <td>0.704273</td>\n","      <td>0.679320</td>\n","      <td>0.731128</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.057510</td>\n","      <td>0.759960</td>\n","      <td>0.756230</td>\n","      <td>0.768683</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.725856</td>\n","      <td>0.720562</td>\n","      <td>0.736963</td>\n","      <td>0.707721</td>\n","      <td>0.685766</td>\n","      <td>0.731128</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.059259</td>\n","      <td>0.756629</td>\n","      <td>0.755923</td>\n","      <td>0.763155</td>\n","      <td>0.924829</td>\n","      <td>0.924829</td>\n","      <td>0.924829</td>\n","      <td>0.721939</td>\n","      <td>0.720224</td>\n","      <td>0.730434</td>\n","      <td>0.704773</td>\n","      <td>0.683999</td>\n","      <td>0.726848</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.059149</td>\n","      <td>0.756292</td>\n","      <td>0.755130</td>\n","      <td>0.763193</td>\n","      <td>0.924597</td>\n","      <td>0.924597</td>\n","      <td>0.924597</td>\n","      <td>0.721606</td>\n","      <td>0.719421</td>\n","      <td>0.730478</td>\n","      <td>0.704726</td>\n","      <td>0.685294</td>\n","      <td>0.725292</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.059490</td>\n","      <td>0.757321</td>\n","      <td>0.757984</td>\n","      <td>0.762203</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.722751</td>\n","      <td>0.722823</td>\n","      <td>0.729142</td>\n","      <td>0.706284</td>\n","      <td>0.689655</td>\n","      <td>0.723735</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.059620</td>\n","      <td>0.758083</td>\n","      <td>0.758842</td>\n","      <td>0.762526</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.723580</td>\n","      <td>0.723863</td>\n","      <td>0.729360</td>\n","      <td>0.708325</td>\n","      <td>0.693916</td>\n","      <td>0.723346</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000500</td>\n","      <td>0.059847</td>\n","      <td>0.757148</td>\n","      <td>0.758068</td>\n","      <td>0.761490</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.722465</td>\n","      <td>0.722970</td>\n","      <td>0.728094</td>\n","      <td>0.707992</td>\n","      <td>0.694351</td>\n","      <td>0.722179</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.059812</td>\n","      <td>0.756865</td>\n","      <td>0.757086</td>\n","      <td>0.761855</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.722147</td>\n","      <td>0.721814</td>\n","      <td>0.728554</td>\n","      <td>0.707833</td>\n","      <td>0.693687</td>\n","      <td>0.722568</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000400</td>\n","      <td>0.059817</td>\n","      <td>0.756912</td>\n","      <td>0.757177</td>\n","      <td>0.761855</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.722202</td>\n","      <td>0.721920</td>\n","      <td>0.728554</td>\n","      <td>0.707833</td>\n","      <td>0.693687</td>\n","      <td>0.722568</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7569122313799383, 'eval_macro_precision': 0.757176890754461, 'eval_macro_recall': 0.7618550086114088, 'eval_micro_f1': 0.9260461342297438, 'eval_micro_precision': 0.9260461342297438, 'eval_micro_recall': 0.9260461342297438, 'eval_macro_f1_no_o': 0.7222016883500144, 'eval_macro_precision_no_o': 0.721920065350603, 'eval_macro_recall_no_o': 0.7285543519607449, 'eval_micro_f1_no_o': 0.707833047455689, 'eval_micro_precision_no_o': 0.6936869630183041, 'eval_micro_recall_no_o': 0.722568093385214, 'eval_loss': 0.059816877161661855, 'eval_runtime': 21.3853, 'eval_samples_per_second': 5.658, 'eval_steps_per_second': 1.45, 'epoch': 18.0}\n","Accuracy for fold  21 :  0.707833047455689  --  0.9260461342297438\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #22\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:46, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026200</td>\n","      <td>0.097707</td>\n","      <td>0.750227</td>\n","      <td>0.749995</td>\n","      <td>0.753947</td>\n","      <td>0.925467</td>\n","      <td>0.925467</td>\n","      <td>0.925467</td>\n","      <td>0.714469</td>\n","      <td>0.713653</td>\n","      <td>0.719351</td>\n","      <td>0.705860</td>\n","      <td>0.692769</td>\n","      <td>0.719455</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009400</td>\n","      <td>0.073411</td>\n","      <td>0.748668</td>\n","      <td>0.748402</td>\n","      <td>0.753313</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.712947</td>\n","      <td>0.711618</td>\n","      <td>0.719372</td>\n","      <td>0.700977</td>\n","      <td>0.677560</td>\n","      <td>0.726070</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004200</td>\n","      <td>0.065796</td>\n","      <td>0.742524</td>\n","      <td>0.752177</td>\n","      <td>0.742488</td>\n","      <td>0.920192</td>\n","      <td>0.920192</td>\n","      <td>0.920192</td>\n","      <td>0.705992</td>\n","      <td>0.716197</td>\n","      <td>0.706992</td>\n","      <td>0.692928</td>\n","      <td>0.668961</td>\n","      <td>0.718677</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002600</td>\n","      <td>0.069820</td>\n","      <td>0.750112</td>\n","      <td>0.793366</td>\n","      <td>0.721059</td>\n","      <td>0.928712</td>\n","      <td>0.928712</td>\n","      <td>0.928712</td>\n","      <td>0.714331</td>\n","      <td>0.765580</td>\n","      <td>0.679643</td>\n","      <td>0.715516</td>\n","      <td>0.736907</td>\n","      <td>0.695331</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.063800</td>\n","      <td>0.742513</td>\n","      <td>0.766329</td>\n","      <td>0.724237</td>\n","      <td>0.927611</td>\n","      <td>0.927611</td>\n","      <td>0.927611</td>\n","      <td>0.705327</td>\n","      <td>0.733694</td>\n","      <td>0.683418</td>\n","      <td>0.704947</td>\n","      <td>0.720260</td>\n","      <td>0.690272</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.062046</td>\n","      <td>0.746307</td>\n","      <td>0.765045</td>\n","      <td>0.734638</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.709970</td>\n","      <td>0.731759</td>\n","      <td>0.696426</td>\n","      <td>0.706385</td>\n","      <td>0.704607</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.066170</td>\n","      <td>0.746117</td>\n","      <td>0.774012</td>\n","      <td>0.726933</td>\n","      <td>0.927959</td>\n","      <td>0.927959</td>\n","      <td>0.927959</td>\n","      <td>0.709545</td>\n","      <td>0.742470</td>\n","      <td>0.686779</td>\n","      <td>0.709665</td>\n","      <td>0.719600</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.063623</td>\n","      <td>0.750332</td>\n","      <td>0.769251</td>\n","      <td>0.740940</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.714612</td>\n","      <td>0.736525</td>\n","      <td>0.703813</td>\n","      <td>0.712323</td>\n","      <td>0.708349</td>\n","      <td>0.716342</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.058041</td>\n","      <td>0.756006</td>\n","      <td>0.761992</td>\n","      <td>0.755587</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.721248</td>\n","      <td>0.727619</td>\n","      <td>0.721366</td>\n","      <td>0.713007</td>\n","      <td>0.698247</td>\n","      <td>0.728405</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.061470</td>\n","      <td>0.754079</td>\n","      <td>0.761510</td>\n","      <td>0.750143</td>\n","      <td>0.928944</td>\n","      <td>0.928944</td>\n","      <td>0.928944</td>\n","      <td>0.718708</td>\n","      <td>0.727454</td>\n","      <td>0.714039</td>\n","      <td>0.714787</td>\n","      <td>0.716745</td>\n","      <td>0.712840</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.060755</td>\n","      <td>0.760158</td>\n","      <td>0.770685</td>\n","      <td>0.754386</td>\n","      <td>0.928654</td>\n","      <td>0.928654</td>\n","      <td>0.928654</td>\n","      <td>0.725932</td>\n","      <td>0.738082</td>\n","      <td>0.719330</td>\n","      <td>0.719210</td>\n","      <td>0.715883</td>\n","      <td>0.722568</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.061926</td>\n","      <td>0.753867</td>\n","      <td>0.760103</td>\n","      <td>0.750508</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.718931</td>\n","      <td>0.725822</td>\n","      <td>0.715396</td>\n","      <td>0.705182</td>\n","      <td>0.695833</td>\n","      <td>0.714786</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.057941</td>\n","      <td>0.752811</td>\n","      <td>0.743099</td>\n","      <td>0.767200</td>\n","      <td>0.922974</td>\n","      <td>0.922974</td>\n","      <td>0.922974</td>\n","      <td>0.717809</td>\n","      <td>0.705162</td>\n","      <td>0.735891</td>\n","      <td>0.707086</td>\n","      <td>0.677235</td>\n","      <td>0.739689</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.059072</td>\n","      <td>0.753955</td>\n","      <td>0.748993</td>\n","      <td>0.763319</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.718994</td>\n","      <td>0.712214</td>\n","      <td>0.730899</td>\n","      <td>0.709156</td>\n","      <td>0.686068</td>\n","      <td>0.733852</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.059494</td>\n","      <td>0.754388</td>\n","      <td>0.753196</td>\n","      <td>0.760003</td>\n","      <td>0.925756</td>\n","      <td>0.925756</td>\n","      <td>0.925756</td>\n","      <td>0.719374</td>\n","      <td>0.717283</td>\n","      <td>0.726621</td>\n","      <td>0.710841</td>\n","      <td>0.694105</td>\n","      <td>0.728405</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.059704</td>\n","      <td>0.751777</td>\n","      <td>0.747859</td>\n","      <td>0.760628</td>\n","      <td>0.925235</td>\n","      <td>0.925235</td>\n","      <td>0.925235</td>\n","      <td>0.716350</td>\n","      <td>0.710950</td>\n","      <td>0.727497</td>\n","      <td>0.709263</td>\n","      <td>0.689706</td>\n","      <td>0.729961</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.059655</td>\n","      <td>0.752594</td>\n","      <td>0.749167</td>\n","      <td>0.760853</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.717250</td>\n","      <td>0.712484</td>\n","      <td>0.727646</td>\n","      <td>0.710741</td>\n","      <td>0.692506</td>\n","      <td>0.729961</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000400</td>\n","      <td>0.059667</td>\n","      <td>0.752352</td>\n","      <td>0.748987</td>\n","      <td>0.760547</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.716979</td>\n","      <td>0.712285</td>\n","      <td>0.727301</td>\n","      <td>0.710362</td>\n","      <td>0.692137</td>\n","      <td>0.729572</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7523521831789944, 'eval_macro_precision': 0.7489869839364999, 'eval_macro_recall': 0.7605473922703551, 'eval_micro_f1': 0.9256983887794135, 'eval_micro_precision': 0.9256983887794135, 'eval_micro_recall': 0.9256983887794135, 'eval_macro_f1_no_o': 0.7169792200381573, 'eval_macro_precision_no_o': 0.712284975014263, 'eval_macro_recall_no_o': 0.7273012049019938, 'eval_micro_f1_no_o': 0.7103618109490434, 'eval_micro_precision_no_o': 0.6921373200442967, 'eval_micro_recall_no_o': 0.7295719844357976, 'eval_loss': 0.059667212139941486, 'eval_runtime': 22.5456, 'eval_samples_per_second': 5.367, 'eval_steps_per_second': 1.375, 'epoch': 18.0}\n","Accuracy for fold  22 :  0.7103618109490434  --  0.9256983887794135\n","--------------------------------\n","Testing process has finished.\n","Train run #23\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:19, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026000</td>\n","      <td>0.096977</td>\n","      <td>0.745561</td>\n","      <td>0.721208</td>\n","      <td>0.779471</td>\n","      <td>0.917352</td>\n","      <td>0.917352</td>\n","      <td>0.917352</td>\n","      <td>0.709733</td>\n","      <td>0.678738</td>\n","      <td>0.751796</td>\n","      <td>0.694410</td>\n","      <td>0.641796</td>\n","      <td>0.756420</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009900</td>\n","      <td>0.071376</td>\n","      <td>0.759386</td>\n","      <td>0.777154</td>\n","      <td>0.746288</td>\n","      <td>0.928480</td>\n","      <td>0.928480</td>\n","      <td>0.928480</td>\n","      <td>0.725054</td>\n","      <td>0.745811</td>\n","      <td>0.709746</td>\n","      <td>0.717429</td>\n","      <td>0.718129</td>\n","      <td>0.716732</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004300</td>\n","      <td>0.061294</td>\n","      <td>0.757412</td>\n","      <td>0.768766</td>\n","      <td>0.752599</td>\n","      <td>0.925409</td>\n","      <td>0.925409</td>\n","      <td>0.925409</td>\n","      <td>0.723042</td>\n","      <td>0.735531</td>\n","      <td>0.718175</td>\n","      <td>0.713663</td>\n","      <td>0.695604</td>\n","      <td>0.732685</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002700</td>\n","      <td>0.058586</td>\n","      <td>0.755877</td>\n","      <td>0.756713</td>\n","      <td>0.757208</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.720977</td>\n","      <td>0.721345</td>\n","      <td>0.723133</td>\n","      <td>0.708571</td>\n","      <td>0.694030</td>\n","      <td>0.723735</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001800</td>\n","      <td>0.059543</td>\n","      <td>0.758770</td>\n","      <td>0.778779</td>\n","      <td>0.744115</td>\n","      <td>0.928828</td>\n","      <td>0.928828</td>\n","      <td>0.928828</td>\n","      <td>0.724260</td>\n","      <td>0.747670</td>\n","      <td>0.707097</td>\n","      <td>0.716849</td>\n","      <td>0.718530</td>\n","      <td>0.715175</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001900</td>\n","      <td>0.060291</td>\n","      <td>0.757740</td>\n","      <td>0.775866</td>\n","      <td>0.747217</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.723094</td>\n","      <td>0.743623</td>\n","      <td>0.711431</td>\n","      <td>0.707159</td>\n","      <td>0.692394</td>\n","      <td>0.722568</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001100</td>\n","      <td>0.083313</td>\n","      <td>0.723242</td>\n","      <td>0.807422</td>\n","      <td>0.668613</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.683040</td>\n","      <td>0.783500</td>\n","      <td>0.616991</td>\n","      <td>0.684333</td>\n","      <td>0.751747</td>\n","      <td>0.628016</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.052778</td>\n","      <td>0.770432</td>\n","      <td>0.773465</td>\n","      <td>0.773468</td>\n","      <td>0.929002</td>\n","      <td>0.929002</td>\n","      <td>0.929002</td>\n","      <td>0.737673</td>\n","      <td>0.740464</td>\n","      <td>0.741956</td>\n","      <td>0.718483</td>\n","      <td>0.700555</td>\n","      <td>0.737354</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.061420</td>\n","      <td>0.760587</td>\n","      <td>0.777347</td>\n","      <td>0.751145</td>\n","      <td>0.927205</td>\n","      <td>0.927205</td>\n","      <td>0.927205</td>\n","      <td>0.726425</td>\n","      <td>0.745737</td>\n","      <td>0.715650</td>\n","      <td>0.710262</td>\n","      <td>0.704285</td>\n","      <td>0.716342</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.060282</td>\n","      <td>0.762270</td>\n","      <td>0.783585</td>\n","      <td>0.746114</td>\n","      <td>0.927321</td>\n","      <td>0.927321</td>\n","      <td>0.927321</td>\n","      <td>0.728418</td>\n","      <td>0.753345</td>\n","      <td>0.709509</td>\n","      <td>0.709300</td>\n","      <td>0.710825</td>\n","      <td>0.707782</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.058000</td>\n","      <td>0.763428</td>\n","      <td>0.783220</td>\n","      <td>0.748405</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.729744</td>\n","      <td>0.752801</td>\n","      <td>0.712249</td>\n","      <td>0.712787</td>\n","      <td>0.711957</td>\n","      <td>0.713619</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.059677</td>\n","      <td>0.763862</td>\n","      <td>0.785347</td>\n","      <td>0.746963</td>\n","      <td>0.928075</td>\n","      <td>0.928075</td>\n","      <td>0.928075</td>\n","      <td>0.730278</td>\n","      <td>0.755485</td>\n","      <td>0.710420</td>\n","      <td>0.713727</td>\n","      <td>0.717374</td>\n","      <td>0.710117</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.060593</td>\n","      <td>0.763414</td>\n","      <td>0.782482</td>\n","      <td>0.748096</td>\n","      <td>0.927727</td>\n","      <td>0.927727</td>\n","      <td>0.927727</td>\n","      <td>0.729785</td>\n","      <td>0.752114</td>\n","      <td>0.711832</td>\n","      <td>0.712976</td>\n","      <td>0.715068</td>\n","      <td>0.710895</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.055819</td>\n","      <td>0.766824</td>\n","      <td>0.770950</td>\n","      <td>0.765583</td>\n","      <td>0.928191</td>\n","      <td>0.928191</td>\n","      <td>0.928191</td>\n","      <td>0.733583</td>\n","      <td>0.737805</td>\n","      <td>0.732722</td>\n","      <td>0.715838</td>\n","      <td>0.701532</td>\n","      <td>0.730739</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.056911</td>\n","      <td>0.768110</td>\n","      <td>0.777877</td>\n","      <td>0.761829</td>\n","      <td>0.928480</td>\n","      <td>0.928480</td>\n","      <td>0.928480</td>\n","      <td>0.735083</td>\n","      <td>0.746048</td>\n","      <td>0.728184</td>\n","      <td>0.716366</td>\n","      <td>0.705816</td>\n","      <td>0.727237</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.058418</td>\n","      <td>0.766608</td>\n","      <td>0.780880</td>\n","      <td>0.756231</td>\n","      <td>0.928306</td>\n","      <td>0.928306</td>\n","      <td>0.928306</td>\n","      <td>0.733371</td>\n","      <td>0.749758</td>\n","      <td>0.721528</td>\n","      <td>0.715112</td>\n","      <td>0.708556</td>\n","      <td>0.721790</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.058751</td>\n","      <td>0.765792</td>\n","      <td>0.779630</td>\n","      <td>0.755840</td>\n","      <td>0.928249</td>\n","      <td>0.928249</td>\n","      <td>0.928249</td>\n","      <td>0.732419</td>\n","      <td>0.748300</td>\n","      <td>0.721071</td>\n","      <td>0.714726</td>\n","      <td>0.708174</td>\n","      <td>0.721401</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000400</td>\n","      <td>0.058799</td>\n","      <td>0.765968</td>\n","      <td>0.780397</td>\n","      <td>0.755557</td>\n","      <td>0.928249</td>\n","      <td>0.928249</td>\n","      <td>0.928249</td>\n","      <td>0.732624</td>\n","      <td>0.749205</td>\n","      <td>0.720730</td>\n","      <td>0.714616</td>\n","      <td>0.708333</td>\n","      <td>0.721012</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7659682471038151, 'eval_macro_precision': 0.7803970056746233, 'eval_macro_recall': 0.7555568533016958, 'eval_micro_f1': 0.9282485220818361, 'eval_micro_precision': 0.9282485220818361, 'eval_micro_recall': 0.9282485220818361, 'eval_macro_f1_no_o': 0.7326242331755819, 'eval_macro_precision_no_o': 0.7492048046574663, 'eval_macro_recall_no_o': 0.72072979475591, 'eval_micro_f1_no_o': 0.7146162745854224, 'eval_micro_precision_no_o': 0.7083333333333334, 'eval_micro_recall_no_o': 0.721011673151751, 'eval_loss': 0.05879911346349496, 'eval_runtime': 21.6295, 'eval_samples_per_second': 5.594, 'eval_steps_per_second': 1.433, 'epoch': 18.0}\n","Accuracy for fold  23 :  0.7146162745854224  --  0.9282485220818361\n","--------------------------------\n","Testing process has finished.\n","Train run #24\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:43, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025800</td>\n","      <td>0.099181</td>\n","      <td>0.757362</td>\n","      <td>0.796863</td>\n","      <td>0.727041</td>\n","      <td>0.929060</td>\n","      <td>0.929060</td>\n","      <td>0.929060</td>\n","      <td>0.722624</td>\n","      <td>0.769575</td>\n","      <td>0.686371</td>\n","      <td>0.711245</td>\n","      <td>0.734855</td>\n","      <td>0.689105</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009500</td>\n","      <td>0.073125</td>\n","      <td>0.747954</td>\n","      <td>0.783876</td>\n","      <td>0.722753</td>\n","      <td>0.927495</td>\n","      <td>0.927495</td>\n","      <td>0.927495</td>\n","      <td>0.711633</td>\n","      <td>0.754162</td>\n","      <td>0.681607</td>\n","      <td>0.702348</td>\n","      <td>0.718648</td>\n","      <td>0.686770</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004200</td>\n","      <td>0.072759</td>\n","      <td>0.735482</td>\n","      <td>0.768766</td>\n","      <td>0.714150</td>\n","      <td>0.923670</td>\n","      <td>0.923670</td>\n","      <td>0.923670</td>\n","      <td>0.697439</td>\n","      <td>0.736445</td>\n","      <td>0.672376</td>\n","      <td>0.693031</td>\n","      <td>0.697400</td>\n","      <td>0.688716</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002800</td>\n","      <td>0.064439</td>\n","      <td>0.728730</td>\n","      <td>0.702780</td>\n","      <td>0.775138</td>\n","      <td>0.916425</td>\n","      <td>0.916425</td>\n","      <td>0.916425</td>\n","      <td>0.690142</td>\n","      <td>0.657719</td>\n","      <td>0.746378</td>\n","      <td>0.686459</td>\n","      <td>0.641842</td>\n","      <td>0.737743</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002100</td>\n","      <td>0.058636</td>\n","      <td>0.745620</td>\n","      <td>0.750375</td>\n","      <td>0.746302</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.709154</td>\n","      <td>0.714078</td>\n","      <td>0.710568</td>\n","      <td>0.700171</td>\n","      <td>0.685427</td>\n","      <td>0.715564</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.060438</td>\n","      <td>0.744149</td>\n","      <td>0.754912</td>\n","      <td>0.739496</td>\n","      <td>0.924829</td>\n","      <td>0.924829</td>\n","      <td>0.924829</td>\n","      <td>0.707435</td>\n","      <td>0.719617</td>\n","      <td>0.702378</td>\n","      <td>0.701997</td>\n","      <td>0.692949</td>\n","      <td>0.711284</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.059295</td>\n","      <td>0.746711</td>\n","      <td>0.754332</td>\n","      <td>0.744970</td>\n","      <td>0.924887</td>\n","      <td>0.924887</td>\n","      <td>0.924887</td>\n","      <td>0.710415</td>\n","      <td>0.718844</td>\n","      <td>0.708844</td>\n","      <td>0.702910</td>\n","      <td>0.691786</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.058356</td>\n","      <td>0.753529</td>\n","      <td>0.760547</td>\n","      <td>0.751454</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.718419</td>\n","      <td>0.726033</td>\n","      <td>0.716568</td>\n","      <td>0.704424</td>\n","      <td>0.690726</td>\n","      <td>0.718677</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.058063</td>\n","      <td>0.755821</td>\n","      <td>0.763825</td>\n","      <td>0.751242</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.926278</td>\n","      <td>0.720969</td>\n","      <td>0.730042</td>\n","      <td>0.715889</td>\n","      <td>0.707787</td>\n","      <td>0.701299</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.057485</td>\n","      <td>0.743296</td>\n","      <td>0.740998</td>\n","      <td>0.751634</td>\n","      <td>0.921120</td>\n","      <td>0.921120</td>\n","      <td>0.921120</td>\n","      <td>0.706798</td>\n","      <td>0.703009</td>\n","      <td>0.717617</td>\n","      <td>0.696255</td>\n","      <td>0.671119</td>\n","      <td>0.723346</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.058411</td>\n","      <td>0.744343</td>\n","      <td>0.746051</td>\n","      <td>0.748378</td>\n","      <td>0.922453</td>\n","      <td>0.922453</td>\n","      <td>0.922453</td>\n","      <td>0.707912</td>\n","      <td>0.709049</td>\n","      <td>0.713467</td>\n","      <td>0.699150</td>\n","      <td>0.679266</td>\n","      <td>0.720233</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.060120</td>\n","      <td>0.744003</td>\n","      <td>0.750907</td>\n","      <td>0.742231</td>\n","      <td>0.923206</td>\n","      <td>0.923206</td>\n","      <td>0.923206</td>\n","      <td>0.707486</td>\n","      <td>0.715023</td>\n","      <td>0.705932</td>\n","      <td>0.700038</td>\n","      <td>0.687688</td>\n","      <td>0.712840</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.061451</td>\n","      <td>0.742627</td>\n","      <td>0.752023</td>\n","      <td>0.738289</td>\n","      <td>0.923670</td>\n","      <td>0.923670</td>\n","      <td>0.923670</td>\n","      <td>0.705806</td>\n","      <td>0.716395</td>\n","      <td>0.701117</td>\n","      <td>0.699309</td>\n","      <td>0.690296</td>\n","      <td>0.708560</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.059684</td>\n","      <td>0.748262</td>\n","      <td>0.750064</td>\n","      <td>0.750046</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.924713</td>\n","      <td>0.712312</td>\n","      <td>0.713913</td>\n","      <td>0.714890</td>\n","      <td>0.705028</td>\n","      <td>0.692973</td>\n","      <td>0.717510</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.064381</td>\n","      <td>0.752300</td>\n","      <td>0.765741</td>\n","      <td>0.743170</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.925814</td>\n","      <td>0.716969</td>\n","      <td>0.732508</td>\n","      <td>0.706460</td>\n","      <td>0.707317</td>\n","      <td>0.703775</td>\n","      <td>0.710895</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.062236</td>\n","      <td>0.749754</td>\n","      <td>0.764946</td>\n","      <td>0.739952</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.714037</td>\n","      <td>0.731531</td>\n","      <td>0.702831</td>\n","      <td>0.703589</td>\n","      <td>0.697933</td>\n","      <td>0.709339</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.062330</td>\n","      <td>0.749598</td>\n","      <td>0.763227</td>\n","      <td>0.741008</td>\n","      <td>0.925003</td>\n","      <td>0.925003</td>\n","      <td>0.925003</td>\n","      <td>0.713833</td>\n","      <td>0.729480</td>\n","      <td>0.704062</td>\n","      <td>0.703432</td>\n","      <td>0.697248</td>\n","      <td>0.709728</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000400</td>\n","      <td>0.062344</td>\n","      <td>0.750255</td>\n","      <td>0.764502</td>\n","      <td>0.741176</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.714611</td>\n","      <td>0.730990</td>\n","      <td>0.704258</td>\n","      <td>0.704090</td>\n","      <td>0.698164</td>\n","      <td>0.710117</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7502554898676674, 'eval_macro_precision': 0.7645019519094788, 'eval_macro_recall': 0.7411755871698344, 'eval_micro_f1': 0.9250608554538078, 'eval_micro_precision': 0.9250608554538078, 'eval_micro_recall': 0.9250608554538078, 'eval_macro_f1_no_o': 0.7146105800377186, 'eval_macro_precision_no_o': 0.730989982145758, 'eval_macro_recall_no_o': 0.704258106941943, 'eval_micro_f1_no_o': 0.7040895061728395, 'eval_micro_precision_no_o': 0.6981637337413925, 'eval_micro_recall_no_o': 0.7101167315175098, 'eval_loss': 0.06234430062104817, 'eval_runtime': 21.832, 'eval_samples_per_second': 5.542, 'eval_steps_per_second': 1.42, 'epoch': 18.0}\n","Accuracy for fold  24 :  0.7040895061728395  --  0.9250608554538078\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #25\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 41:05, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026100</td>\n","      <td>0.108394</td>\n","      <td>0.717296</td>\n","      <td>0.791670</td>\n","      <td>0.671172</td>\n","      <td>0.925872</td>\n","      <td>0.925872</td>\n","      <td>0.925872</td>\n","      <td>0.676083</td>\n","      <td>0.764740</td>\n","      <td>0.620340</td>\n","      <td>0.685607</td>\n","      <td>0.740189</td>\n","      <td>0.638521</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009500</td>\n","      <td>0.075819</td>\n","      <td>0.749230</td>\n","      <td>0.735863</td>\n","      <td>0.765668</td>\n","      <td>0.920598</td>\n","      <td>0.920598</td>\n","      <td>0.920598</td>\n","      <td>0.713690</td>\n","      <td>0.696734</td>\n","      <td>0.734206</td>\n","      <td>0.694150</td>\n","      <td>0.663943</td>\n","      <td>0.727237</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004200</td>\n","      <td>0.066404</td>\n","      <td>0.752684</td>\n","      <td>0.727043</td>\n","      <td>0.782692</td>\n","      <td>0.918801</td>\n","      <td>0.918801</td>\n","      <td>0.918801</td>\n","      <td>0.717975</td>\n","      <td>0.686042</td>\n","      <td>0.754953</td>\n","      <td>0.696601</td>\n","      <td>0.653702</td>\n","      <td>0.745525</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002600</td>\n","      <td>0.068826</td>\n","      <td>0.754869</td>\n","      <td>0.750441</td>\n","      <td>0.760086</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.923090</td>\n","      <td>0.720131</td>\n","      <td>0.714531</td>\n","      <td>0.726649</td>\n","      <td>0.697452</td>\n","      <td>0.687052</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.062110</td>\n","      <td>0.755910</td>\n","      <td>0.758636</td>\n","      <td>0.755814</td>\n","      <td>0.921815</td>\n","      <td>0.921815</td>\n","      <td>0.921815</td>\n","      <td>0.721510</td>\n","      <td>0.723985</td>\n","      <td>0.722097</td>\n","      <td>0.697039</td>\n","      <td>0.680504</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.057705</td>\n","      <td>0.757428</td>\n","      <td>0.755560</td>\n","      <td>0.762943</td>\n","      <td>0.921699</td>\n","      <td>0.921699</td>\n","      <td>0.921699</td>\n","      <td>0.723275</td>\n","      <td>0.720095</td>\n","      <td>0.730698</td>\n","      <td>0.698741</td>\n","      <td>0.675754</td>\n","      <td>0.723346</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.057655</td>\n","      <td>0.739743</td>\n","      <td>0.726796</td>\n","      <td>0.770762</td>\n","      <td>0.906456</td>\n","      <td>0.906456</td>\n","      <td>0.906456</td>\n","      <td>0.703892</td>\n","      <td>0.685030</td>\n","      <td>0.743668</td>\n","      <td>0.667011</td>\n","      <td>0.598700</td>\n","      <td>0.752918</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.058878</td>\n","      <td>0.751137</td>\n","      <td>0.761983</td>\n","      <td>0.745719</td>\n","      <td>0.922221</td>\n","      <td>0.922221</td>\n","      <td>0.922221</td>\n","      <td>0.715800</td>\n","      <td>0.728009</td>\n","      <td>0.709922</td>\n","      <td>0.692588</td>\n","      <td>0.682007</td>\n","      <td>0.703502</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.053155</td>\n","      <td>0.754114</td>\n","      <td>0.743294</td>\n","      <td>0.772635</td>\n","      <td>0.919903</td>\n","      <td>0.919903</td>\n","      <td>0.919903</td>\n","      <td>0.719384</td>\n","      <td>0.704756</td>\n","      <td>0.742947</td>\n","      <td>0.695162</td>\n","      <td>0.652664</td>\n","      <td>0.743580</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.054451</td>\n","      <td>0.759349</td>\n","      <td>0.756189</td>\n","      <td>0.767110</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.725317</td>\n","      <td>0.720438</td>\n","      <td>0.735548</td>\n","      <td>0.702521</td>\n","      <td>0.675404</td>\n","      <td>0.731907</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.054575</td>\n","      <td>0.760014</td>\n","      <td>0.756814</td>\n","      <td>0.767219</td>\n","      <td>0.923612</td>\n","      <td>0.923612</td>\n","      <td>0.923612</td>\n","      <td>0.726061</td>\n","      <td>0.721241</td>\n","      <td>0.735539</td>\n","      <td>0.704273</td>\n","      <td>0.679320</td>\n","      <td>0.731128</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.056841</td>\n","      <td>0.762844</td>\n","      <td>0.766701</td>\n","      <td>0.760934</td>\n","      <td>0.926104</td>\n","      <td>0.926104</td>\n","      <td>0.926104</td>\n","      <td>0.729195</td>\n","      <td>0.733432</td>\n","      <td>0.727230</td>\n","      <td>0.707787</td>\n","      <td>0.701299</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.055891</td>\n","      <td>0.763565</td>\n","      <td>0.760865</td>\n","      <td>0.769373</td>\n","      <td>0.925351</td>\n","      <td>0.925351</td>\n","      <td>0.925351</td>\n","      <td>0.730080</td>\n","      <td>0.726101</td>\n","      <td>0.737678</td>\n","      <td>0.709263</td>\n","      <td>0.689706</td>\n","      <td>0.729961</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.054483</td>\n","      <td>0.761700</td>\n","      <td>0.752189</td>\n","      <td>0.775133</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.728100</td>\n","      <td>0.715675</td>\n","      <td>0.745079</td>\n","      <td>0.706823</td>\n","      <td>0.676753</td>\n","      <td>0.739689</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.056032</td>\n","      <td>0.763567</td>\n","      <td>0.756744</td>\n","      <td>0.773485</td>\n","      <td>0.924249</td>\n","      <td>0.924249</td>\n","      <td>0.924249</td>\n","      <td>0.730202</td>\n","      <td>0.721127</td>\n","      <td>0.742872</td>\n","      <td>0.708482</td>\n","      <td>0.682786</td>\n","      <td>0.736187</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.055986</td>\n","      <td>0.764642</td>\n","      <td>0.757130</td>\n","      <td>0.775218</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.731467</td>\n","      <td>0.721589</td>\n","      <td>0.744905</td>\n","      <td>0.710728</td>\n","      <td>0.684951</td>\n","      <td>0.738521</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.055671</td>\n","      <td>0.763461</td>\n","      <td>0.755313</td>\n","      <td>0.775141</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.923960</td>\n","      <td>0.730104</td>\n","      <td>0.719394</td>\n","      <td>0.744917</td>\n","      <td>0.708232</td>\n","      <td>0.680660</td>\n","      <td>0.738132</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.055686</td>\n","      <td>0.763870</td>\n","      <td>0.756084</td>\n","      <td>0.775160</td>\n","      <td>0.924076</td>\n","      <td>0.924076</td>\n","      <td>0.924076</td>\n","      <td>0.730569</td>\n","      <td>0.720293</td>\n","      <td>0.744917</td>\n","      <td>0.708497</td>\n","      <td>0.681149</td>\n","      <td>0.738132</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7638695930817108, 'eval_macro_precision': 0.7560843466139465, 'eval_macro_recall': 0.7751602975394211, 'eval_micro_f1': 0.9240755766778717, 'eval_micro_precision': 0.9240755766778718, 'eval_micro_recall': 0.9240755766778718, 'eval_macro_f1_no_o': 0.7305687906892246, 'eval_macro_precision_no_o': 0.7202927048413891, 'eval_macro_recall_no_o': 0.7449171055057887, 'eval_micro_f1_no_o': 0.7084967320261438, 'eval_micro_precision_no_o': 0.681149012567325, 'eval_micro_recall_no_o': 0.7381322957198444, 'eval_loss': 0.05568594649203303, 'eval_runtime': 20.8316, 'eval_samples_per_second': 5.808, 'eval_steps_per_second': 1.488, 'epoch': 18.0}\n","Accuracy for fold  25 :  0.7084967320261438  --  0.9240755766778717\n","--------------------------------\n","Testing process has finished.\n","Train run #26\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 40:23, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025900</td>\n","      <td>0.094110</td>\n","      <td>0.758361</td>\n","      <td>0.768203</td>\n","      <td>0.751398</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.926220</td>\n","      <td>0.723975</td>\n","      <td>0.735259</td>\n","      <td>0.716048</td>\n","      <td>0.708269</td>\n","      <td>0.703377</td>\n","      <td>0.713230</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009300</td>\n","      <td>0.072742</td>\n","      <td>0.751198</td>\n","      <td>0.761144</td>\n","      <td>0.751692</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.715919</td>\n","      <td>0.726873</td>\n","      <td>0.717140</td>\n","      <td>0.698364</td>\n","      <td>0.683036</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004800</td>\n","      <td>0.078669</td>\n","      <td>0.722522</td>\n","      <td>0.744335</td>\n","      <td>0.709486</td>\n","      <td>0.915962</td>\n","      <td>0.915962</td>\n","      <td>0.915962</td>\n","      <td>0.683060</td>\n","      <td>0.708720</td>\n","      <td>0.667639</td>\n","      <td>0.666144</td>\n","      <td>0.671276</td>\n","      <td>0.661089</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002600</td>\n","      <td>0.066826</td>\n","      <td>0.741388</td>\n","      <td>0.762347</td>\n","      <td>0.727279</td>\n","      <td>0.921989</td>\n","      <td>0.921989</td>\n","      <td>0.921989</td>\n","      <td>0.704706</td>\n","      <td>0.729099</td>\n","      <td>0.688307</td>\n","      <td>0.696952</td>\n","      <td>0.695467</td>\n","      <td>0.698444</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.061883</td>\n","      <td>0.735042</td>\n","      <td>0.731169</td>\n","      <td>0.744173</td>\n","      <td>0.917932</td>\n","      <td>0.917932</td>\n","      <td>0.917932</td>\n","      <td>0.697546</td>\n","      <td>0.691890</td>\n","      <td>0.709322</td>\n","      <td>0.688365</td>\n","      <td>0.662824</td>\n","      <td>0.715953</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.002500</td>\n","      <td>0.061978</td>\n","      <td>0.725603</td>\n","      <td>0.729647</td>\n","      <td>0.728721</td>\n","      <td>0.916193</td>\n","      <td>0.916193</td>\n","      <td>0.916193</td>\n","      <td>0.686541</td>\n","      <td>0.690093</td>\n","      <td>0.691328</td>\n","      <td>0.677630</td>\n","      <td>0.651924</td>\n","      <td>0.705447</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.063323</td>\n","      <td>0.736000</td>\n","      <td>0.743048</td>\n","      <td>0.733616</td>\n","      <td>0.920250</td>\n","      <td>0.920250</td>\n","      <td>0.920250</td>\n","      <td>0.698381</td>\n","      <td>0.706125</td>\n","      <td>0.696074</td>\n","      <td>0.687966</td>\n","      <td>0.676703</td>\n","      <td>0.699611</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.065078</td>\n","      <td>0.744235</td>\n","      <td>0.760843</td>\n","      <td>0.731389</td>\n","      <td>0.923844</td>\n","      <td>0.923844</td>\n","      <td>0.923844</td>\n","      <td>0.707787</td>\n","      <td>0.727326</td>\n","      <td>0.692636</td>\n","      <td>0.699022</td>\n","      <td>0.703150</td>\n","      <td>0.694942</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.066234</td>\n","      <td>0.735568</td>\n","      <td>0.746476</td>\n","      <td>0.727878</td>\n","      <td>0.921467</td>\n","      <td>0.921467</td>\n","      <td>0.921467</td>\n","      <td>0.697730</td>\n","      <td>0.710380</td>\n","      <td>0.688836</td>\n","      <td>0.687233</td>\n","      <td>0.685372</td>\n","      <td>0.689105</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000400</td>\n","      <td>0.064790</td>\n","      <td>0.741108</td>\n","      <td>0.749203</td>\n","      <td>0.735187</td>\n","      <td>0.922453</td>\n","      <td>0.922453</td>\n","      <td>0.922453</td>\n","      <td>0.704157</td>\n","      <td>0.713453</td>\n","      <td>0.697396</td>\n","      <td>0.693246</td>\n","      <td>0.689642</td>\n","      <td>0.696887</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.063481</td>\n","      <td>0.743501</td>\n","      <td>0.745111</td>\n","      <td>0.744351</td>\n","      <td>0.921410</td>\n","      <td>0.921410</td>\n","      <td>0.921410</td>\n","      <td>0.707045</td>\n","      <td>0.708325</td>\n","      <td>0.708632</td>\n","      <td>0.693846</td>\n","      <td>0.679731</td>\n","      <td>0.708560</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.064014</td>\n","      <td>0.748464</td>\n","      <td>0.746775</td>\n","      <td>0.751299</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.712872</td>\n","      <td>0.710374</td>\n","      <td>0.716705</td>\n","      <td>0.698625</td>\n","      <td>0.686047</td>\n","      <td>0.711673</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.064488</td>\n","      <td>0.747645</td>\n","      <td>0.756586</td>\n","      <td>0.742470</td>\n","      <td>0.923496</td>\n","      <td>0.923496</td>\n","      <td>0.923496</td>\n","      <td>0.711777</td>\n","      <td>0.721928</td>\n","      <td>0.706018</td>\n","      <td>0.701214</td>\n","      <td>0.694391</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.065137</td>\n","      <td>0.748977</td>\n","      <td>0.759232</td>\n","      <td>0.742653</td>\n","      <td>0.923844</td>\n","      <td>0.923844</td>\n","      <td>0.923844</td>\n","      <td>0.713300</td>\n","      <td>0.725023</td>\n","      <td>0.706163</td>\n","      <td>0.702160</td>\n","      <td>0.696251</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.064570</td>\n","      <td>0.745944</td>\n","      <td>0.751955</td>\n","      <td>0.743925</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.709778</td>\n","      <td>0.716279</td>\n","      <td>0.707931</td>\n","      <td>0.698261</td>\n","      <td>0.686068</td>\n","      <td>0.710895</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.063681</td>\n","      <td>0.746022</td>\n","      <td>0.749004</td>\n","      <td>0.747472</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.709946</td>\n","      <td>0.712703</td>\n","      <td>0.712353</td>\n","      <td>0.697780</td>\n","      <td>0.680859</td>\n","      <td>0.715564</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.063565</td>\n","      <td>0.745840</td>\n","      <td>0.748645</td>\n","      <td>0.747473</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.709751</td>\n","      <td>0.712274</td>\n","      <td>0.712399</td>\n","      <td>0.697498</td>\n","      <td>0.679970</td>\n","      <td>0.715953</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000700</td>\n","      <td>0.063527</td>\n","      <td>0.745478</td>\n","      <td>0.747957</td>\n","      <td>0.747463</td>\n","      <td>0.921815</td>\n","      <td>0.921815</td>\n","      <td>0.921815</td>\n","      <td>0.709324</td>\n","      <td>0.711450</td>\n","      <td>0.712399</td>\n","      <td>0.697102</td>\n","      <td>0.679217</td>\n","      <td>0.715953</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7454777019390336, 'eval_macro_precision': 0.7479572118785659, 'eval_macro_recall': 0.747463116850177, 'eval_micro_f1': 0.9218152312507245, 'eval_micro_precision': 0.9218152312507245, 'eval_micro_recall': 0.9218152312507245, 'eval_macro_f1_no_o': 0.7093239262937043, 'eval_macro_precision_no_o': 0.7114502524053649, 'eval_macro_recall_no_o': 0.7123994240306454, 'eval_micro_f1_no_o': 0.697101723811328, 'eval_micro_precision_no_o': 0.6792174234034699, 'eval_micro_recall_no_o': 0.7159533073929961, 'eval_loss': 0.06352651312514083, 'eval_runtime': 22.586, 'eval_samples_per_second': 5.357, 'eval_steps_per_second': 1.373, 'epoch': 18.0}\n","Accuracy for fold  26 :  0.697101723811328  --  0.9218152312507245\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #27\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 48:02, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025800</td>\n","      <td>0.099538</td>\n","      <td>0.740746</td>\n","      <td>0.759336</td>\n","      <td>0.725974</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.923322</td>\n","      <td>0.703795</td>\n","      <td>0.725971</td>\n","      <td>0.686069</td>\n","      <td>0.695050</td>\n","      <td>0.707661</td>\n","      <td>0.682879</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009200</td>\n","      <td>0.075092</td>\n","      <td>0.749475</td>\n","      <td>0.764798</td>\n","      <td>0.737200</td>\n","      <td>0.925409</td>\n","      <td>0.925409</td>\n","      <td>0.925409</td>\n","      <td>0.713721</td>\n","      <td>0.731707</td>\n","      <td>0.699290</td>\n","      <td>0.703906</td>\n","      <td>0.706667</td>\n","      <td>0.701167</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004200</td>\n","      <td>0.074378</td>\n","      <td>0.724797</td>\n","      <td>0.709135</td>\n","      <td>0.753922</td>\n","      <td>0.905181</td>\n","      <td>0.905181</td>\n","      <td>0.905181</td>\n","      <td>0.686475</td>\n","      <td>0.664890</td>\n","      <td>0.723635</td>\n","      <td>0.656190</td>\n","      <td>0.595185</td>\n","      <td>0.731128</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002600</td>\n","      <td>0.063486</td>\n","      <td>0.727688</td>\n","      <td>0.714199</td>\n","      <td>0.746642</td>\n","      <td>0.912136</td>\n","      <td>0.912136</td>\n","      <td>0.912136</td>\n","      <td>0.689379</td>\n","      <td>0.671835</td>\n","      <td>0.713258</td>\n","      <td>0.670814</td>\n","      <td>0.633161</td>\n","      <td>0.713230</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001800</td>\n","      <td>0.061884</td>\n","      <td>0.738744</td>\n","      <td>0.732668</td>\n","      <td>0.747199</td>\n","      <td>0.919555</td>\n","      <td>0.919555</td>\n","      <td>0.919555</td>\n","      <td>0.701617</td>\n","      <td>0.693590</td>\n","      <td>0.712409</td>\n","      <td>0.688889</td>\n","      <td>0.667518</td>\n","      <td>0.711673</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.064108</td>\n","      <td>0.733792</td>\n","      <td>0.722095</td>\n","      <td>0.748640</td>\n","      <td>0.916947</td>\n","      <td>0.916947</td>\n","      <td>0.916947</td>\n","      <td>0.696023</td>\n","      <td>0.681137</td>\n","      <td>0.714567</td>\n","      <td>0.680835</td>\n","      <td>0.653543</td>\n","      <td>0.710506</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000900</td>\n","      <td>0.063050</td>\n","      <td>0.745353</td>\n","      <td>0.738159</td>\n","      <td>0.757620</td>\n","      <td>0.917179</td>\n","      <td>0.917179</td>\n","      <td>0.917179</td>\n","      <td>0.709587</td>\n","      <td>0.699634</td>\n","      <td>0.725429</td>\n","      <td>0.687696</td>\n","      <td>0.653806</td>\n","      <td>0.725292</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.058472</td>\n","      <td>0.751530</td>\n","      <td>0.755237</td>\n","      <td>0.752514</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.716339</td>\n","      <td>0.719747</td>\n","      <td>0.718395</td>\n","      <td>0.698455</td>\n","      <td>0.677266</td>\n","      <td>0.721012</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.061139</td>\n","      <td>0.744061</td>\n","      <td>0.755427</td>\n","      <td>0.737418</td>\n","      <td>0.920946</td>\n","      <td>0.920946</td>\n","      <td>0.920946</td>\n","      <td>0.707699</td>\n","      <td>0.720487</td>\n","      <td>0.700420</td>\n","      <td>0.689629</td>\n","      <td>0.678464</td>\n","      <td>0.701167</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.065967</td>\n","      <td>0.750953</td>\n","      <td>0.782417</td>\n","      <td>0.726991</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.924481</td>\n","      <td>0.715596</td>\n","      <td>0.752776</td>\n","      <td>0.687165</td>\n","      <td>0.699387</td>\n","      <td>0.711639</td>\n","      <td>0.687549</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.061415</td>\n","      <td>0.742862</td>\n","      <td>0.757746</td>\n","      <td>0.737393</td>\n","      <td>0.918512</td>\n","      <td>0.918512</td>\n","      <td>0.918512</td>\n","      <td>0.706394</td>\n","      <td>0.722653</td>\n","      <td>0.701105</td>\n","      <td>0.682772</td>\n","      <td>0.658123</td>\n","      <td>0.709339</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.060145</td>\n","      <td>0.739486</td>\n","      <td>0.734804</td>\n","      <td>0.751528</td>\n","      <td>0.915672</td>\n","      <td>0.915672</td>\n","      <td>0.915672</td>\n","      <td>0.702633</td>\n","      <td>0.695331</td>\n","      <td>0.718481</td>\n","      <td>0.677148</td>\n","      <td>0.638621</td>\n","      <td>0.720623</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.060788</td>\n","      <td>0.740905</td>\n","      <td>0.738754</td>\n","      <td>0.750007</td>\n","      <td>0.916657</td>\n","      <td>0.916657</td>\n","      <td>0.916657</td>\n","      <td>0.704229</td>\n","      <td>0.700078</td>\n","      <td>0.716457</td>\n","      <td>0.679669</td>\n","      <td>0.644677</td>\n","      <td>0.718677</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.062058</td>\n","      <td>0.742714</td>\n","      <td>0.746534</td>\n","      <td>0.745635</td>\n","      <td>0.917932</td>\n","      <td>0.917932</td>\n","      <td>0.917932</td>\n","      <td>0.706212</td>\n","      <td>0.709354</td>\n","      <td>0.710913</td>\n","      <td>0.680677</td>\n","      <td>0.651942</td>\n","      <td>0.712062</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.061991</td>\n","      <td>0.741780</td>\n","      <td>0.743850</td>\n","      <td>0.746336</td>\n","      <td>0.917526</td>\n","      <td>0.917526</td>\n","      <td>0.917526</td>\n","      <td>0.705178</td>\n","      <td>0.706194</td>\n","      <td>0.711867</td>\n","      <td>0.680638</td>\n","      <td>0.650248</td>\n","      <td>0.714008</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.062047</td>\n","      <td>0.742479</td>\n","      <td>0.746868</td>\n","      <td>0.744197</td>\n","      <td>0.918743</td>\n","      <td>0.918743</td>\n","      <td>0.918743</td>\n","      <td>0.705887</td>\n","      <td>0.709828</td>\n","      <td>0.709054</td>\n","      <td>0.682982</td>\n","      <td>0.656845</td>\n","      <td>0.711284</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.061941</td>\n","      <td>0.741922</td>\n","      <td>0.743702</td>\n","      <td>0.746988</td>\n","      <td>0.917468</td>\n","      <td>0.917468</td>\n","      <td>0.917468</td>\n","      <td>0.705356</td>\n","      <td>0.705989</td>\n","      <td>0.712685</td>\n","      <td>0.681111</td>\n","      <td>0.649823</td>\n","      <td>0.715564</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000600</td>\n","      <td>0.061952</td>\n","      <td>0.741922</td>\n","      <td>0.743702</td>\n","      <td>0.746988</td>\n","      <td>0.917468</td>\n","      <td>0.917468</td>\n","      <td>0.917468</td>\n","      <td>0.705356</td>\n","      <td>0.705989</td>\n","      <td>0.712685</td>\n","      <td>0.681111</td>\n","      <td>0.649823</td>\n","      <td>0.715564</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7419220957525879, 'eval_macro_precision': 0.7437016999265543, 'eval_macro_recall': 0.7469875765478756, 'eval_micro_f1': 0.917468413121595, 'eval_micro_precision': 0.917468413121595, 'eval_micro_recall': 0.917468413121595, 'eval_macro_f1_no_o': 0.7053563656163501, 'eval_macro_precision_no_o': 0.7059885519294733, 'eval_macro_recall_no_o': 0.7126845434736566, 'eval_micro_f1_no_o': 0.6811111111111111, 'eval_micro_precision_no_o': 0.6498233215547703, 'eval_micro_recall_no_o': 0.7155642023346304, 'eval_loss': 0.06195225992499872, 'eval_runtime': 28.5685, 'eval_samples_per_second': 4.235, 'eval_steps_per_second': 1.085, 'epoch': 18.0}\n","Accuracy for fold  27 :  0.6811111111111111  --  0.917468413121595\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #28\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 42:40, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025500</td>\n","      <td>0.098346</td>\n","      <td>0.751766</td>\n","      <td>0.786147</td>\n","      <td>0.725617</td>\n","      <td>0.926626</td>\n","      <td>0.926626</td>\n","      <td>0.926626</td>\n","      <td>0.716337</td>\n","      <td>0.757309</td>\n","      <td>0.684960</td>\n","      <td>0.703072</td>\n","      <td>0.726255</td>\n","      <td>0.681323</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009300</td>\n","      <td>0.078958</td>\n","      <td>0.734894</td>\n","      <td>0.770253</td>\n","      <td>0.707469</td>\n","      <td>0.924018</td>\n","      <td>0.924018</td>\n","      <td>0.924018</td>\n","      <td>0.696657</td>\n","      <td>0.738684</td>\n","      <td>0.663878</td>\n","      <td>0.686012</td>\n","      <td>0.706222</td>\n","      <td>0.666926</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004300</td>\n","      <td>0.078316</td>\n","      <td>0.740141</td>\n","      <td>0.782141</td>\n","      <td>0.706940</td>\n","      <td>0.922279</td>\n","      <td>0.922279</td>\n","      <td>0.922279</td>\n","      <td>0.703114</td>\n","      <td>0.753260</td>\n","      <td>0.663216</td>\n","      <td>0.681957</td>\n","      <td>0.712770</td>\n","      <td>0.653696</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003300</td>\n","      <td>0.065121</td>\n","      <td>0.750404</td>\n","      <td>0.769935</td>\n","      <td>0.735991</td>\n","      <td>0.920946</td>\n","      <td>0.920946</td>\n","      <td>0.920946</td>\n","      <td>0.715203</td>\n","      <td>0.737782</td>\n","      <td>0.698596</td>\n","      <td>0.690614</td>\n","      <td>0.685583</td>\n","      <td>0.695720</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.064822</td>\n","      <td>0.750230</td>\n","      <td>0.761657</td>\n","      <td>0.744835</td>\n","      <td>0.918975</td>\n","      <td>0.918975</td>\n","      <td>0.918975</td>\n","      <td>0.715280</td>\n","      <td>0.727804</td>\n","      <td>0.709787</td>\n","      <td>0.692642</td>\n","      <td>0.673905</td>\n","      <td>0.712451</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.067768</td>\n","      <td>0.747691</td>\n","      <td>0.781632</td>\n","      <td>0.725073</td>\n","      <td>0.924076</td>\n","      <td>0.924076</td>\n","      <td>0.924076</td>\n","      <td>0.711823</td>\n","      <td>0.751612</td>\n","      <td>0.685245</td>\n","      <td>0.700490</td>\n","      <td>0.705325</td>\n","      <td>0.695720</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000900</td>\n","      <td>0.075094</td>\n","      <td>0.743684</td>\n","      <td>0.783434</td>\n","      <td>0.713912</td>\n","      <td>0.924655</td>\n","      <td>0.924655</td>\n","      <td>0.924655</td>\n","      <td>0.707036</td>\n","      <td>0.754350</td>\n","      <td>0.671350</td>\n","      <td>0.693113</td>\n","      <td>0.718280</td>\n","      <td>0.669650</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.070990</td>\n","      <td>0.744663</td>\n","      <td>0.766752</td>\n","      <td>0.730452</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.708319</td>\n","      <td>0.733981</td>\n","      <td>0.691849</td>\n","      <td>0.694961</td>\n","      <td>0.692278</td>\n","      <td>0.697665</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.071298</td>\n","      <td>0.746670</td>\n","      <td>0.769858</td>\n","      <td>0.729346</td>\n","      <td>0.924365</td>\n","      <td>0.924365</td>\n","      <td>0.924365</td>\n","      <td>0.710506</td>\n","      <td>0.737783</td>\n","      <td>0.690071</td>\n","      <td>0.697784</td>\n","      <td>0.703440</td>\n","      <td>0.692218</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000700</td>\n","      <td>0.074900</td>\n","      <td>0.731957</td>\n","      <td>0.760679</td>\n","      <td>0.715329</td>\n","      <td>0.918512</td>\n","      <td>0.918512</td>\n","      <td>0.918512</td>\n","      <td>0.693792</td>\n","      <td>0.727170</td>\n","      <td>0.674523</td>\n","      <td>0.677382</td>\n","      <td>0.674248</td>\n","      <td>0.680545</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.075636</td>\n","      <td>0.734799</td>\n","      <td>0.779664</td>\n","      <td>0.704681</td>\n","      <td>0.922743</td>\n","      <td>0.922743</td>\n","      <td>0.922743</td>\n","      <td>0.696836</td>\n","      <td>0.749883</td>\n","      <td>0.660989</td>\n","      <td>0.688224</td>\n","      <td>0.706557</td>\n","      <td>0.670817</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.067676</td>\n","      <td>0.757061</td>\n","      <td>0.782439</td>\n","      <td>0.739483</td>\n","      <td>0.926800</td>\n","      <td>0.926800</td>\n","      <td>0.926800</td>\n","      <td>0.722451</td>\n","      <td>0.752490</td>\n","      <td>0.701512</td>\n","      <td>0.706185</td>\n","      <td>0.717383</td>\n","      <td>0.695331</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000600</td>\n","      <td>0.066510</td>\n","      <td>0.754794</td>\n","      <td>0.778801</td>\n","      <td>0.741423</td>\n","      <td>0.925351</td>\n","      <td>0.925351</td>\n","      <td>0.925351</td>\n","      <td>0.719875</td>\n","      <td>0.747851</td>\n","      <td>0.704308</td>\n","      <td>0.703070</td>\n","      <td>0.702252</td>\n","      <td>0.703891</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.068638</td>\n","      <td>0.755873</td>\n","      <td>0.783945</td>\n","      <td>0.735742</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.721110</td>\n","      <td>0.754323</td>\n","      <td>0.697159</td>\n","      <td>0.704253</td>\n","      <td>0.716298</td>\n","      <td>0.692607</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.069652</td>\n","      <td>0.752094</td>\n","      <td>0.780572</td>\n","      <td>0.730793</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.925698</td>\n","      <td>0.716711</td>\n","      <td>0.750441</td>\n","      <td>0.691350</td>\n","      <td>0.699822</td>\n","      <td>0.712959</td>\n","      <td>0.687160</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.069297</td>\n","      <td>0.753074</td>\n","      <td>0.781443</td>\n","      <td>0.732334</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.717822</td>\n","      <td>0.751350</td>\n","      <td>0.693194</td>\n","      <td>0.701838</td>\n","      <td>0.712967</td>\n","      <td>0.691051</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.069498</td>\n","      <td>0.753648</td>\n","      <td>0.782654</td>\n","      <td>0.732283</td>\n","      <td>0.926104</td>\n","      <td>0.926104</td>\n","      <td>0.926104</td>\n","      <td>0.718492</td>\n","      <td>0.752794</td>\n","      <td>0.693100</td>\n","      <td>0.701879</td>\n","      <td>0.713883</td>\n","      <td>0.690272</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.069458</td>\n","      <td>0.753758</td>\n","      <td>0.782715</td>\n","      <td>0.732451</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.926162</td>\n","      <td>0.718615</td>\n","      <td>0.752855</td>\n","      <td>0.693296</td>\n","      <td>0.702136</td>\n","      <td>0.713998</td>\n","      <td>0.690661</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.7537582465826519, 'eval_macro_precision': 0.7827153050784841, 'eval_macro_recall': 0.7324508960611414, 'eval_micro_f1': 0.9261620493798539, 'eval_micro_precision': 0.9261620493798539, 'eval_micro_recall': 0.9261620493798539, 'eval_macro_f1_no_o': 0.7186145544641329, 'eval_macro_precision_no_o': 0.7528550173550174, 'eval_macro_recall_no_o': 0.6932961352984269, 'eval_micro_f1_no_o': 0.702136075949367, 'eval_micro_precision_no_o': 0.7139983909895414, 'eval_micro_recall_no_o': 0.6906614785992218, 'eval_loss': 0.06945772881747624, 'eval_runtime': 21.6435, 'eval_samples_per_second': 5.591, 'eval_steps_per_second': 1.432, 'epoch': 18.0}\n","Accuracy for fold  28 :  0.702136075949367  --  0.9261620493798539\n","--------------------------------\n","Testing process has finished.\n","Train run #29\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4608' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4608/6840 20:03 < 09:43, 3.83 it/s, Epoch 12.12/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025700</td>\n","      <td>0.107893</td>\n","      <td>0.735532</td>\n","      <td>0.772327</td>\n","      <td>0.706906</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.697704</td>\n","      <td>0.741746</td>\n","      <td>0.663175</td>\n","      <td>0.685181</td>\n","      <td>0.715193</td>\n","      <td>0.657588</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009300</td>\n","      <td>0.076823</td>\n","      <td>0.725689</td>\n","      <td>0.696547</td>\n","      <td>0.766605</td>\n","      <td>0.916541</td>\n","      <td>0.916541</td>\n","      <td>0.916541</td>\n","      <td>0.686575</td>\n","      <td>0.651049</td>\n","      <td>0.735810</td>\n","      <td>0.681071</td>\n","      <td>0.648155</td>\n","      <td>0.717510</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004200</td>\n","      <td>0.071442</td>\n","      <td>0.740056</td>\n","      <td>0.752706</td>\n","      <td>0.731208</td>\n","      <td>0.921525</td>\n","      <td>0.921525</td>\n","      <td>0.921525</td>\n","      <td>0.702995</td>\n","      <td>0.717671</td>\n","      <td>0.692754</td>\n","      <td>0.688652</td>\n","      <td>0.686654</td>\n","      <td>0.690661</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002700</td>\n","      <td>0.069884</td>\n","      <td>0.737526</td>\n","      <td>0.745433</td>\n","      <td>0.738731</td>\n","      <td>0.915672</td>\n","      <td>0.915672</td>\n","      <td>0.915672</td>\n","      <td>0.700540</td>\n","      <td>0.708505</td>\n","      <td>0.703188</td>\n","      <td>0.678092</td>\n","      <td>0.650465</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.065307</td>\n","      <td>0.740138</td>\n","      <td>0.755662</td>\n","      <td>0.730257</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.702997</td>\n","      <td>0.720795</td>\n","      <td>0.691781</td>\n","      <td>0.691168</td>\n","      <td>0.683670</td>\n","      <td>0.698833</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.073092</td>\n","      <td>0.732341</td>\n","      <td>0.757178</td>\n","      <td>0.715015</td>\n","      <td>0.922163</td>\n","      <td>0.922163</td>\n","      <td>0.922163</td>\n","      <td>0.694029</td>\n","      <td>0.723322</td>\n","      <td>0.673499</td>\n","      <td>0.690279</td>\n","      <td>0.698248</td>\n","      <td>0.682490</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.070567</td>\n","      <td>0.743769</td>\n","      <td>0.770681</td>\n","      <td>0.724608</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.707189</td>\n","      <td>0.738852</td>\n","      <td>0.684566</td>\n","      <td>0.698488</td>\n","      <td>0.705276</td>\n","      <td>0.691829</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.073073</td>\n","      <td>0.740357</td>\n","      <td>0.783997</td>\n","      <td>0.709553</td>\n","      <td>0.925003</td>\n","      <td>0.925003</td>\n","      <td>0.925003</td>\n","      <td>0.703188</td>\n","      <td>0.755020</td>\n","      <td>0.666321</td>\n","      <td>0.696982</td>\n","      <td>0.721667</td>\n","      <td>0.673930</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.067672</td>\n","      <td>0.747635</td>\n","      <td>0.761393</td>\n","      <td>0.738185</td>\n","      <td>0.925177</td>\n","      <td>0.925177</td>\n","      <td>0.925177</td>\n","      <td>0.711559</td>\n","      <td>0.727975</td>\n","      <td>0.700167</td>\n","      <td>0.699389</td>\n","      <td>0.708750</td>\n","      <td>0.690272</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.064503</td>\n","      <td>0.744897</td>\n","      <td>0.756263</td>\n","      <td>0.739647</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.708272</td>\n","      <td>0.721330</td>\n","      <td>0.702350</td>\n","      <td>0.697315</td>\n","      <td>0.692367</td>\n","      <td>0.702335</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.069078</td>\n","      <td>0.756271</td>\n","      <td>0.783879</td>\n","      <td>0.733616</td>\n","      <td>0.928075</td>\n","      <td>0.928075</td>\n","      <td>0.928075</td>\n","      <td>0.721445</td>\n","      <td>0.754404</td>\n","      <td>0.694258</td>\n","      <td>0.708916</td>\n","      <td>0.729030</td>\n","      <td>0.689883</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.067193</td>\n","      <td>0.745324</td>\n","      <td>0.771213</td>\n","      <td>0.729228</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.708702</td>\n","      <td>0.739041</td>\n","      <td>0.689786</td>\n","      <td>0.694428</td>\n","      <td>0.697839</td>\n","      <td>0.691051</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 28:25, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025700</td>\n","      <td>0.107893</td>\n","      <td>0.735532</td>\n","      <td>0.772327</td>\n","      <td>0.706906</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.697704</td>\n","      <td>0.741746</td>\n","      <td>0.663175</td>\n","      <td>0.685181</td>\n","      <td>0.715193</td>\n","      <td>0.657588</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009300</td>\n","      <td>0.076823</td>\n","      <td>0.725689</td>\n","      <td>0.696547</td>\n","      <td>0.766605</td>\n","      <td>0.916541</td>\n","      <td>0.916541</td>\n","      <td>0.916541</td>\n","      <td>0.686575</td>\n","      <td>0.651049</td>\n","      <td>0.735810</td>\n","      <td>0.681071</td>\n","      <td>0.648155</td>\n","      <td>0.717510</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004200</td>\n","      <td>0.071442</td>\n","      <td>0.740056</td>\n","      <td>0.752706</td>\n","      <td>0.731208</td>\n","      <td>0.921525</td>\n","      <td>0.921525</td>\n","      <td>0.921525</td>\n","      <td>0.702995</td>\n","      <td>0.717671</td>\n","      <td>0.692754</td>\n","      <td>0.688652</td>\n","      <td>0.686654</td>\n","      <td>0.690661</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002700</td>\n","      <td>0.069884</td>\n","      <td>0.737526</td>\n","      <td>0.745433</td>\n","      <td>0.738731</td>\n","      <td>0.915672</td>\n","      <td>0.915672</td>\n","      <td>0.915672</td>\n","      <td>0.700540</td>\n","      <td>0.708505</td>\n","      <td>0.703188</td>\n","      <td>0.678092</td>\n","      <td>0.650465</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.065307</td>\n","      <td>0.740138</td>\n","      <td>0.755662</td>\n","      <td>0.730257</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.702997</td>\n","      <td>0.720795</td>\n","      <td>0.691781</td>\n","      <td>0.691168</td>\n","      <td>0.683670</td>\n","      <td>0.698833</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.073092</td>\n","      <td>0.732341</td>\n","      <td>0.757178</td>\n","      <td>0.715015</td>\n","      <td>0.922163</td>\n","      <td>0.922163</td>\n","      <td>0.922163</td>\n","      <td>0.694029</td>\n","      <td>0.723322</td>\n","      <td>0.673499</td>\n","      <td>0.690279</td>\n","      <td>0.698248</td>\n","      <td>0.682490</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.070567</td>\n","      <td>0.743769</td>\n","      <td>0.770681</td>\n","      <td>0.724608</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.707189</td>\n","      <td>0.738852</td>\n","      <td>0.684566</td>\n","      <td>0.698488</td>\n","      <td>0.705276</td>\n","      <td>0.691829</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.073073</td>\n","      <td>0.740357</td>\n","      <td>0.783997</td>\n","      <td>0.709553</td>\n","      <td>0.925003</td>\n","      <td>0.925003</td>\n","      <td>0.925003</td>\n","      <td>0.703188</td>\n","      <td>0.755020</td>\n","      <td>0.666321</td>\n","      <td>0.696982</td>\n","      <td>0.721667</td>\n","      <td>0.673930</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.067672</td>\n","      <td>0.747635</td>\n","      <td>0.761393</td>\n","      <td>0.738185</td>\n","      <td>0.925177</td>\n","      <td>0.925177</td>\n","      <td>0.925177</td>\n","      <td>0.711559</td>\n","      <td>0.727975</td>\n","      <td>0.700167</td>\n","      <td>0.699389</td>\n","      <td>0.708750</td>\n","      <td>0.690272</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.064503</td>\n","      <td>0.744897</td>\n","      <td>0.756263</td>\n","      <td>0.739647</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.708272</td>\n","      <td>0.721330</td>\n","      <td>0.702350</td>\n","      <td>0.697315</td>\n","      <td>0.692367</td>\n","      <td>0.702335</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.069078</td>\n","      <td>0.756271</td>\n","      <td>0.783879</td>\n","      <td>0.733616</td>\n","      <td>0.928075</td>\n","      <td>0.928075</td>\n","      <td>0.928075</td>\n","      <td>0.721445</td>\n","      <td>0.754404</td>\n","      <td>0.694258</td>\n","      <td>0.708916</td>\n","      <td>0.729030</td>\n","      <td>0.689883</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.067193</td>\n","      <td>0.745324</td>\n","      <td>0.771213</td>\n","      <td>0.729228</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.924945</td>\n","      <td>0.708702</td>\n","      <td>0.739041</td>\n","      <td>0.689786</td>\n","      <td>0.694428</td>\n","      <td>0.697839</td>\n","      <td>0.691051</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.069459</td>\n","      <td>0.739909</td>\n","      <td>0.757133</td>\n","      <td>0.728923</td>\n","      <td>0.920888</td>\n","      <td>0.920888</td>\n","      <td>0.920888</td>\n","      <td>0.702702</td>\n","      <td>0.722468</td>\n","      <td>0.690214</td>\n","      <td>0.682692</td>\n","      <td>0.674905</td>\n","      <td>0.690661</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.069502</td>\n","      <td>0.738325</td>\n","      <td>0.758359</td>\n","      <td>0.724986</td>\n","      <td>0.920424</td>\n","      <td>0.920424</td>\n","      <td>0.920424</td>\n","      <td>0.700945</td>\n","      <td>0.724127</td>\n","      <td>0.685574</td>\n","      <td>0.681353</td>\n","      <td>0.676775</td>\n","      <td>0.685992</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.068484</td>\n","      <td>0.739225</td>\n","      <td>0.752651</td>\n","      <td>0.731016</td>\n","      <td>0.920250</td>\n","      <td>0.920250</td>\n","      <td>0.920250</td>\n","      <td>0.702037</td>\n","      <td>0.717322</td>\n","      <td>0.692836</td>\n","      <td>0.683433</td>\n","      <td>0.674498</td>\n","      <td>0.692607</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000500</td>\n","      <td>0.068641</td>\n","      <td>0.740848</td>\n","      <td>0.755817</td>\n","      <td>0.731221</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.703894</td>\n","      <td>0.721034</td>\n","      <td>0.692985</td>\n","      <td>0.684747</td>\n","      <td>0.677064</td>\n","      <td>0.692607</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.068878</td>\n","      <td>0.741115</td>\n","      <td>0.756176</td>\n","      <td>0.731353</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.704199</td>\n","      <td>0.721464</td>\n","      <td>0.693116</td>\n","      <td>0.684373</td>\n","      <td>0.677075</td>\n","      <td>0.691829</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.068891</td>\n","      <td>0.741115</td>\n","      <td>0.756176</td>\n","      <td>0.731353</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.704199</td>\n","      <td>0.721464</td>\n","      <td>0.693116</td>\n","      <td>0.684373</td>\n","      <td>0.677075</td>\n","      <td>0.691829</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7411150581085081, 'eval_macro_precision': 0.756176194191742, 'eval_macro_recall': 0.7313529396623375, 'eval_micro_f1': 0.9207140373246784, 'eval_micro_precision': 0.9207140373246784, 'eval_micro_recall': 0.9207140373246784, 'eval_macro_f1_no_o': 0.7041993727423134, 'eval_macro_precision_no_o': 0.7214636038243737, 'eval_macro_recall_no_o': 0.6931161577455319, 'eval_micro_f1_no_o': 0.6843725943033102, 'eval_micro_precision_no_o': 0.6770753998476771, 'eval_micro_recall_no_o': 0.691828793774319, 'eval_loss': 0.06889064301884157, 'eval_runtime': 10.4511, 'eval_samples_per_second': 11.578, 'eval_steps_per_second': 2.966, 'epoch': 18.0}\n","Accuracy for fold  29 :  0.6843725943033102  --  0.9207140373246784\n","--------------------------------\n","Testing process has finished.\n","Train run #30\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 25:25, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025600</td>\n","      <td>0.102209</td>\n","      <td>0.739297</td>\n","      <td>0.757622</td>\n","      <td>0.726496</td>\n","      <td>0.924307</td>\n","      <td>0.924307</td>\n","      <td>0.924307</td>\n","      <td>0.701950</td>\n","      <td>0.723700</td>\n","      <td>0.686643</td>\n","      <td>0.697555</td>\n","      <td>0.707034</td>\n","      <td>0.688327</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009200</td>\n","      <td>0.080006</td>\n","      <td>0.743238</td>\n","      <td>0.782185</td>\n","      <td>0.713922</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.706402</td>\n","      <td>0.752637</td>\n","      <td>0.671396</td>\n","      <td>0.700180</td>\n","      <td>0.721420</td>\n","      <td>0.680156</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004300</td>\n","      <td>0.071776</td>\n","      <td>0.741647</td>\n","      <td>0.734998</td>\n","      <td>0.754989</td>\n","      <td>0.917990</td>\n","      <td>0.917990</td>\n","      <td>0.917990</td>\n","      <td>0.705239</td>\n","      <td>0.696237</td>\n","      <td>0.722031</td>\n","      <td>0.689282</td>\n","      <td>0.661538</td>\n","      <td>0.719455</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002600</td>\n","      <td>0.063684</td>\n","      <td>0.760103</td>\n","      <td>0.770684</td>\n","      <td>0.756725</td>\n","      <td>0.928480</td>\n","      <td>0.928480</td>\n","      <td>0.928480</td>\n","      <td>0.725826</td>\n","      <td>0.738248</td>\n","      <td>0.721809</td>\n","      <td>0.714787</td>\n","      <td>0.716745</td>\n","      <td>0.712840</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.070760</td>\n","      <td>0.740759</td>\n","      <td>0.773560</td>\n","      <td>0.713062</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.703401</td>\n","      <td>0.742813</td>\n","      <td>0.669926</td>\n","      <td>0.697646</td>\n","      <td>0.729008</td>\n","      <td>0.668872</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.062321</td>\n","      <td>0.735955</td>\n","      <td>0.723622</td>\n","      <td>0.753130</td>\n","      <td>0.918454</td>\n","      <td>0.918454</td>\n","      <td>0.918454</td>\n","      <td>0.698349</td>\n","      <td>0.682647</td>\n","      <td>0.719681</td>\n","      <td>0.684768</td>\n","      <td>0.655860</td>\n","      <td>0.716342</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.069312</td>\n","      <td>0.741166</td>\n","      <td>0.750057</td>\n","      <td>0.736371</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.924539</td>\n","      <td>0.704035</td>\n","      <td>0.714918</td>\n","      <td>0.697927</td>\n","      <td>0.694411</td>\n","      <td>0.707593</td>\n","      <td>0.681712</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.059997</td>\n","      <td>0.759447</td>\n","      <td>0.776956</td>\n","      <td>0.746626</td>\n","      <td>0.929755</td>\n","      <td>0.929755</td>\n","      <td>0.929755</td>\n","      <td>0.724931</td>\n","      <td>0.746070</td>\n","      <td>0.709255</td>\n","      <td>0.713116</td>\n","      <td>0.732267</td>\n","      <td>0.694942</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.056071</td>\n","      <td>0.759921</td>\n","      <td>0.755154</td>\n","      <td>0.768092</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.725690</td>\n","      <td>0.719214</td>\n","      <td>0.736126</td>\n","      <td>0.711781</td>\n","      <td>0.690311</td>\n","      <td>0.734630</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.057359</td>\n","      <td>0.761112</td>\n","      <td>0.763122</td>\n","      <td>0.761374</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.927843</td>\n","      <td>0.726985</td>\n","      <td>0.728956</td>\n","      <td>0.727664</td>\n","      <td>0.713902</td>\n","      <td>0.704701</td>\n","      <td>0.723346</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.057824</td>\n","      <td>0.760233</td>\n","      <td>0.761861</td>\n","      <td>0.759310</td>\n","      <td>0.927205</td>\n","      <td>0.927205</td>\n","      <td>0.927205</td>\n","      <td>0.726037</td>\n","      <td>0.727778</td>\n","      <td>0.725120</td>\n","      <td>0.710389</td>\n","      <td>0.706426</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.058820</td>\n","      <td>0.756852</td>\n","      <td>0.762360</td>\n","      <td>0.752590</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.927379</td>\n","      <td>0.722077</td>\n","      <td>0.728508</td>\n","      <td>0.717098</td>\n","      <td>0.709477</td>\n","      <td>0.709615</td>\n","      <td>0.709339</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.056877</td>\n","      <td>0.761126</td>\n","      <td>0.756013</td>\n","      <td>0.767770</td>\n","      <td>0.928017</td>\n","      <td>0.928017</td>\n","      <td>0.928017</td>\n","      <td>0.727010</td>\n","      <td>0.720416</td>\n","      <td>0.735387</td>\n","      <td>0.717549</td>\n","      <td>0.702310</td>\n","      <td>0.733463</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.057313</td>\n","      <td>0.759796</td>\n","      <td>0.751385</td>\n","      <td>0.771516</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.725523</td>\n","      <td>0.714739</td>\n","      <td>0.740155</td>\n","      <td>0.714958</td>\n","      <td>0.692168</td>\n","      <td>0.739300</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.057312</td>\n","      <td>0.762700</td>\n","      <td>0.757280</td>\n","      <td>0.771050</td>\n","      <td>0.927727</td>\n","      <td>0.927727</td>\n","      <td>0.927727</td>\n","      <td>0.728861</td>\n","      <td>0.721701</td>\n","      <td>0.739429</td>\n","      <td>0.717823</td>\n","      <td>0.697905</td>\n","      <td>0.738911</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000500</td>\n","      <td>0.057076</td>\n","      <td>0.760879</td>\n","      <td>0.752589</td>\n","      <td>0.772611</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.926046</td>\n","      <td>0.726887</td>\n","      <td>0.716138</td>\n","      <td>0.741636</td>\n","      <td>0.713911</td>\n","      <td>0.688857</td>\n","      <td>0.740856</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.057083</td>\n","      <td>0.761445</td>\n","      <td>0.753167</td>\n","      <td>0.773044</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.727524</td>\n","      <td>0.716812</td>\n","      <td>0.742097</td>\n","      <td>0.714822</td>\n","      <td>0.690217</td>\n","      <td>0.741245</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000400</td>\n","      <td>0.057094</td>\n","      <td>0.761544</td>\n","      <td>0.753345</td>\n","      <td>0.773044</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.727645</td>\n","      <td>0.717030</td>\n","      <td>0.742097</td>\n","      <td>0.714956</td>\n","      <td>0.690468</td>\n","      <td>0.741245</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.7615442419323559, 'eval_macro_precision': 0.7533447013285567, 'eval_macro_recall': 0.773044475312547, 'eval_micro_f1': 0.9263359221050191, 'eval_micro_precision': 0.9263359221050191, 'eval_micro_recall': 0.9263359221050191, 'eval_macro_f1_no_o': 0.7276448590162419, 'eval_macro_precision_no_o': 0.7170302416960102, 'eval_macro_recall_no_o': 0.7420967893447074, 'eval_micro_f1_no_o': 0.714955901670107, 'eval_micro_precision_no_o': 0.6904675607104023, 'eval_micro_recall_no_o': 0.7412451361867705, 'eval_loss': 0.057093858099686576, 'eval_runtime': 10.3845, 'eval_samples_per_second': 11.652, 'eval_steps_per_second': 2.985, 'epoch': 18.0}\n","Accuracy for fold  30 :  0.714955901670107  --  0.9263359221050191\n","--------------------------------\n","Testing process has finished.\n","Train run #31\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 25:27, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025400</td>\n","      <td>0.104648</td>\n","      <td>0.735069</td>\n","      <td>0.787353</td>\n","      <td>0.695766</td>\n","      <td>0.925003</td>\n","      <td>0.925003</td>\n","      <td>0.925003</td>\n","      <td>0.697067</td>\n","      <td>0.759625</td>\n","      <td>0.649623</td>\n","      <td>0.692101</td>\n","      <td>0.736288</td>\n","      <td>0.652918</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009400</td>\n","      <td>0.077763</td>\n","      <td>0.735743</td>\n","      <td>0.733937</td>\n","      <td>0.744190</td>\n","      <td>0.918396</td>\n","      <td>0.918396</td>\n","      <td>0.918396</td>\n","      <td>0.698184</td>\n","      <td>0.694718</td>\n","      <td>0.709375</td>\n","      <td>0.687465</td>\n","      <td>0.657549</td>\n","      <td>0.720233</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004800</td>\n","      <td>0.073883</td>\n","      <td>0.736566</td>\n","      <td>0.732416</td>\n","      <td>0.745034</td>\n","      <td>0.918222</td>\n","      <td>0.918222</td>\n","      <td>0.918222</td>\n","      <td>0.699275</td>\n","      <td>0.693384</td>\n","      <td>0.710190</td>\n","      <td>0.687805</td>\n","      <td>0.664130</td>\n","      <td>0.713230</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002600</td>\n","      <td>0.065809</td>\n","      <td>0.741830</td>\n","      <td>0.737431</td>\n","      <td>0.748644</td>\n","      <td>0.923554</td>\n","      <td>0.923554</td>\n","      <td>0.923554</td>\n","      <td>0.704811</td>\n","      <td>0.699228</td>\n","      <td>0.713209</td>\n","      <td>0.697051</td>\n","      <td>0.686275</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.063954</td>\n","      <td>0.744623</td>\n","      <td>0.739878</td>\n","      <td>0.755073</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.708050</td>\n","      <td>0.701512</td>\n","      <td>0.721232</td>\n","      <td>0.697989</td>\n","      <td>0.675027</td>\n","      <td>0.722568</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.068290</td>\n","      <td>0.738006</td>\n","      <td>0.750947</td>\n","      <td>0.731568</td>\n","      <td>0.921699</td>\n","      <td>0.921699</td>\n","      <td>0.921699</td>\n","      <td>0.700444</td>\n","      <td>0.715024</td>\n","      <td>0.693447</td>\n","      <td>0.688575</td>\n","      <td>0.676426</td>\n","      <td>0.701167</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.068092</td>\n","      <td>0.739848</td>\n","      <td>0.744249</td>\n","      <td>0.738817</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.702440</td>\n","      <td>0.707168</td>\n","      <td>0.701642</td>\n","      <td>0.693517</td>\n","      <td>0.683812</td>\n","      <td>0.703502</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.065834</td>\n","      <td>0.727864</td>\n","      <td>0.720878</td>\n","      <td>0.742928</td>\n","      <td>0.916541</td>\n","      <td>0.916541</td>\n","      <td>0.916541</td>\n","      <td>0.688914</td>\n","      <td>0.679206</td>\n","      <td>0.708017</td>\n","      <td>0.674908</td>\n","      <td>0.641754</td>\n","      <td>0.711673</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.059892</td>\n","      <td>0.743877</td>\n","      <td>0.735702</td>\n","      <td>0.757627</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.707169</td>\n","      <td>0.696324</td>\n","      <td>0.724496</td>\n","      <td>0.696930</td>\n","      <td>0.667736</td>\n","      <td>0.728794</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000400</td>\n","      <td>0.062398</td>\n","      <td>0.739426</td>\n","      <td>0.730772</td>\n","      <td>0.755987</td>\n","      <td>0.920308</td>\n","      <td>0.920308</td>\n","      <td>0.920308</td>\n","      <td>0.702119</td>\n","      <td>0.690526</td>\n","      <td>0.722911</td>\n","      <td>0.689222</td>\n","      <td>0.656569</td>\n","      <td>0.725292</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.061987</td>\n","      <td>0.751401</td>\n","      <td>0.752724</td>\n","      <td>0.755587</td>\n","      <td>0.925525</td>\n","      <td>0.925525</td>\n","      <td>0.925525</td>\n","      <td>0.715734</td>\n","      <td>0.716486</td>\n","      <td>0.721401</td>\n","      <td>0.704903</td>\n","      <td>0.686325</td>\n","      <td>0.724514</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.058083</td>\n","      <td>0.752099</td>\n","      <td>0.731864</td>\n","      <td>0.775580</td>\n","      <td>0.922395</td>\n","      <td>0.922395</td>\n","      <td>0.922395</td>\n","      <td>0.716854</td>\n","      <td>0.691577</td>\n","      <td>0.745884</td>\n","      <td>0.702335</td>\n","      <td>0.665737</td>\n","      <td>0.743191</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.059087</td>\n","      <td>0.749029</td>\n","      <td>0.728880</td>\n","      <td>0.772096</td>\n","      <td>0.921178</td>\n","      <td>0.921178</td>\n","      <td>0.921178</td>\n","      <td>0.713421</td>\n","      <td>0.688400</td>\n","      <td>0.741819</td>\n","      <td>0.698078</td>\n","      <td>0.664673</td>\n","      <td>0.735019</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.059451</td>\n","      <td>0.750067</td>\n","      <td>0.733443</td>\n","      <td>0.769184</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.921873</td>\n","      <td>0.714574</td>\n","      <td>0.693829</td>\n","      <td>0.738205</td>\n","      <td>0.699238</td>\n","      <td>0.669037</td>\n","      <td>0.732296</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.058156</td>\n","      <td>0.749076</td>\n","      <td>0.731515</td>\n","      <td>0.769938</td>\n","      <td>0.920946</td>\n","      <td>0.920946</td>\n","      <td>0.920946</td>\n","      <td>0.713465</td>\n","      <td>0.691451</td>\n","      <td>0.739301</td>\n","      <td>0.696343</td>\n","      <td>0.662799</td>\n","      <td>0.733463</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.058652</td>\n","      <td>0.751043</td>\n","      <td>0.737189</td>\n","      <td>0.767827</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.715703</td>\n","      <td>0.698155</td>\n","      <td>0.736646</td>\n","      <td>0.698460</td>\n","      <td>0.667613</td>\n","      <td>0.732296</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.058539</td>\n","      <td>0.750816</td>\n","      <td>0.734854</td>\n","      <td>0.769782</td>\n","      <td>0.921525</td>\n","      <td>0.921525</td>\n","      <td>0.921525</td>\n","      <td>0.715452</td>\n","      <td>0.695377</td>\n","      <td>0.739006</td>\n","      <td>0.698019</td>\n","      <td>0.665842</td>\n","      <td>0.733463</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.058571</td>\n","      <td>0.750951</td>\n","      <td>0.735118</td>\n","      <td>0.769792</td>\n","      <td>0.921583</td>\n","      <td>0.921583</td>\n","      <td>0.921583</td>\n","      <td>0.715604</td>\n","      <td>0.695685</td>\n","      <td>0.739006</td>\n","      <td>0.698148</td>\n","      <td>0.666078</td>\n","      <td>0.733463</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7509509874739957, 'eval_macro_precision': 0.7351176922355339, 'eval_macro_recall': 0.7697919455909547, 'eval_micro_f1': 0.9215834009505043, 'eval_micro_precision': 0.9215834009505043, 'eval_micro_recall': 0.9215834009505043, 'eval_macro_f1_no_o': 0.7156037824867818, 'eval_macro_precision_no_o': 0.6956850058887664, 'eval_macro_recall_no_o': 0.739005885128973, 'eval_micro_f1_no_o': 0.6981481481481481, 'eval_micro_precision_no_o': 0.666077738515901, 'eval_micro_recall_no_o': 0.7334630350194552, 'eval_loss': 0.05857050968317843, 'eval_runtime': 10.4626, 'eval_samples_per_second': 11.565, 'eval_steps_per_second': 2.963, 'epoch': 18.0}\n","Accuracy for fold  31 :  0.6981481481481481  --  0.9215834009505043\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"stream","name":"stdout","text":["Testing process has finished.\n","Train run #32\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 25:30, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025300</td>\n","      <td>0.100535</td>\n","      <td>0.751144</td>\n","      <td>0.776881</td>\n","      <td>0.729592</td>\n","      <td>0.927901</td>\n","      <td>0.927901</td>\n","      <td>0.927901</td>\n","      <td>0.715393</td>\n","      <td>0.746243</td>\n","      <td>0.689416</td>\n","      <td>0.704491</td>\n","      <td>0.726634</td>\n","      <td>0.683658</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009100</td>\n","      <td>0.076811</td>\n","      <td>0.731991</td>\n","      <td>0.728862</td>\n","      <td>0.740705</td>\n","      <td>0.918222</td>\n","      <td>0.918222</td>\n","      <td>0.918222</td>\n","      <td>0.693724</td>\n","      <td>0.688792</td>\n","      <td>0.705151</td>\n","      <td>0.682927</td>\n","      <td>0.654766</td>\n","      <td>0.713619</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004100</td>\n","      <td>0.069002</td>\n","      <td>0.747472</td>\n","      <td>0.773635</td>\n","      <td>0.726459</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.711523</td>\n","      <td>0.743109</td>\n","      <td>0.685931</td>\n","      <td>0.698766</td>\n","      <td>0.727771</td>\n","      <td>0.671984</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002600</td>\n","      <td>0.064385</td>\n","      <td>0.724597</td>\n","      <td>0.697789</td>\n","      <td>0.758067</td>\n","      <td>0.912252</td>\n","      <td>0.912252</td>\n","      <td>0.912252</td>\n","      <td>0.685700</td>\n","      <td>0.652555</td>\n","      <td>0.726576</td>\n","      <td>0.669832</td>\n","      <td>0.631108</td>\n","      <td>0.713619</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001800</td>\n","      <td>0.061043</td>\n","      <td>0.733573</td>\n","      <td>0.731752</td>\n","      <td>0.740933</td>\n","      <td>0.917352</td>\n","      <td>0.917352</td>\n","      <td>0.917352</td>\n","      <td>0.695734</td>\n","      <td>0.692789</td>\n","      <td>0.705133</td>\n","      <td>0.678389</td>\n","      <td>0.659801</td>\n","      <td>0.698054</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001400</td>\n","      <td>0.065531</td>\n","      <td>0.743845</td>\n","      <td>0.751379</td>\n","      <td>0.738535</td>\n","      <td>0.921989</td>\n","      <td>0.921989</td>\n","      <td>0.921989</td>\n","      <td>0.707431</td>\n","      <td>0.716303</td>\n","      <td>0.701155</td>\n","      <td>0.690732</td>\n","      <td>0.692759</td>\n","      <td>0.688716</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.063724</td>\n","      <td>0.735708</td>\n","      <td>0.737534</td>\n","      <td>0.736235</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.920714</td>\n","      <td>0.698017</td>\n","      <td>0.700124</td>\n","      <td>0.698653</td>\n","      <td>0.685848</td>\n","      <td>0.685315</td>\n","      <td>0.686381</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.060848</td>\n","      <td>0.737973</td>\n","      <td>0.729014</td>\n","      <td>0.752066</td>\n","      <td>0.919033</td>\n","      <td>0.919033</td>\n","      <td>0.919033</td>\n","      <td>0.700754</td>\n","      <td>0.689319</td>\n","      <td>0.718167</td>\n","      <td>0.687100</td>\n","      <td>0.664847</td>\n","      <td>0.710895</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.066793</td>\n","      <td>0.745475</td>\n","      <td>0.754901</td>\n","      <td>0.738579</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.709348</td>\n","      <td>0.720487</td>\n","      <td>0.701161</td>\n","      <td>0.694173</td>\n","      <td>0.697720</td>\n","      <td>0.690661</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.060044</td>\n","      <td>0.743464</td>\n","      <td>0.732276</td>\n","      <td>0.758132</td>\n","      <td>0.919555</td>\n","      <td>0.919555</td>\n","      <td>0.919555</td>\n","      <td>0.707137</td>\n","      <td>0.692972</td>\n","      <td>0.725346</td>\n","      <td>0.690882</td>\n","      <td>0.665825</td>\n","      <td>0.717899</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.071730</td>\n","      <td>0.746467</td>\n","      <td>0.771348</td>\n","      <td>0.729936</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.710293</td>\n","      <td>0.739826</td>\n","      <td>0.690498</td>\n","      <td>0.700614</td>\n","      <td>0.713767</td>\n","      <td>0.687938</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.062060</td>\n","      <td>0.732609</td>\n","      <td>0.723407</td>\n","      <td>0.752572</td>\n","      <td>0.912484</td>\n","      <td>0.912484</td>\n","      <td>0.912484</td>\n","      <td>0.695011</td>\n","      <td>0.682189</td>\n","      <td>0.720335</td>\n","      <td>0.672107</td>\n","      <td>0.629416</td>\n","      <td>0.721012</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.064161</td>\n","      <td>0.740733</td>\n","      <td>0.735938</td>\n","      <td>0.750322</td>\n","      <td>0.917526</td>\n","      <td>0.917526</td>\n","      <td>0.917526</td>\n","      <td>0.704178</td>\n","      <td>0.697345</td>\n","      <td>0.716586</td>\n","      <td>0.686428</td>\n","      <td>0.658912</td>\n","      <td>0.716342</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.065763</td>\n","      <td>0.740447</td>\n","      <td>0.737875</td>\n","      <td>0.747393</td>\n","      <td>0.917816</td>\n","      <td>0.917816</td>\n","      <td>0.917816</td>\n","      <td>0.703825</td>\n","      <td>0.699787</td>\n","      <td>0.712953</td>\n","      <td>0.685811</td>\n","      <td>0.662437</td>\n","      <td>0.710895</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.059661</td>\n","      <td>0.741477</td>\n","      <td>0.723573</td>\n","      <td>0.764543</td>\n","      <td>0.916251</td>\n","      <td>0.916251</td>\n","      <td>0.916251</td>\n","      <td>0.705186</td>\n","      <td>0.682470</td>\n","      <td>0.733881</td>\n","      <td>0.688003</td>\n","      <td>0.649068</td>\n","      <td>0.731907</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.058338</td>\n","      <td>0.741625</td>\n","      <td>0.720562</td>\n","      <td>0.769920</td>\n","      <td>0.914744</td>\n","      <td>0.914744</td>\n","      <td>0.914744</td>\n","      <td>0.705463</td>\n","      <td>0.678612</td>\n","      <td>0.740687</td>\n","      <td>0.685776</td>\n","      <td>0.638898</td>\n","      <td>0.740078</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.058663</td>\n","      <td>0.742500</td>\n","      <td>0.722124</td>\n","      <td>0.769542</td>\n","      <td>0.915324</td>\n","      <td>0.915324</td>\n","      <td>0.915324</td>\n","      <td>0.706450</td>\n","      <td>0.680520</td>\n","      <td>0.740099</td>\n","      <td>0.687296</td>\n","      <td>0.642422</td>\n","      <td>0.738911</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000600</td>\n","      <td>0.058702</td>\n","      <td>0.742600</td>\n","      <td>0.722284</td>\n","      <td>0.769552</td>\n","      <td>0.915382</td>\n","      <td>0.915382</td>\n","      <td>0.915382</td>\n","      <td>0.706560</td>\n","      <td>0.680707</td>\n","      <td>0.740099</td>\n","      <td>0.687421</td>\n","      <td>0.642640</td>\n","      <td>0.738911</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7425996339330829, 'eval_macro_precision': 0.7222844193281244, 'eval_macro_recall': 0.7695521159035772, 'eval_micro_f1': 0.9153819404196128, 'eval_micro_precision': 0.9153819404196128, 'eval_micro_recall': 0.9153819404196128, 'eval_macro_f1_no_o': 0.706559916341636, 'eval_macro_precision_no_o': 0.6807074432222527, 'eval_macro_recall_no_o': 0.7400994607452204, 'eval_micro_f1_no_o': 0.6874208144796381, 'eval_micro_precision_no_o': 0.6426395939086295, 'eval_micro_recall_no_o': 0.7389105058365759, 'eval_loss': 0.058702428970545874, 'eval_runtime': 10.3914, 'eval_samples_per_second': 11.644, 'eval_steps_per_second': 2.983, 'epoch': 18.0}\n","Accuracy for fold  32 :  0.6874208144796381  --  0.9153819404196128\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"stream","name":"stdout","text":["Testing process has finished.\n","Train run #33\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 25:29, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025100</td>\n","      <td>0.102447</td>\n","      <td>0.748835</td>\n","      <td>0.781906</td>\n","      <td>0.724240</td>\n","      <td>0.925409</td>\n","      <td>0.925409</td>\n","      <td>0.925409</td>\n","      <td>0.713059</td>\n","      <td>0.752342</td>\n","      <td>0.683660</td>\n","      <td>0.701257</td>\n","      <td>0.719787</td>\n","      <td>0.683658</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.010200</td>\n","      <td>0.086698</td>\n","      <td>0.691729</td>\n","      <td>0.685869</td>\n","      <td>0.706044</td>\n","      <td>0.908543</td>\n","      <td>0.908543</td>\n","      <td>0.908543</td>\n","      <td>0.647482</td>\n","      <td>0.639382</td>\n","      <td>0.665428</td>\n","      <td>0.644440</td>\n","      <td>0.618078</td>\n","      <td>0.673152</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004100</td>\n","      <td>0.068104</td>\n","      <td>0.731163</td>\n","      <td>0.748110</td>\n","      <td>0.724615</td>\n","      <td>0.920540</td>\n","      <td>0.920540</td>\n","      <td>0.920540</td>\n","      <td>0.692687</td>\n","      <td>0.711953</td>\n","      <td>0.685550</td>\n","      <td>0.688456</td>\n","      <td>0.676559</td>\n","      <td>0.700778</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002600</td>\n","      <td>0.069569</td>\n","      <td>0.726748</td>\n","      <td>0.749720</td>\n","      <td>0.713594</td>\n","      <td>0.917179</td>\n","      <td>0.917179</td>\n","      <td>0.917179</td>\n","      <td>0.687865</td>\n","      <td>0.714215</td>\n","      <td>0.672965</td>\n","      <td>0.676752</td>\n","      <td>0.666290</td>\n","      <td>0.687549</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001800</td>\n","      <td>0.065525</td>\n","      <td>0.734896</td>\n","      <td>0.753916</td>\n","      <td>0.723406</td>\n","      <td>0.923264</td>\n","      <td>0.923264</td>\n","      <td>0.923264</td>\n","      <td>0.696818</td>\n","      <td>0.718849</td>\n","      <td>0.683572</td>\n","      <td>0.695686</td>\n","      <td>0.691805</td>\n","      <td>0.699611</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.061647</td>\n","      <td>0.737539</td>\n","      <td>0.733001</td>\n","      <td>0.746882</td>\n","      <td>0.920077</td>\n","      <td>0.920077</td>\n","      <td>0.920077</td>\n","      <td>0.700185</td>\n","      <td>0.693739</td>\n","      <td>0.712221</td>\n","      <td>0.693343</td>\n","      <td>0.667387</td>\n","      <td>0.721401</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.061260</td>\n","      <td>0.738134</td>\n","      <td>0.741727</td>\n","      <td>0.737546</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.922047</td>\n","      <td>0.700677</td>\n","      <td>0.704380</td>\n","      <td>0.700477</td>\n","      <td>0.693440</td>\n","      <td>0.681835</td>\n","      <td>0.705447</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.061439</td>\n","      <td>0.739907</td>\n","      <td>0.758072</td>\n","      <td>0.727612</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.925293</td>\n","      <td>0.702532</td>\n","      <td>0.723932</td>\n","      <td>0.687980</td>\n","      <td>0.701294</td>\n","      <td>0.706556</td>\n","      <td>0.696109</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.065069</td>\n","      <td>0.739887</td>\n","      <td>0.762428</td>\n","      <td>0.723188</td>\n","      <td>0.925119</td>\n","      <td>0.925119</td>\n","      <td>0.925119</td>\n","      <td>0.702519</td>\n","      <td>0.729203</td>\n","      <td>0.682649</td>\n","      <td>0.698757</td>\n","      <td>0.708683</td>\n","      <td>0.689105</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000500</td>\n","      <td>0.063695</td>\n","      <td>0.724287</td>\n","      <td>0.702951</td>\n","      <td>0.755325</td>\n","      <td>0.910919</td>\n","      <td>0.910919</td>\n","      <td>0.910919</td>\n","      <td>0.685484</td>\n","      <td>0.657857</td>\n","      <td>0.724341</td>\n","      <td>0.673894</td>\n","      <td>0.620216</td>\n","      <td>0.737743</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000400</td>\n","      <td>0.066658</td>\n","      <td>0.739900</td>\n","      <td>0.762129</td>\n","      <td>0.725450</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.702579</td>\n","      <td>0.728627</td>\n","      <td>0.685605</td>\n","      <td>0.705997</td>\n","      <td>0.708905</td>\n","      <td>0.703113</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000600</td>\n","      <td>0.059716</td>\n","      <td>0.727400</td>\n","      <td>0.715312</td>\n","      <td>0.752380</td>\n","      <td>0.909528</td>\n","      <td>0.909528</td>\n","      <td>0.909528</td>\n","      <td>0.689342</td>\n","      <td>0.672256</td>\n","      <td>0.721360</td>\n","      <td>0.674427</td>\n","      <td>0.616774</td>\n","      <td>0.743969</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.063319</td>\n","      <td>0.736570</td>\n","      <td>0.760838</td>\n","      <td>0.718448</td>\n","      <td>0.923554</td>\n","      <td>0.923554</td>\n","      <td>0.923554</td>\n","      <td>0.698899</td>\n","      <td>0.727576</td>\n","      <td>0.677390</td>\n","      <td>0.697023</td>\n","      <td>0.706352</td>\n","      <td>0.687938</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.062908</td>\n","      <td>0.736511</td>\n","      <td>0.759248</td>\n","      <td>0.721096</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.923032</td>\n","      <td>0.698803</td>\n","      <td>0.725373</td>\n","      <td>0.680775</td>\n","      <td>0.695635</td>\n","      <td>0.696721</td>\n","      <td>0.694553</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.063163</td>\n","      <td>0.736538</td>\n","      <td>0.758943</td>\n","      <td>0.721118</td>\n","      <td>0.923148</td>\n","      <td>0.923148</td>\n","      <td>0.923148</td>\n","      <td>0.698817</td>\n","      <td>0.725005</td>\n","      <td>0.680778</td>\n","      <td>0.695771</td>\n","      <td>0.696993</td>\n","      <td>0.694553</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000300</td>\n","      <td>0.063212</td>\n","      <td>0.737066</td>\n","      <td>0.760022</td>\n","      <td>0.721646</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.699393</td>\n","      <td>0.726220</td>\n","      <td>0.681360</td>\n","      <td>0.696415</td>\n","      <td>0.697502</td>\n","      <td>0.695331</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.063309</td>\n","      <td>0.736429</td>\n","      <td>0.759897</td>\n","      <td>0.720439</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.698666</td>\n","      <td>0.726148</td>\n","      <td>0.679907</td>\n","      <td>0.695958</td>\n","      <td>0.698550</td>\n","      <td>0.693385</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000900</td>\n","      <td>0.063334</td>\n","      <td>0.737109</td>\n","      <td>0.761552</td>\n","      <td>0.720446</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.699453</td>\n","      <td>0.728079</td>\n","      <td>0.679903</td>\n","      <td>0.696094</td>\n","      <td>0.698824</td>\n","      <td>0.693385</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.7371091673970246, 'eval_macro_precision': 0.7615524881152851, 'eval_macro_recall': 0.7204458016525914, 'eval_micro_f1': 0.9234380433522661, 'eval_micro_precision': 0.9234380433522661, 'eval_micro_recall': 0.9234380433522661, 'eval_macro_f1_no_o': 0.699453005082836, 'eval_macro_precision_no_o': 0.7280793717891965, 'eval_macro_recall_no_o': 0.6799031038348607, 'eval_micro_f1_no_o': 0.69609375, 'eval_micro_precision_no_o': 0.6988235294117647, 'eval_micro_recall_no_o': 0.6933852140077821, 'eval_loss': 0.0633341749829443, 'eval_runtime': 10.4332, 'eval_samples_per_second': 11.598, 'eval_steps_per_second': 2.971, 'epoch': 18.0}\n","Accuracy for fold  33 :  0.69609375  --  0.9234380433522661\n","--------------------------------\n","Testing process has finished.\n","Train run #34\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 25:30, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.025200</td>\n","      <td>0.101711</td>\n","      <td>0.720152</td>\n","      <td>0.773949</td>\n","      <td>0.688380</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.679652</td>\n","      <td>0.743291</td>\n","      <td>0.641698</td>\n","      <td>0.693049</td>\n","      <td>0.716362</td>\n","      <td>0.671206</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009000</td>\n","      <td>0.075714</td>\n","      <td>0.725609</td>\n","      <td>0.728250</td>\n","      <td>0.728701</td>\n","      <td>0.920888</td>\n","      <td>0.920888</td>\n","      <td>0.920888</td>\n","      <td>0.686051</td>\n","      <td>0.688548</td>\n","      <td>0.690238</td>\n","      <td>0.686237</td>\n","      <td>0.672646</td>\n","      <td>0.700389</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004200</td>\n","      <td>0.070140</td>\n","      <td>0.728797</td>\n","      <td>0.766520</td>\n","      <td>0.699582</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.689732</td>\n","      <td>0.734746</td>\n","      <td>0.654631</td>\n","      <td>0.686314</td>\n","      <td>0.713087</td>\n","      <td>0.661479</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002800</td>\n","      <td>0.063233</td>\n","      <td>0.730212</td>\n","      <td>0.724872</td>\n","      <td>0.742038</td>\n","      <td>0.913295</td>\n","      <td>0.913295</td>\n","      <td>0.913295</td>\n","      <td>0.692164</td>\n","      <td>0.684354</td>\n","      <td>0.707512</td>\n","      <td>0.670968</td>\n","      <td>0.637478</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001800</td>\n","      <td>0.066238</td>\n","      <td>0.743272</td>\n","      <td>0.783037</td>\n","      <td>0.710611</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.925061</td>\n","      <td>0.706659</td>\n","      <td>0.754215</td>\n","      <td>0.667374</td>\n","      <td>0.697400</td>\n","      <td>0.729397</td>\n","      <td>0.668093</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.063550</td>\n","      <td>0.738375</td>\n","      <td>0.740714</td>\n","      <td>0.742628</td>\n","      <td>0.918280</td>\n","      <td>0.918280</td>\n","      <td>0.918280</td>\n","      <td>0.701393</td>\n","      <td>0.703162</td>\n","      <td>0.707303</td>\n","      <td>0.687618</td>\n","      <td>0.665816</td>\n","      <td>0.710895</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000900</td>\n","      <td>0.067717</td>\n","      <td>0.734156</td>\n","      <td>0.762986</td>\n","      <td>0.713035</td>\n","      <td>0.921178</td>\n","      <td>0.921178</td>\n","      <td>0.921178</td>\n","      <td>0.696372</td>\n","      <td>0.730490</td>\n","      <td>0.671246</td>\n","      <td>0.689764</td>\n","      <td>0.702136</td>\n","      <td>0.677821</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.068102</td>\n","      <td>0.731468</td>\n","      <td>0.733283</td>\n","      <td>0.735181</td>\n","      <td>0.917526</td>\n","      <td>0.917526</td>\n","      <td>0.917526</td>\n","      <td>0.693482</td>\n","      <td>0.695090</td>\n","      <td>0.698320</td>\n","      <td>0.683356</td>\n","      <td>0.671423</td>\n","      <td>0.695720</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.071383</td>\n","      <td>0.736826</td>\n","      <td>0.777894</td>\n","      <td>0.704902</td>\n","      <td>0.921583</td>\n","      <td>0.921583</td>\n","      <td>0.921583</td>\n","      <td>0.699547</td>\n","      <td>0.748433</td>\n","      <td>0.661315</td>\n","      <td>0.689655</td>\n","      <td>0.715781</td>\n","      <td>0.665370</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000400</td>\n","      <td>0.066302</td>\n","      <td>0.746180</td>\n","      <td>0.763480</td>\n","      <td>0.731717</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.710237</td>\n","      <td>0.730866</td>\n","      <td>0.692917</td>\n","      <td>0.693555</td>\n","      <td>0.704984</td>\n","      <td>0.682490</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000400</td>\n","      <td>0.067611</td>\n","      <td>0.746704</td>\n","      <td>0.770670</td>\n","      <td>0.726608</td>\n","      <td>0.923264</td>\n","      <td>0.923264</td>\n","      <td>0.923264</td>\n","      <td>0.710801</td>\n","      <td>0.739417</td>\n","      <td>0.686695</td>\n","      <td>0.694959</td>\n","      <td>0.712127</td>\n","      <td>0.678599</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.067893</td>\n","      <td>0.745393</td>\n","      <td>0.770733</td>\n","      <td>0.724258</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.709274</td>\n","      <td>0.739583</td>\n","      <td>0.683863</td>\n","      <td>0.694922</td>\n","      <td>0.714638</td>\n","      <td>0.676265</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.064138</td>\n","      <td>0.743047</td>\n","      <td>0.743330</td>\n","      <td>0.746428</td>\n","      <td>0.919961</td>\n","      <td>0.919961</td>\n","      <td>0.919961</td>\n","      <td>0.706700</td>\n","      <td>0.706387</td>\n","      <td>0.711283</td>\n","      <td>0.690888</td>\n","      <td>0.675847</td>\n","      <td>0.706615</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.061940</td>\n","      <td>0.749033</td>\n","      <td>0.750255</td>\n","      <td>0.750036</td>\n","      <td>0.922337</td>\n","      <td>0.922337</td>\n","      <td>0.922337</td>\n","      <td>0.713537</td>\n","      <td>0.714562</td>\n","      <td>0.715105</td>\n","      <td>0.699405</td>\n","      <td>0.689747</td>\n","      <td>0.709339</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.061968</td>\n","      <td>0.751629</td>\n","      <td>0.752499</td>\n","      <td>0.752875</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.716578</td>\n","      <td>0.717181</td>\n","      <td>0.718441</td>\n","      <td>0.701438</td>\n","      <td>0.691493</td>\n","      <td>0.711673</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.062454</td>\n","      <td>0.745919</td>\n","      <td>0.743490</td>\n","      <td>0.750613</td>\n","      <td>0.921583</td>\n","      <td>0.921583</td>\n","      <td>0.921583</td>\n","      <td>0.709969</td>\n","      <td>0.706685</td>\n","      <td>0.715893</td>\n","      <td>0.697051</td>\n","      <td>0.686275</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.062748</td>\n","      <td>0.745783</td>\n","      <td>0.744177</td>\n","      <td>0.749666</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.921757</td>\n","      <td>0.709780</td>\n","      <td>0.707485</td>\n","      <td>0.714731</td>\n","      <td>0.696952</td>\n","      <td>0.686815</td>\n","      <td>0.707393</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.001700</td>\n","      <td>0.062922</td>\n","      <td>0.748251</td>\n","      <td>0.749923</td>\n","      <td>0.748787</td>\n","      <td>0.922221</td>\n","      <td>0.922221</td>\n","      <td>0.922221</td>\n","      <td>0.712612</td>\n","      <td>0.714216</td>\n","      <td>0.713580</td>\n","      <td>0.697674</td>\n","      <td>0.689328</td>\n","      <td>0.706226</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.7482514340506803, 'eval_macro_precision': 0.7499226032729732, 'eval_macro_recall': 0.7487865275243121, 'eval_micro_f1': 0.9222209342761098, 'eval_micro_precision': 0.9222209342761098, 'eval_micro_recall': 0.9222209342761098, 'eval_macro_f1_no_o': 0.7126119429220629, 'eval_macro_precision_no_o': 0.714216180803627, 'eval_macro_recall_no_o': 0.7135801960316103, 'eval_micro_f1_no_o': 0.6976744186046513, 'eval_micro_precision_no_o': 0.6893277630079757, 'eval_micro_recall_no_o': 0.7062256809338522, 'eval_loss': 0.06292184718379511, 'eval_runtime': 10.4497, 'eval_samples_per_second': 11.579, 'eval_steps_per_second': 2.967, 'epoch': 18.0}\n","Accuracy for fold  34 :  0.6976744186046513  --  0.9222209342761098\n","--------------------------------\n","Testing process has finished.\n","Train run #35\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 25:31, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.024900</td>\n","      <td>0.100634</td>\n","      <td>0.743832</td>\n","      <td>0.764450</td>\n","      <td>0.729791</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.922858</td>\n","      <td>0.707467</td>\n","      <td>0.731599</td>\n","      <td>0.691010</td>\n","      <td>0.698400</td>\n","      <td>0.700313</td>\n","      <td>0.696498</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009000</td>\n","      <td>0.072897</td>\n","      <td>0.721468</td>\n","      <td>0.716684</td>\n","      <td>0.732073</td>\n","      <td>0.916483</td>\n","      <td>0.916483</td>\n","      <td>0.916483</td>\n","      <td>0.681694</td>\n","      <td>0.674956</td>\n","      <td>0.695205</td>\n","      <td>0.678632</td>\n","      <td>0.653113</td>\n","      <td>0.706226</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004100</td>\n","      <td>0.065535</td>\n","      <td>0.734113</td>\n","      <td>0.780182</td>\n","      <td>0.706572</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.924771</td>\n","      <td>0.695876</td>\n","      <td>0.750225</td>\n","      <td>0.663138</td>\n","      <td>0.697554</td>\n","      <td>0.713298</td>\n","      <td>0.682490</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002700</td>\n","      <td>0.064884</td>\n","      <td>0.723536</td>\n","      <td>0.747356</td>\n","      <td>0.712734</td>\n","      <td>0.918801</td>\n","      <td>0.918801</td>\n","      <td>0.918801</td>\n","      <td>0.683997</td>\n","      <td>0.711502</td>\n","      <td>0.671677</td>\n","      <td>0.681818</td>\n","      <td>0.675057</td>\n","      <td>0.688716</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.066867</td>\n","      <td>0.687796</td>\n","      <td>0.638561</td>\n","      <td>0.770071</td>\n","      <td>0.893184</td>\n","      <td>0.893184</td>\n","      <td>0.893184</td>\n","      <td>0.644486</td>\n","      <td>0.581980</td>\n","      <td>0.745234</td>\n","      <td>0.635685</td>\n","      <td>0.554269</td>\n","      <td>0.745136</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.068835</td>\n","      <td>0.722707</td>\n","      <td>0.761969</td>\n","      <td>0.698222</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.682654</td>\n","      <td>0.729346</td>\n","      <td>0.653193</td>\n","      <td>0.684887</td>\n","      <td>0.708229</td>\n","      <td>0.663035</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.071086</td>\n","      <td>0.723676</td>\n","      <td>0.750462</td>\n","      <td>0.705648</td>\n","      <td>0.919845</td>\n","      <td>0.919845</td>\n","      <td>0.919845</td>\n","      <td>0.684240</td>\n","      <td>0.715865</td>\n","      <td>0.662832</td>\n","      <td>0.685072</td>\n","      <td>0.694522</td>\n","      <td>0.675875</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.070637</td>\n","      <td>0.726407</td>\n","      <td>0.760564</td>\n","      <td>0.700766</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.922685</td>\n","      <td>0.687256</td>\n","      <td>0.727906</td>\n","      <td>0.656535</td>\n","      <td>0.694311</td>\n","      <td>0.715524</td>\n","      <td>0.674319</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.058089</td>\n","      <td>0.745304</td>\n","      <td>0.766705</td>\n","      <td>0.732218</td>\n","      <td>0.924597</td>\n","      <td>0.924597</td>\n","      <td>0.924597</td>\n","      <td>0.708971</td>\n","      <td>0.734109</td>\n","      <td>0.693535</td>\n","      <td>0.701899</td>\n","      <td>0.706184</td>\n","      <td>0.697665</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000400</td>\n","      <td>0.060033</td>\n","      <td>0.747341</td>\n","      <td>0.775046</td>\n","      <td>0.727884</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.925640</td>\n","      <td>0.711289</td>\n","      <td>0.744139</td>\n","      <td>0.688059</td>\n","      <td>0.703549</td>\n","      <td>0.717347</td>\n","      <td>0.690272</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000400</td>\n","      <td>0.060597</td>\n","      <td>0.742925</td>\n","      <td>0.772804</td>\n","      <td>0.721578</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.925582</td>\n","      <td>0.706122</td>\n","      <td>0.741594</td>\n","      <td>0.680599</td>\n","      <td>0.701810</td>\n","      <td>0.717949</td>\n","      <td>0.686381</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000400</td>\n","      <td>0.060806</td>\n","      <td>0.747088</td>\n","      <td>0.773493</td>\n","      <td>0.727633</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.710898</td>\n","      <td>0.742226</td>\n","      <td>0.687675</td>\n","      <td>0.704996</td>\n","      <td>0.718674</td>\n","      <td>0.691829</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.061271</td>\n","      <td>0.748296</td>\n","      <td>0.778730</td>\n","      <td>0.724714</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.927089</td>\n","      <td>0.712277</td>\n","      <td>0.748510</td>\n","      <td>0.684032</td>\n","      <td>0.707151</td>\n","      <td>0.726601</td>\n","      <td>0.688716</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.061275</td>\n","      <td>0.747708</td>\n","      <td>0.776923</td>\n","      <td>0.725505</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.711594</td>\n","      <td>0.746319</td>\n","      <td>0.685045</td>\n","      <td>0.705695</td>\n","      <td>0.722675</td>\n","      <td>0.689494</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.061004</td>\n","      <td>0.748837</td>\n","      <td>0.773302</td>\n","      <td>0.731191</td>\n","      <td>0.926510</td>\n","      <td>0.926510</td>\n","      <td>0.926510</td>\n","      <td>0.712909</td>\n","      <td>0.741865</td>\n","      <td>0.691906</td>\n","      <td>0.706161</td>\n","      <td>0.716921</td>\n","      <td>0.695720</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000300</td>\n","      <td>0.061035</td>\n","      <td>0.749281</td>\n","      <td>0.773658</td>\n","      <td>0.731355</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.926742</td>\n","      <td>0.713425</td>\n","      <td>0.742322</td>\n","      <td>0.692051</td>\n","      <td>0.707278</td>\n","      <td>0.719228</td>\n","      <td>0.695720</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.061268</td>\n","      <td>0.749439</td>\n","      <td>0.775462</td>\n","      <td>0.729886</td>\n","      <td>0.926973</td>\n","      <td>0.926973</td>\n","      <td>0.926973</td>\n","      <td>0.713601</td>\n","      <td>0.744478</td>\n","      <td>0.690270</td>\n","      <td>0.708028</td>\n","      <td>0.721616</td>\n","      <td>0.694942</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.001200</td>\n","      <td>0.061245</td>\n","      <td>0.749008</td>\n","      <td>0.774965</td>\n","      <td>0.729584</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.926858</td>\n","      <td>0.713099</td>\n","      <td>0.743888</td>\n","      <td>0.689929</td>\n","      <td>0.707351</td>\n","      <td>0.720630</td>\n","      <td>0.694553</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.7490079652307488, 'eval_macro_precision': 0.7749652605699265, 'eval_macro_recall': 0.7295839774937518, 'eval_micro_f1': 0.9268575402805147, 'eval_micro_precision': 0.9268575402805147, 'eval_micro_recall': 0.9268575402805147, 'eval_macro_f1_no_o': 0.7130990441919919, 'eval_macro_precision_no_o': 0.7438883804571591, 'eval_macro_recall_no_o': 0.6899286965248771, 'eval_micro_f1_no_o': 0.7073509015256588, 'eval_micro_precision_no_o': 0.7206297941057731, 'eval_micro_recall_no_o': 0.6945525291828794, 'eval_loss': 0.061244733816403656, 'eval_runtime': 10.4257, 'eval_samples_per_second': 11.606, 'eval_steps_per_second': 2.973, 'epoch': 18.0}\n","Accuracy for fold  35 :  0.7073509015256588  --  0.9268575402805147\n","--------------------------------\n","Testing process has finished.\n","Train run #36\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 25:30, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.024800</td>\n","      <td>0.098421</td>\n","      <td>0.742452</td>\n","      <td>0.755034</td>\n","      <td>0.734807</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.922511</td>\n","      <td>0.705905</td>\n","      <td>0.720469</td>\n","      <td>0.697101</td>\n","      <td>0.699477</td>\n","      <td>0.696642</td>\n","      <td>0.702335</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.008900</td>\n","      <td>0.069156</td>\n","      <td>0.748064</td>\n","      <td>0.748947</td>\n","      <td>0.750888</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.926336</td>\n","      <td>0.711989</td>\n","      <td>0.712838</td>\n","      <td>0.715464</td>\n","      <td>0.709840</td>\n","      <td>0.705340</td>\n","      <td>0.714397</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004100</td>\n","      <td>0.062069</td>\n","      <td>0.742725</td>\n","      <td>0.749855</td>\n","      <td>0.741467</td>\n","      <td>0.924829</td>\n","      <td>0.924829</td>\n","      <td>0.924829</td>\n","      <td>0.705877</td>\n","      <td>0.714075</td>\n","      <td>0.704530</td>\n","      <td>0.703216</td>\n","      <td>0.700231</td>\n","      <td>0.706226</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002700</td>\n","      <td>0.065833</td>\n","      <td>0.715770</td>\n","      <td>0.701755</td>\n","      <td>0.740198</td>\n","      <td>0.908369</td>\n","      <td>0.908369</td>\n","      <td>0.908369</td>\n","      <td>0.675761</td>\n","      <td>0.657367</td>\n","      <td>0.706250</td>\n","      <td>0.658435</td>\n","      <td>0.617297</td>\n","      <td>0.705447</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002000</td>\n","      <td>0.059792</td>\n","      <td>0.715804</td>\n","      <td>0.705064</td>\n","      <td>0.732704</td>\n","      <td>0.913817</td>\n","      <td>0.913817</td>\n","      <td>0.913817</td>\n","      <td>0.675032</td>\n","      <td>0.660843</td>\n","      <td>0.696372</td>\n","      <td>0.664583</td>\n","      <td>0.630056</td>\n","      <td>0.703113</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.074595</td>\n","      <td>0.701107</td>\n","      <td>0.712107</td>\n","      <td>0.711376</td>\n","      <td>0.902515</td>\n","      <td>0.902515</td>\n","      <td>0.902515</td>\n","      <td>0.659060</td>\n","      <td>0.669525</td>\n","      <td>0.673340</td>\n","      <td>0.637916</td>\n","      <td>0.592654</td>\n","      <td>0.690661</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001200</td>\n","      <td>0.062453</td>\n","      <td>0.702693</td>\n","      <td>0.699867</td>\n","      <td>0.726220</td>\n","      <td>0.900951</td>\n","      <td>0.900951</td>\n","      <td>0.900951</td>\n","      <td>0.660957</td>\n","      <td>0.654540</td>\n","      <td>0.691407</td>\n","      <td>0.637050</td>\n","      <td>0.580480</td>\n","      <td>0.705837</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.056899</td>\n","      <td>0.713201</td>\n","      <td>0.706522</td>\n","      <td>0.740686</td>\n","      <td>0.903617</td>\n","      <td>0.903617</td>\n","      <td>0.903617</td>\n","      <td>0.672981</td>\n","      <td>0.661741</td>\n","      <td>0.708352</td>\n","      <td>0.649043</td>\n","      <td>0.586792</td>\n","      <td>0.726070</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.067320</td>\n","      <td>0.721087</td>\n","      <td>0.727322</td>\n","      <td>0.723493</td>\n","      <td>0.916367</td>\n","      <td>0.916367</td>\n","      <td>0.916367</td>\n","      <td>0.681211</td>\n","      <td>0.687996</td>\n","      <td>0.684503</td>\n","      <td>0.670109</td>\n","      <td>0.658894</td>\n","      <td>0.681712</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000700</td>\n","      <td>0.071104</td>\n","      <td>0.726883</td>\n","      <td>0.749902</td>\n","      <td>0.710706</td>\n","      <td>0.920077</td>\n","      <td>0.920077</td>\n","      <td>0.920077</td>\n","      <td>0.687815</td>\n","      <td>0.715136</td>\n","      <td>0.668472</td>\n","      <td>0.679858</td>\n","      <td>0.691626</td>\n","      <td>0.668482</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.070554</td>\n","      <td>0.728940</td>\n","      <td>0.748186</td>\n","      <td>0.714836</td>\n","      <td>0.920077</td>\n","      <td>0.920077</td>\n","      <td>0.920077</td>\n","      <td>0.690259</td>\n","      <td>0.713054</td>\n","      <td>0.673461</td>\n","      <td>0.682687</td>\n","      <td>0.691264</td>\n","      <td>0.674319</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000400</td>\n","      <td>0.071745</td>\n","      <td>0.730604</td>\n","      <td>0.751638</td>\n","      <td>0.714446</td>\n","      <td>0.920598</td>\n","      <td>0.920598</td>\n","      <td>0.920598</td>\n","      <td>0.692174</td>\n","      <td>0.717163</td>\n","      <td>0.672870</td>\n","      <td>0.684200</td>\n","      <td>0.695617</td>\n","      <td>0.673152</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000500</td>\n","      <td>0.069650</td>\n","      <td>0.723672</td>\n","      <td>0.730267</td>\n","      <td>0.720275</td>\n","      <td>0.918164</td>\n","      <td>0.918164</td>\n","      <td>0.918164</td>\n","      <td>0.684200</td>\n","      <td>0.691779</td>\n","      <td>0.680351</td>\n","      <td>0.677388</td>\n","      <td>0.674643</td>\n","      <td>0.680156</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.070671</td>\n","      <td>0.728403</td>\n","      <td>0.738797</td>\n","      <td>0.721468</td>\n","      <td>0.919381</td>\n","      <td>0.919381</td>\n","      <td>0.919381</td>\n","      <td>0.689677</td>\n","      <td>0.701895</td>\n","      <td>0.681493</td>\n","      <td>0.682022</td>\n","      <td>0.684293</td>\n","      <td>0.679767</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.071320</td>\n","      <td>0.728652</td>\n","      <td>0.741338</td>\n","      <td>0.719621</td>\n","      <td>0.919903</td>\n","      <td>0.919903</td>\n","      <td>0.919903</td>\n","      <td>0.689912</td>\n","      <td>0.704908</td>\n","      <td>0.679179</td>\n","      <td>0.682602</td>\n","      <td>0.687451</td>\n","      <td>0.677821</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.071696</td>\n","      <td>0.728619</td>\n","      <td>0.742274</td>\n","      <td>0.718752</td>\n","      <td>0.919845</td>\n","      <td>0.919845</td>\n","      <td>0.919845</td>\n","      <td>0.689888</td>\n","      <td>0.706064</td>\n","      <td>0.678131</td>\n","      <td>0.682237</td>\n","      <td>0.688317</td>\n","      <td>0.676265</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.071512</td>\n","      <td>0.727869</td>\n","      <td>0.740792</td>\n","      <td>0.718723</td>\n","      <td>0.919671</td>\n","      <td>0.919671</td>\n","      <td>0.919671</td>\n","      <td>0.689026</td>\n","      <td>0.704326</td>\n","      <td>0.678131</td>\n","      <td>0.681702</td>\n","      <td>0.687228</td>\n","      <td>0.676265</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.071575</td>\n","      <td>0.728125</td>\n","      <td>0.741201</td>\n","      <td>0.718723</td>\n","      <td>0.919671</td>\n","      <td>0.919671</td>\n","      <td>0.919671</td>\n","      <td>0.689330</td>\n","      <td>0.704814</td>\n","      <td>0.678131</td>\n","      <td>0.681836</td>\n","      <td>0.687500</td>\n","      <td>0.676265</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.7281249722714715, 'eval_macro_precision': 0.7412012674526407, 'eval_macro_recall': 0.7187228993452413, 'eval_micro_f1': 0.9196708009736873, 'eval_micro_precision': 0.9196708009736873, 'eval_micro_recall': 0.9196708009736873, 'eval_macro_f1_no_o': 0.6893295253401782, 'eval_macro_precision_no_o': 0.7048135842676571, 'eval_macro_recall_no_o': 0.6781314058147038, 'eval_micro_f1_no_o': 0.6818360141231855, 'eval_micro_precision_no_o': 0.6875, 'eval_micro_recall_no_o': 0.6762645914396888, 'eval_loss': 0.07157490506744564, 'eval_runtime': 10.4402, 'eval_samples_per_second': 11.59, 'eval_steps_per_second': 2.969, 'epoch': 18.0}\n","Accuracy for fold  36 :  0.6818360141231855  --  0.9196708009736873\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"stream","name":"stdout","text":["Testing process has finished.\n","Train run #37\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 25:36, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.026500</td>\n","      <td>0.109337</td>\n","      <td>0.714919</td>\n","      <td>0.755934</td>\n","      <td>0.692763</td>\n","      <td>0.920019</td>\n","      <td>0.920019</td>\n","      <td>0.920019</td>\n","      <td>0.673805</td>\n","      <td>0.722552</td>\n","      <td>0.647051</td>\n","      <td>0.673100</td>\n","      <td>0.696339</td>\n","      <td>0.651362</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009100</td>\n","      <td>0.083736</td>\n","      <td>0.724928</td>\n","      <td>0.749874</td>\n","      <td>0.705876</td>\n","      <td>0.919555</td>\n","      <td>0.919555</td>\n","      <td>0.919555</td>\n","      <td>0.685615</td>\n","      <td>0.715400</td>\n","      <td>0.662701</td>\n","      <td>0.676905</td>\n","      <td>0.694354</td>\n","      <td>0.660311</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004100</td>\n","      <td>0.083003</td>\n","      <td>0.684806</td>\n","      <td>0.718867</td>\n","      <td>0.680476</td>\n","      <td>0.915324</td>\n","      <td>0.915324</td>\n","      <td>0.915324</td>\n","      <td>0.639050</td>\n","      <td>0.678787</td>\n","      <td>0.633998</td>\n","      <td>0.663813</td>\n","      <td>0.663813</td>\n","      <td>0.663813</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002500</td>\n","      <td>0.065888</td>\n","      <td>0.736710</td>\n","      <td>0.732575</td>\n","      <td>0.742055</td>\n","      <td>0.920134</td>\n","      <td>0.920134</td>\n","      <td>0.920134</td>\n","      <td>0.699134</td>\n","      <td>0.693859</td>\n","      <td>0.705818</td>\n","      <td>0.684412</td>\n","      <td>0.673831</td>\n","      <td>0.695331</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.068371</td>\n","      <td>0.717952</td>\n","      <td>0.719974</td>\n","      <td>0.719693</td>\n","      <td>0.916019</td>\n","      <td>0.916019</td>\n","      <td>0.916019</td>\n","      <td>0.677542</td>\n","      <td>0.679274</td>\n","      <td>0.680194</td>\n","      <td>0.668824</td>\n","      <td>0.654620</td>\n","      <td>0.683658</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.062116</td>\n","      <td>0.726569</td>\n","      <td>0.726251</td>\n","      <td>0.730069</td>\n","      <td>0.918859</td>\n","      <td>0.918859</td>\n","      <td>0.918859</td>\n","      <td>0.687385</td>\n","      <td>0.686338</td>\n","      <td>0.692140</td>\n","      <td>0.680980</td>\n","      <td>0.665429</td>\n","      <td>0.697276</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000900</td>\n","      <td>0.066695</td>\n","      <td>0.726030</td>\n","      <td>0.742787</td>\n","      <td>0.713093</td>\n","      <td>0.921236</td>\n","      <td>0.921236</td>\n","      <td>0.921236</td>\n","      <td>0.686594</td>\n","      <td>0.706296</td>\n","      <td>0.671347</td>\n","      <td>0.683099</td>\n","      <td>0.686861</td>\n","      <td>0.679377</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.065757</td>\n","      <td>0.731963</td>\n","      <td>0.743430</td>\n","      <td>0.723002</td>\n","      <td>0.921815</td>\n","      <td>0.921815</td>\n","      <td>0.921815</td>\n","      <td>0.693464</td>\n","      <td>0.706864</td>\n","      <td>0.682987</td>\n","      <td>0.686526</td>\n","      <td>0.687062</td>\n","      <td>0.685992</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.070413</td>\n","      <td>0.723993</td>\n","      <td>0.751030</td>\n","      <td>0.702057</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.684116</td>\n","      <td>0.716374</td>\n","      <td>0.657803</td>\n","      <td>0.685304</td>\n","      <td>0.703856</td>\n","      <td>0.667704</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000400</td>\n","      <td>0.067565</td>\n","      <td>0.736969</td>\n","      <td>0.752001</td>\n","      <td>0.725029</td>\n","      <td>0.922627</td>\n","      <td>0.922627</td>\n","      <td>0.922627</td>\n","      <td>0.699291</td>\n","      <td>0.717062</td>\n","      <td>0.685125</td>\n","      <td>0.689425</td>\n","      <td>0.695291</td>\n","      <td>0.683658</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000400</td>\n","      <td>0.068000</td>\n","      <td>0.738336</td>\n","      <td>0.749727</td>\n","      <td>0.729265</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.700872</td>\n","      <td>0.714293</td>\n","      <td>0.690158</td>\n","      <td>0.691947</td>\n","      <td>0.695208</td>\n","      <td>0.688716</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000500</td>\n","      <td>0.066218</td>\n","      <td>0.731869</td>\n","      <td>0.731373</td>\n","      <td>0.735549</td>\n","      <td>0.919497</td>\n","      <td>0.919497</td>\n","      <td>0.919497</td>\n","      <td>0.693567</td>\n","      <td>0.692484</td>\n","      <td>0.698363</td>\n","      <td>0.683486</td>\n","      <td>0.671675</td>\n","      <td>0.695720</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.063208</td>\n","      <td>0.742135</td>\n","      <td>0.745459</td>\n","      <td>0.741327</td>\n","      <td>0.921931</td>\n","      <td>0.921931</td>\n","      <td>0.921931</td>\n","      <td>0.705401</td>\n","      <td>0.708812</td>\n","      <td>0.704922</td>\n","      <td>0.694354</td>\n","      <td>0.683239</td>\n","      <td>0.705837</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.064203</td>\n","      <td>0.739267</td>\n","      <td>0.751286</td>\n","      <td>0.730761</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.702032</td>\n","      <td>0.716022</td>\n","      <td>0.692142</td>\n","      <td>0.696075</td>\n","      <td>0.695264</td>\n","      <td>0.696887</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.062780</td>\n","      <td>0.733810</td>\n","      <td>0.733963</td>\n","      <td>0.738213</td>\n","      <td>0.919323</td>\n","      <td>0.919323</td>\n","      <td>0.919323</td>\n","      <td>0.695878</td>\n","      <td>0.695145</td>\n","      <td>0.701914</td>\n","      <td>0.687653</td>\n","      <td>0.666910</td>\n","      <td>0.709728</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000400</td>\n","      <td>0.063108</td>\n","      <td>0.733324</td>\n","      <td>0.737685</td>\n","      <td>0.734032</td>\n","      <td>0.919961</td>\n","      <td>0.919961</td>\n","      <td>0.919961</td>\n","      <td>0.695264</td>\n","      <td>0.699592</td>\n","      <td>0.696843</td>\n","      <td>0.688897</td>\n","      <td>0.671344</td>\n","      <td>0.707393</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.063288</td>\n","      <td>0.733784</td>\n","      <td>0.740206</td>\n","      <td>0.732402</td>\n","      <td>0.920308</td>\n","      <td>0.920308</td>\n","      <td>0.920308</td>\n","      <td>0.695757</td>\n","      <td>0.702594</td>\n","      <td>0.694794</td>\n","      <td>0.688724</td>\n","      <td>0.673485</td>\n","      <td>0.704669</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.063306</td>\n","      <td>0.734072</td>\n","      <td>0.740328</td>\n","      <td>0.732875</td>\n","      <td>0.920482</td>\n","      <td>0.920482</td>\n","      <td>0.920482</td>\n","      <td>0.696081</td>\n","      <td>0.702725</td>\n","      <td>0.695334</td>\n","      <td>0.689485</td>\n","      <td>0.674228</td>\n","      <td>0.705447</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.7340720677393355, 'eval_macro_precision': 0.7403280005249541, 'eval_macro_recall': 0.7328748976539786, 'eval_micro_f1': 0.920482207024458, 'eval_micro_precision': 0.920482207024458, 'eval_micro_recall': 0.920482207024458, 'eval_macro_f1_no_o': 0.6960814464605901, 'eval_macro_precision_no_o': 0.7027250405483659, 'eval_macro_recall_no_o': 0.6953344340785567, 'eval_micro_f1_no_o': 0.689484692907397, 'eval_micro_precision_no_o': 0.6742283376719971, 'eval_micro_recall_no_o': 0.7054474708171207, 'eval_loss': 0.06330642707839126, 'eval_runtime': 10.5357, 'eval_samples_per_second': 11.485, 'eval_steps_per_second': 2.942, 'epoch': 18.0}\n","Accuracy for fold  37 :  0.689484692907397  --  0.920482207024458\n","--------------------------------\n","Testing process has finished.\n","Train run #38\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6840' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6840/6840 25:39, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.024700</td>\n","      <td>0.123693</td>\n","      <td>0.680698</td>\n","      <td>0.793893</td>\n","      <td>0.617456</td>\n","      <td>0.917932</td>\n","      <td>0.917932</td>\n","      <td>0.917932</td>\n","      <td>0.634428</td>\n","      <td>0.769734</td>\n","      <td>0.557263</td>\n","      <td>0.649845</td>\n","      <td>0.753593</td>\n","      <td>0.571206</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.008900</td>\n","      <td>0.081634</td>\n","      <td>0.720943</td>\n","      <td>0.738047</td>\n","      <td>0.708212</td>\n","      <td>0.920482</td>\n","      <td>0.920482</td>\n","      <td>0.920482</td>\n","      <td>0.680699</td>\n","      <td>0.700811</td>\n","      <td>0.665687</td>\n","      <td>0.679319</td>\n","      <td>0.683196</td>\n","      <td>0.675486</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004000</td>\n","      <td>0.072072</td>\n","      <td>0.723000</td>\n","      <td>0.699946</td>\n","      <td>0.751737</td>\n","      <td>0.912484</td>\n","      <td>0.912484</td>\n","      <td>0.912484</td>\n","      <td>0.683887</td>\n","      <td>0.654877</td>\n","      <td>0.719474</td>\n","      <td>0.675122</td>\n","      <td>0.631740</td>\n","      <td>0.724903</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002700</td>\n","      <td>0.077295</td>\n","      <td>0.711618</td>\n","      <td>0.754235</td>\n","      <td>0.680053</td>\n","      <td>0.918743</td>\n","      <td>0.918743</td>\n","      <td>0.918743</td>\n","      <td>0.670062</td>\n","      <td>0.721069</td>\n","      <td>0.631927</td>\n","      <td>0.663673</td>\n","      <td>0.697854</td>\n","      <td>0.632685</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001700</td>\n","      <td>0.070233</td>\n","      <td>0.725613</td>\n","      <td>0.762830</td>\n","      <td>0.696937</td>\n","      <td>0.922163</td>\n","      <td>0.922163</td>\n","      <td>0.922163</td>\n","      <td>0.686058</td>\n","      <td>0.730321</td>\n","      <td>0.651749</td>\n","      <td>0.680578</td>\n","      <td>0.702568</td>\n","      <td>0.659922</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.074654</td>\n","      <td>0.725090</td>\n","      <td>0.760325</td>\n","      <td>0.695384</td>\n","      <td>0.921989</td>\n","      <td>0.921989</td>\n","      <td>0.921989</td>\n","      <td>0.685537</td>\n","      <td>0.727929</td>\n","      <td>0.649574</td>\n","      <td>0.677821</td>\n","      <td>0.712570</td>\n","      <td>0.646304</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000900</td>\n","      <td>0.068073</td>\n","      <td>0.733772</td>\n","      <td>0.755641</td>\n","      <td>0.717437</td>\n","      <td>0.922453</td>\n","      <td>0.922453</td>\n","      <td>0.922453</td>\n","      <td>0.695605</td>\n","      <td>0.721238</td>\n","      <td>0.676426</td>\n","      <td>0.690895</td>\n","      <td>0.693878</td>\n","      <td>0.687938</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.068655</td>\n","      <td>0.732102</td>\n","      <td>0.755098</td>\n","      <td>0.714760</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.693640</td>\n","      <td>0.720708</td>\n","      <td>0.673168</td>\n","      <td>0.689953</td>\n","      <td>0.695962</td>\n","      <td>0.684047</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.069859</td>\n","      <td>0.734469</td>\n","      <td>0.759423</td>\n","      <td>0.713476</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.696300</td>\n","      <td>0.726102</td>\n","      <td>0.671114</td>\n","      <td>0.692998</td>\n","      <td>0.711011</td>\n","      <td>0.675875</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000400</td>\n","      <td>0.070141</td>\n","      <td>0.733047</td>\n","      <td>0.755775</td>\n","      <td>0.714923</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.694629</td>\n","      <td>0.721552</td>\n","      <td>0.673074</td>\n","      <td>0.689832</td>\n","      <td>0.700200</td>\n","      <td>0.679767</td>\n","    </tr>\n","    <tr>\n","      <td>4180</td>\n","      <td>0.000500</td>\n","      <td>0.071679</td>\n","      <td>0.734233</td>\n","      <td>0.761620</td>\n","      <td>0.711885</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.924191</td>\n","      <td>0.696013</td>\n","      <td>0.728664</td>\n","      <td>0.669235</td>\n","      <td>0.692477</td>\n","      <td>0.710774</td>\n","      <td>0.675097</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000400</td>\n","      <td>0.071260</td>\n","      <td>0.733585</td>\n","      <td>0.756598</td>\n","      <td>0.714504</td>\n","      <td>0.923612</td>\n","      <td>0.923612</td>\n","      <td>0.923612</td>\n","      <td>0.695274</td>\n","      <td>0.722649</td>\n","      <td>0.672483</td>\n","      <td>0.690859</td>\n","      <td>0.704408</td>\n","      <td>0.677821</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.000400</td>\n","      <td>0.071380</td>\n","      <td>0.733560</td>\n","      <td>0.756202</td>\n","      <td>0.714579</td>\n","      <td>0.923612</td>\n","      <td>0.923612</td>\n","      <td>0.923612</td>\n","      <td>0.695251</td>\n","      <td>0.722178</td>\n","      <td>0.672593</td>\n","      <td>0.691241</td>\n","      <td>0.704362</td>\n","      <td>0.678599</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.000300</td>\n","      <td>0.069229</td>\n","      <td>0.736932</td>\n","      <td>0.752177</td>\n","      <td>0.724746</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.923438</td>\n","      <td>0.699118</td>\n","      <td>0.716974</td>\n","      <td>0.684830</td>\n","      <td>0.692023</td>\n","      <td>0.693782</td>\n","      <td>0.690272</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000400</td>\n","      <td>0.069539</td>\n","      <td>0.735704</td>\n","      <td>0.750133</td>\n","      <td>0.724056</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.922916</td>\n","      <td>0.697752</td>\n","      <td>0.714657</td>\n","      <td>0.684093</td>\n","      <td>0.690852</td>\n","      <td>0.692609</td>\n","      <td>0.689105</td>\n","    </tr>\n","    <tr>\n","      <td>6080</td>\n","      <td>0.000300</td>\n","      <td>0.069601</td>\n","      <td>0.735089</td>\n","      <td>0.748330</td>\n","      <td>0.724675</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.922569</td>\n","      <td>0.697044</td>\n","      <td>0.712481</td>\n","      <td>0.684905</td>\n","      <td>0.689615</td>\n","      <td>0.689347</td>\n","      <td>0.689883</td>\n","    </tr>\n","    <tr>\n","      <td>6460</td>\n","      <td>0.000300</td>\n","      <td>0.069735</td>\n","      <td>0.736272</td>\n","      <td>0.750599</td>\n","      <td>0.724984</td>\n","      <td>0.922743</td>\n","      <td>0.922743</td>\n","      <td>0.922743</td>\n","      <td>0.698418</td>\n","      <td>0.715139</td>\n","      <td>0.685243</td>\n","      <td>0.690407</td>\n","      <td>0.690541</td>\n","      <td>0.690272</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000500</td>\n","      <td>0.069801</td>\n","      <td>0.736103</td>\n","      <td>0.750176</td>\n","      <td>0.724994</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.698220</td>\n","      <td>0.714655</td>\n","      <td>0.685243</td>\n","      <td>0.690675</td>\n","      <td>0.691079</td>\n","      <td>0.690272</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3800\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4180\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4560\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4940\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5320\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5700\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6080\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6460\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6840\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 18\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6840\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.7361031218931592, 'eval_macro_precision': 0.7501761339781974, 'eval_macro_recall': 0.7249939261569651, 'eval_micro_f1': 0.9228005100266605, 'eval_micro_precision': 0.9228005100266605, 'eval_micro_recall': 0.9228005100266605, 'eval_macro_f1_no_o': 0.6982204303103279, 'eval_macro_precision_no_o': 0.7146553432532681, 'eval_macro_recall_no_o': 0.6852432997573564, 'eval_micro_f1_no_o': 0.6906754915320226, 'eval_micro_precision_no_o': 0.691079080638878, 'eval_micro_recall_no_o': 0.690272373540856, 'eval_loss': 0.06980091975617315, 'eval_runtime': 10.4734, 'eval_samples_per_second': 11.553, 'eval_steps_per_second': 2.96, 'epoch': 18.0}\n","Accuracy for fold  38 :  0.6906754915320226  --  0.9228005100266605\n","--------------------------------\n","Testing process has finished.\n","Train run #39\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3393' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3393/6840 12:28 < 12:41, 4.53 it/s, Epoch 8.93/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.024700</td>\n","      <td>0.104479</td>\n","      <td>0.732387</td>\n","      <td>0.745551</td>\n","      <td>0.721228</td>\n","      <td>0.923264</td>\n","      <td>0.923264</td>\n","      <td>0.923264</td>\n","      <td>0.693920</td>\n","      <td>0.709848</td>\n","      <td>0.680327</td>\n","      <td>0.689573</td>\n","      <td>0.704260</td>\n","      <td>0.675486</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.008900</td>\n","      <td>0.089304</td>\n","      <td>0.725969</td>\n","      <td>0.799297</td>\n","      <td>0.671474</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.923728</td>\n","      <td>0.686693</td>\n","      <td>0.774594</td>\n","      <td>0.620692</td>\n","      <td>0.683135</td>\n","      <td>0.754468</td>\n","      <td>0.624125</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.004000</td>\n","      <td>0.072586</td>\n","      <td>0.714248</td>\n","      <td>0.697758</td>\n","      <td>0.738636</td>\n","      <td>0.910282</td>\n","      <td>0.910282</td>\n","      <td>0.910282</td>\n","      <td>0.673737</td>\n","      <td>0.652449</td>\n","      <td>0.704189</td>\n","      <td>0.662672</td>\n","      <td>0.621171</td>\n","      <td>0.710117</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.002500</td>\n","      <td>0.068425</td>\n","      <td>0.727229</td>\n","      <td>0.739098</td>\n","      <td>0.724047</td>\n","      <td>0.916483</td>\n","      <td>0.916483</td>\n","      <td>0.916483</td>\n","      <td>0.688447</td>\n","      <td>0.701195</td>\n","      <td>0.685818</td>\n","      <td>0.679153</td>\n","      <td>0.654749</td>\n","      <td>0.705447</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001900</td>\n","      <td>0.070226</td>\n","      <td>0.721357</td>\n","      <td>0.746753</td>\n","      <td>0.706585</td>\n","      <td>0.920366</td>\n","      <td>0.920366</td>\n","      <td>0.920366</td>\n","      <td>0.681384</td>\n","      <td>0.710794</td>\n","      <td>0.664368</td>\n","      <td>0.689189</td>\n","      <td>0.683908</td>\n","      <td>0.694553</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001300</td>\n","      <td>0.068931</td>\n","      <td>0.745837</td>\n","      <td>0.764205</td>\n","      <td>0.732955</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.922801</td>\n","      <td>0.709840</td>\n","      <td>0.731660</td>\n","      <td>0.694418</td>\n","      <td>0.696133</td>\n","      <td>0.706165</td>\n","      <td>0.686381</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.067664</td>\n","      <td>0.741608</td>\n","      <td>0.760482</td>\n","      <td>0.727215</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.923380</td>\n","      <td>0.704767</td>\n","      <td>0.727264</td>\n","      <td>0.687494</td>\n","      <td>0.694378</td>\n","      <td>0.706688</td>\n","      <td>0.682490</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.059130</td>\n","      <td>0.740140</td>\n","      <td>0.729284</td>\n","      <td>0.755924</td>\n","      <td>0.916657</td>\n","      <td>0.916657</td>\n","      <td>0.916657</td>\n","      <td>0.703632</td>\n","      <td>0.689537</td>\n","      <td>0.723450</td>\n","      <td>0.687292</td>\n","      <td>0.655941</td>\n","      <td>0.721790</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"]}]},{"cell_type":"code","source":["loop_results"],"metadata":{"id":"9ZyF37RkPx8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loop_resultss"],"metadata":{"id":"xyOgajywP1OX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum = 0.0\n","for value in loop_results:\n","  sum += value\n","print(f'Average micro_f1_no_o: {sum/len(loop_results)} %')\n","\n","sum = 0.0\n","for value in loop_resultss:\n","  sum += value\n","print(f'Average micro_f1: {sum/len(loop_results)} %')"],"metadata":{"id":"krSqZMYibTus"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ner_model = NerModel(BertEmbModel).to('cuda')  # make sure we move the model to the GPU for training\n","ner_model.load_state_dict(torch.load(save_path))\n","ner_model.eval()"],"metadata":{"id":"lsAYGx7wO2bL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dcdDIwq0kXy2"},"source":["# Get Values of Thruth"]},{"cell_type":"code","metadata":{"id":"uCnsc2rplFzW"},"source":["# Pytorch thing (if we aren't training, do this)\n","ner_model.eval()\n","ner_model.to('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVfjoHmcI0pL"},"source":["output_preds = []\n","output_real = []\n","for x in range(len(test)):\n","  inputs1 = test[x]['input_ids'][0].clone().detach().to(torch.long).unsqueeze(0).to('cuda')\n","  inputs2 = test[x]['attention_mask'][0].clone().detach().to(torch.long).unsqueeze(0).to('cuda')\n","  inputs = {'input_ids': inputs1,  'attention_mask': inputs2}\n","  #print(inputs)\n","\n","  temp_test = test[x]\n","  temp_out = temp_test.pop(\"labels\")\n","  output_real.append(np.array(temp_out[temp_out != -100])) \n","\n","  gen_preds = ner_model(**inputs)\n","  label_preds = np.argmax(gen_preds.cpu().detach().numpy(), axis=-1)[0]\n","  output_preds.append(label_preds[temp_out != -100])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXBvXjpP-VGh"},"source":["for x in range(len(output_real)):\n","  output_real[x] = [ID2Entity(y) for y in output_real[x]]\n","  output_preds[x] = [ID2Entity(z) for z in output_preds[x]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pd1Xl1giQ7z7"},"source":["#output_real[28]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rslGT68vvC8"},"source":["from sklearn.metrics import mean_squared_error, multilabel_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams['figure.figsize'] = [12, 9]\n","plt.rcParams[\"figure.autolayout\"] = True\n","plt.rcParams.update({'font.size': 13})\n","\n","labels = [\"None\", 'B-TASK', 'I-TASK', 'B-DATASET', 'I-DATASET', 'B-METRIC', 'I-METRIC']\n","\n","cm = confusion_matrix(output_real[0], output_preds[0], labels=labels)\n","for x in range(len(output_real)-1):\n","  cm += confusion_matrix(output_real[x+1], output_preds[x+1], labels=labels)\n","print(cm)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","disp.plot(cmap=plt.cm.Blues)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m10aBi5x5Hdb"},"source":["del cm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgGd1XfheERK"},"source":["len(output_real)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHlg1QV3eZCa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0w9Lc0d3iOT"},"source":["# Test over Text"]},{"cell_type":"code","metadata":{"id":"mgNM3p1q3hrf"},"source":["def prepare_input(txt):\n","  inputs = BertTokenizer(txt, return_tensors='pt', padding='max_length', truncation=True, max_length=150).to('cuda')\n","  return inputs\n","\n","input_text = [\"English is shown to be trans-context-free on the basis of coordinations of the respectively type that involve strictly syntactic cross-serial agreement.\"]\n","##input_text = [\"The transient analysis of gyro-elastic structured media, composed of periodically placed masses interconnected by elastic rods and attached to gyroscopic spinners, is presented.\"] \n","##input_text = [\"The results indicated that thermal curing promoted the early strength of mortars, while decreased the late strength of mortars.\"] \n","##input_text = [\"A wide variety of processes are attested in the literature, and we find different forms of clippings in our data, including mixtures of different clippings, homophone respellings, phonetic respellings in-cluding informal oral forms, initialisms (but no acronyms), and mixtures of clipping together with homo-phone and phonetic respellings.\"] \n","\n","#input_text = [\"The goal is to accurately predict the running time of applications for task scheduling and job migration.\"] \n","#input_text = [\"This paper reports on the development of a cross-domain framework for describing complex design practices.\"] \n","#input_text = [\"Studies of inequality in China typically ignore cost of liv-ing differences between areas.\"] \n","#input_text = [\"The present study was designed to explore the long-term differences be-tween three mouse models for depression.\"]\n","#input_text = [\"Finally, regarding professional competencies, teachers appeared to be largely unprepared to conduct language assessments consistent with the LAR demands.\"] \n","\n","##input_text = [\"propose a fast and reliable restoration method of virtual resources on OpenStack when physical servers or virtual machines are down.\"] \n","##input_text = [\"The results from our simulations reveal that the network assisted adaptation clearly outperforms the purely client-based DASH heuristics in some of the metrics, not all of them, particularly, in situations when the achievable throughput is moderately high or the link quality of the mobile clients does not differ from each other substantially.\"] \n","##input_text = [\"For hard rock drilling in coal mine, the drilling efficiency and service life of polycrystalline diamond compact bit are very low.\"] \n","##input_text = [\"Capturing changes in foreign reserves and exchange rates through the exchange market pressure, this article investigates whether economic policy uncertainty plays any role in exchange market pressure movements while controlling for the effects of domestic and external factors.\"] \n","##input_text = [\"This paper presents design of an self contained actuators unit in wide area damping control of power system in stabilizing system response for both nominal system condition and during actuator faults.\"] \n","\n","##input_text = [\"Ultrasound-based brain stimulation techniques may become a powerful new technique to modulate the human brain in a focal and targeted manner.\"] \n","#input_text = [\"Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization.\"]\n","\n","# Tokenize + pad\n","inputs = prepare_input(input_text)\n","\n","#inputs\n","#print(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaG1vUq58BFI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642318432393,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"2b83b4d9-4f3e-488d-8763-5171a5ab7811"},"source":["# Pytorch thing (if we aren't training, do this)\n","ner_model.eval()\n","\n","# Get predictions\n","preds = ner_model(**inputs).cpu().detach().numpy()\n","preds = np.argmax(preds, axis=-1)[0]\n","pred_labels = [ID2Entity(x) for x in preds]\n","\n","# Convert token ids to text\n","tokens = BertTokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","\n","# Display result\n","for token, label in zip(tokens, pred_labels):\n","  if token == '[SEP]':\n","    break\n","  if token == '[CLS]':\n","    continue\n","  print('{} -> {}'.format(token, label))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["english -> None\n","is -> None\n","shown -> None\n","to -> None\n","be -> None\n","trans -> None\n","- -> None\n","context -> None\n","- -> None\n","free -> None\n","on -> None\n","the -> None\n","basis -> None\n","of -> None\n","coordination -> None\n","##s -> None\n","of -> None\n","the -> None\n","respectively -> None\n","type -> None\n","that -> None\n","involve -> None\n","strictly -> None\n","syntactic -> None\n","cross -> None\n","- -> None\n","serial -> None\n","agreement -> None\n",". -> None\n"]}]},{"cell_type":"markdown","metadata":{"id":"hC35X0v72kXB"},"source":["# Model Save and Load"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AnK4H5vP2kJU","executionInfo":{"status":"ok","timestamp":1638821351076,"user_tz":360,"elapsed":10,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"e55e8ead-0b25-4eb8-bdc5-f7376e13eaaf"},"source":["print(\"Our model: \\n\\n\", ner_model, '\\n')\n","print(\"The state dict keys: \\n\\n\", ner_model.state_dict().keys())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our model: \n","\n"," NerModel(\n","  (sci_embeddings): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (embd_dropout): Dropout(p=0.1, inplace=False)\n","  (ff_dropout): Dropout(p=0.1, inplace=False)\n","  (ff): Linear(in_features=200, out_features=14, bias=True)\n","  (tanh): Tanh()\n","  (lstm): LSTM(768, 100, bidirectional=True)\n","  (lstm_drop): Dropout(p=0.4, inplace=False)\n","  (ff_act): ReLU()\n","  (classifier): Linear(in_features=14, out_features=4, bias=True)\n",") \n","\n","The state dict keys: \n","\n"," odict_keys(['sci_embeddings.embeddings.position_ids', 'sci_embeddings.embeddings.word_embeddings.weight', 'sci_embeddings.embeddings.position_embeddings.weight', 'sci_embeddings.embeddings.token_type_embeddings.weight', 'sci_embeddings.embeddings.LayerNorm.weight', 'sci_embeddings.embeddings.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.attention.self.query.weight', 'sci_embeddings.encoder.layer.0.attention.self.query.bias', 'sci_embeddings.encoder.layer.0.attention.self.key.weight', 'sci_embeddings.encoder.layer.0.attention.self.key.bias', 'sci_embeddings.encoder.layer.0.attention.self.value.weight', 'sci_embeddings.encoder.layer.0.attention.self.value.bias', 'sci_embeddings.encoder.layer.0.attention.output.dense.weight', 'sci_embeddings.encoder.layer.0.attention.output.dense.bias', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.intermediate.dense.weight', 'sci_embeddings.encoder.layer.0.intermediate.dense.bias', 'sci_embeddings.encoder.layer.0.output.dense.weight', 'sci_embeddings.encoder.layer.0.output.dense.bias', 'sci_embeddings.encoder.layer.0.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.attention.self.query.weight', 'sci_embeddings.encoder.layer.1.attention.self.query.bias', 'sci_embeddings.encoder.layer.1.attention.self.key.weight', 'sci_embeddings.encoder.layer.1.attention.self.key.bias', 'sci_embeddings.encoder.layer.1.attention.self.value.weight', 'sci_embeddings.encoder.layer.1.attention.self.value.bias', 'sci_embeddings.encoder.layer.1.attention.output.dense.weight', 'sci_embeddings.encoder.layer.1.attention.output.dense.bias', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.intermediate.dense.weight', 'sci_embeddings.encoder.layer.1.intermediate.dense.bias', 'sci_embeddings.encoder.layer.1.output.dense.weight', 'sci_embeddings.encoder.layer.1.output.dense.bias', 'sci_embeddings.encoder.layer.1.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.attention.self.query.weight', 'sci_embeddings.encoder.layer.2.attention.self.query.bias', 'sci_embeddings.encoder.layer.2.attention.self.key.weight', 'sci_embeddings.encoder.layer.2.attention.self.key.bias', 'sci_embeddings.encoder.layer.2.attention.self.value.weight', 'sci_embeddings.encoder.layer.2.attention.self.value.bias', 'sci_embeddings.encoder.layer.2.attention.output.dense.weight', 'sci_embeddings.encoder.layer.2.attention.output.dense.bias', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.intermediate.dense.weight', 'sci_embeddings.encoder.layer.2.intermediate.dense.bias', 'sci_embeddings.encoder.layer.2.output.dense.weight', 'sci_embeddings.encoder.layer.2.output.dense.bias', 'sci_embeddings.encoder.layer.2.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.attention.self.query.weight', 'sci_embeddings.encoder.layer.3.attention.self.query.bias', 'sci_embeddings.encoder.layer.3.attention.self.key.weight', 'sci_embeddings.encoder.layer.3.attention.self.key.bias', 'sci_embeddings.encoder.layer.3.attention.self.value.weight', 'sci_embeddings.encoder.layer.3.attention.self.value.bias', 'sci_embeddings.encoder.layer.3.attention.output.dense.weight', 'sci_embeddings.encoder.layer.3.attention.output.dense.bias', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.intermediate.dense.weight', 'sci_embeddings.encoder.layer.3.intermediate.dense.bias', 'sci_embeddings.encoder.layer.3.output.dense.weight', 'sci_embeddings.encoder.layer.3.output.dense.bias', 'sci_embeddings.encoder.layer.3.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.attention.self.query.weight', 'sci_embeddings.encoder.layer.4.attention.self.query.bias', 'sci_embeddings.encoder.layer.4.attention.self.key.weight', 'sci_embeddings.encoder.layer.4.attention.self.key.bias', 'sci_embeddings.encoder.layer.4.attention.self.value.weight', 'sci_embeddings.encoder.layer.4.attention.self.value.bias', 'sci_embeddings.encoder.layer.4.attention.output.dense.weight', 'sci_embeddings.encoder.layer.4.attention.output.dense.bias', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.intermediate.dense.weight', 'sci_embeddings.encoder.layer.4.intermediate.dense.bias', 'sci_embeddings.encoder.layer.4.output.dense.weight', 'sci_embeddings.encoder.layer.4.output.dense.bias', 'sci_embeddings.encoder.layer.4.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.attention.self.query.weight', 'sci_embeddings.encoder.layer.5.attention.self.query.bias', 'sci_embeddings.encoder.layer.5.attention.self.key.weight', 'sci_embeddings.encoder.layer.5.attention.self.key.bias', 'sci_embeddings.encoder.layer.5.attention.self.value.weight', 'sci_embeddings.encoder.layer.5.attention.self.value.bias', 'sci_embeddings.encoder.layer.5.attention.output.dense.weight', 'sci_embeddings.encoder.layer.5.attention.output.dense.bias', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.intermediate.dense.weight', 'sci_embeddings.encoder.layer.5.intermediate.dense.bias', 'sci_embeddings.encoder.layer.5.output.dense.weight', 'sci_embeddings.encoder.layer.5.output.dense.bias', 'sci_embeddings.encoder.layer.5.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.attention.self.query.weight', 'sci_embeddings.encoder.layer.6.attention.self.query.bias', 'sci_embeddings.encoder.layer.6.attention.self.key.weight', 'sci_embeddings.encoder.layer.6.attention.self.key.bias', 'sci_embeddings.encoder.layer.6.attention.self.value.weight', 'sci_embeddings.encoder.layer.6.attention.self.value.bias', 'sci_embeddings.encoder.layer.6.attention.output.dense.weight', 'sci_embeddings.encoder.layer.6.attention.output.dense.bias', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.intermediate.dense.weight', 'sci_embeddings.encoder.layer.6.intermediate.dense.bias', 'sci_embeddings.encoder.layer.6.output.dense.weight', 'sci_embeddings.encoder.layer.6.output.dense.bias', 'sci_embeddings.encoder.layer.6.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.attention.self.query.weight', 'sci_embeddings.encoder.layer.7.attention.self.query.bias', 'sci_embeddings.encoder.layer.7.attention.self.key.weight', 'sci_embeddings.encoder.layer.7.attention.self.key.bias', 'sci_embeddings.encoder.layer.7.attention.self.value.weight', 'sci_embeddings.encoder.layer.7.attention.self.value.bias', 'sci_embeddings.encoder.layer.7.attention.output.dense.weight', 'sci_embeddings.encoder.layer.7.attention.output.dense.bias', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.intermediate.dense.weight', 'sci_embeddings.encoder.layer.7.intermediate.dense.bias', 'sci_embeddings.encoder.layer.7.output.dense.weight', 'sci_embeddings.encoder.layer.7.output.dense.bias', 'sci_embeddings.encoder.layer.7.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.attention.self.query.weight', 'sci_embeddings.encoder.layer.8.attention.self.query.bias', 'sci_embeddings.encoder.layer.8.attention.self.key.weight', 'sci_embeddings.encoder.layer.8.attention.self.key.bias', 'sci_embeddings.encoder.layer.8.attention.self.value.weight', 'sci_embeddings.encoder.layer.8.attention.self.value.bias', 'sci_embeddings.encoder.layer.8.attention.output.dense.weight', 'sci_embeddings.encoder.layer.8.attention.output.dense.bias', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.intermediate.dense.weight', 'sci_embeddings.encoder.layer.8.intermediate.dense.bias', 'sci_embeddings.encoder.layer.8.output.dense.weight', 'sci_embeddings.encoder.layer.8.output.dense.bias', 'sci_embeddings.encoder.layer.8.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.attention.self.query.weight', 'sci_embeddings.encoder.layer.9.attention.self.query.bias', 'sci_embeddings.encoder.layer.9.attention.self.key.weight', 'sci_embeddings.encoder.layer.9.attention.self.key.bias', 'sci_embeddings.encoder.layer.9.attention.self.value.weight', 'sci_embeddings.encoder.layer.9.attention.self.value.bias', 'sci_embeddings.encoder.layer.9.attention.output.dense.weight', 'sci_embeddings.encoder.layer.9.attention.output.dense.bias', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.intermediate.dense.weight', 'sci_embeddings.encoder.layer.9.intermediate.dense.bias', 'sci_embeddings.encoder.layer.9.output.dense.weight', 'sci_embeddings.encoder.layer.9.output.dense.bias', 'sci_embeddings.encoder.layer.9.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.attention.self.query.weight', 'sci_embeddings.encoder.layer.10.attention.self.query.bias', 'sci_embeddings.encoder.layer.10.attention.self.key.weight', 'sci_embeddings.encoder.layer.10.attention.self.key.bias', 'sci_embeddings.encoder.layer.10.attention.self.value.weight', 'sci_embeddings.encoder.layer.10.attention.self.value.bias', 'sci_embeddings.encoder.layer.10.attention.output.dense.weight', 'sci_embeddings.encoder.layer.10.attention.output.dense.bias', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.intermediate.dense.weight', 'sci_embeddings.encoder.layer.10.intermediate.dense.bias', 'sci_embeddings.encoder.layer.10.output.dense.weight', 'sci_embeddings.encoder.layer.10.output.dense.bias', 'sci_embeddings.encoder.layer.10.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.attention.self.query.weight', 'sci_embeddings.encoder.layer.11.attention.self.query.bias', 'sci_embeddings.encoder.layer.11.attention.self.key.weight', 'sci_embeddings.encoder.layer.11.attention.self.key.bias', 'sci_embeddings.encoder.layer.11.attention.self.value.weight', 'sci_embeddings.encoder.layer.11.attention.self.value.bias', 'sci_embeddings.encoder.layer.11.attention.output.dense.weight', 'sci_embeddings.encoder.layer.11.attention.output.dense.bias', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.intermediate.dense.weight', 'sci_embeddings.encoder.layer.11.intermediate.dense.bias', 'sci_embeddings.encoder.layer.11.output.dense.weight', 'sci_embeddings.encoder.layer.11.output.dense.bias', 'sci_embeddings.encoder.layer.11.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.output.LayerNorm.bias', 'sci_embeddings.pooler.dense.weight', 'sci_embeddings.pooler.dense.bias', 'ff.weight', 'ff.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias'])\n"]}]},{"cell_type":"code","metadata":{"id":"KftdO6GD2rWF"},"source":["torch.save(ner_model.state_dict(), 'trained_model_dic.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"kd4NJxK14P14","executionInfo":{"status":"ok","timestamp":1638821352369,"user_tz":360,"elapsed":7,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"335c36f7-29b2-4d7c-d212-441f42d34dc9"},"source":["# download checkpoint file\n","files.download('trained_model_dic.pth')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_96adb4c2-159a-402a-aa47-28def7a32b49\", \"trained_model_dic.pth\", 442559463)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1hvL5Nb3kmo","executionInfo":{"status":"ok","timestamp":1637655465909,"user_tz":360,"elapsed":11,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"a2910628-2592-4edd-b135-c329d0352c5e"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["checkpoint.pth\t  model-fold-1.pth  model-fold-4.pth  trained_model_dic.pth\n","dev.json\t  model-fold-2.pth  sample_data       trained_scibert_ner_model\n","model-fold-0.pth  model-fold-3.pth  test.json\t      train.json\n"]}]},{"cell_type":"markdown","metadata":{"id":"hQL7fqSY2x9e"},"source":["Loading the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ufmrdfeT6Qz","executionInfo":{"status":"ok","timestamp":1638565657653,"user_tz":360,"elapsed":324,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"fd7fd743-41b0-4b32-eefd-db4f427d5da0"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data  test_500_v2.conll\ttrain_1500_v2.conll  trained_model_dic.pth\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4Eus2gmJ-bS","executionInfo":{"status":"ok","timestamp":1638831550021,"user_tz":360,"elapsed":10896,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"1e8838d1-d51c-47db-8ee2-c0bbb2bb3a0b"},"source":["state_dict = torch.load('trained_model_dic.pth')\n","print(state_dict.keys())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["odict_keys(['sci_embeddings.embeddings.position_ids', 'sci_embeddings.embeddings.word_embeddings.weight', 'sci_embeddings.embeddings.position_embeddings.weight', 'sci_embeddings.embeddings.token_type_embeddings.weight', 'sci_embeddings.embeddings.LayerNorm.weight', 'sci_embeddings.embeddings.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.attention.self.query.weight', 'sci_embeddings.encoder.layer.0.attention.self.query.bias', 'sci_embeddings.encoder.layer.0.attention.self.key.weight', 'sci_embeddings.encoder.layer.0.attention.self.key.bias', 'sci_embeddings.encoder.layer.0.attention.self.value.weight', 'sci_embeddings.encoder.layer.0.attention.self.value.bias', 'sci_embeddings.encoder.layer.0.attention.output.dense.weight', 'sci_embeddings.encoder.layer.0.attention.output.dense.bias', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.intermediate.dense.weight', 'sci_embeddings.encoder.layer.0.intermediate.dense.bias', 'sci_embeddings.encoder.layer.0.output.dense.weight', 'sci_embeddings.encoder.layer.0.output.dense.bias', 'sci_embeddings.encoder.layer.0.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.attention.self.query.weight', 'sci_embeddings.encoder.layer.1.attention.self.query.bias', 'sci_embeddings.encoder.layer.1.attention.self.key.weight', 'sci_embeddings.encoder.layer.1.attention.self.key.bias', 'sci_embeddings.encoder.layer.1.attention.self.value.weight', 'sci_embeddings.encoder.layer.1.attention.self.value.bias', 'sci_embeddings.encoder.layer.1.attention.output.dense.weight', 'sci_embeddings.encoder.layer.1.attention.output.dense.bias', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.intermediate.dense.weight', 'sci_embeddings.encoder.layer.1.intermediate.dense.bias', 'sci_embeddings.encoder.layer.1.output.dense.weight', 'sci_embeddings.encoder.layer.1.output.dense.bias', 'sci_embeddings.encoder.layer.1.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.attention.self.query.weight', 'sci_embeddings.encoder.layer.2.attention.self.query.bias', 'sci_embeddings.encoder.layer.2.attention.self.key.weight', 'sci_embeddings.encoder.layer.2.attention.self.key.bias', 'sci_embeddings.encoder.layer.2.attention.self.value.weight', 'sci_embeddings.encoder.layer.2.attention.self.value.bias', 'sci_embeddings.encoder.layer.2.attention.output.dense.weight', 'sci_embeddings.encoder.layer.2.attention.output.dense.bias', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.intermediate.dense.weight', 'sci_embeddings.encoder.layer.2.intermediate.dense.bias', 'sci_embeddings.encoder.layer.2.output.dense.weight', 'sci_embeddings.encoder.layer.2.output.dense.bias', 'sci_embeddings.encoder.layer.2.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.attention.self.query.weight', 'sci_embeddings.encoder.layer.3.attention.self.query.bias', 'sci_embeddings.encoder.layer.3.attention.self.key.weight', 'sci_embeddings.encoder.layer.3.attention.self.key.bias', 'sci_embeddings.encoder.layer.3.attention.self.value.weight', 'sci_embeddings.encoder.layer.3.attention.self.value.bias', 'sci_embeddings.encoder.layer.3.attention.output.dense.weight', 'sci_embeddings.encoder.layer.3.attention.output.dense.bias', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.intermediate.dense.weight', 'sci_embeddings.encoder.layer.3.intermediate.dense.bias', 'sci_embeddings.encoder.layer.3.output.dense.weight', 'sci_embeddings.encoder.layer.3.output.dense.bias', 'sci_embeddings.encoder.layer.3.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.attention.self.query.weight', 'sci_embeddings.encoder.layer.4.attention.self.query.bias', 'sci_embeddings.encoder.layer.4.attention.self.key.weight', 'sci_embeddings.encoder.layer.4.attention.self.key.bias', 'sci_embeddings.encoder.layer.4.attention.self.value.weight', 'sci_embeddings.encoder.layer.4.attention.self.value.bias', 'sci_embeddings.encoder.layer.4.attention.output.dense.weight', 'sci_embeddings.encoder.layer.4.attention.output.dense.bias', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.intermediate.dense.weight', 'sci_embeddings.encoder.layer.4.intermediate.dense.bias', 'sci_embeddings.encoder.layer.4.output.dense.weight', 'sci_embeddings.encoder.layer.4.output.dense.bias', 'sci_embeddings.encoder.layer.4.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.attention.self.query.weight', 'sci_embeddings.encoder.layer.5.attention.self.query.bias', 'sci_embeddings.encoder.layer.5.attention.self.key.weight', 'sci_embeddings.encoder.layer.5.attention.self.key.bias', 'sci_embeddings.encoder.layer.5.attention.self.value.weight', 'sci_embeddings.encoder.layer.5.attention.self.value.bias', 'sci_embeddings.encoder.layer.5.attention.output.dense.weight', 'sci_embeddings.encoder.layer.5.attention.output.dense.bias', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.intermediate.dense.weight', 'sci_embeddings.encoder.layer.5.intermediate.dense.bias', 'sci_embeddings.encoder.layer.5.output.dense.weight', 'sci_embeddings.encoder.layer.5.output.dense.bias', 'sci_embeddings.encoder.layer.5.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.attention.self.query.weight', 'sci_embeddings.encoder.layer.6.attention.self.query.bias', 'sci_embeddings.encoder.layer.6.attention.self.key.weight', 'sci_embeddings.encoder.layer.6.attention.self.key.bias', 'sci_embeddings.encoder.layer.6.attention.self.value.weight', 'sci_embeddings.encoder.layer.6.attention.self.value.bias', 'sci_embeddings.encoder.layer.6.attention.output.dense.weight', 'sci_embeddings.encoder.layer.6.attention.output.dense.bias', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.intermediate.dense.weight', 'sci_embeddings.encoder.layer.6.intermediate.dense.bias', 'sci_embeddings.encoder.layer.6.output.dense.weight', 'sci_embeddings.encoder.layer.6.output.dense.bias', 'sci_embeddings.encoder.layer.6.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.attention.self.query.weight', 'sci_embeddings.encoder.layer.7.attention.self.query.bias', 'sci_embeddings.encoder.layer.7.attention.self.key.weight', 'sci_embeddings.encoder.layer.7.attention.self.key.bias', 'sci_embeddings.encoder.layer.7.attention.self.value.weight', 'sci_embeddings.encoder.layer.7.attention.self.value.bias', 'sci_embeddings.encoder.layer.7.attention.output.dense.weight', 'sci_embeddings.encoder.layer.7.attention.output.dense.bias', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.intermediate.dense.weight', 'sci_embeddings.encoder.layer.7.intermediate.dense.bias', 'sci_embeddings.encoder.layer.7.output.dense.weight', 'sci_embeddings.encoder.layer.7.output.dense.bias', 'sci_embeddings.encoder.layer.7.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.attention.self.query.weight', 'sci_embeddings.encoder.layer.8.attention.self.query.bias', 'sci_embeddings.encoder.layer.8.attention.self.key.weight', 'sci_embeddings.encoder.layer.8.attention.self.key.bias', 'sci_embeddings.encoder.layer.8.attention.self.value.weight', 'sci_embeddings.encoder.layer.8.attention.self.value.bias', 'sci_embeddings.encoder.layer.8.attention.output.dense.weight', 'sci_embeddings.encoder.layer.8.attention.output.dense.bias', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.intermediate.dense.weight', 'sci_embeddings.encoder.layer.8.intermediate.dense.bias', 'sci_embeddings.encoder.layer.8.output.dense.weight', 'sci_embeddings.encoder.layer.8.output.dense.bias', 'sci_embeddings.encoder.layer.8.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.attention.self.query.weight', 'sci_embeddings.encoder.layer.9.attention.self.query.bias', 'sci_embeddings.encoder.layer.9.attention.self.key.weight', 'sci_embeddings.encoder.layer.9.attention.self.key.bias', 'sci_embeddings.encoder.layer.9.attention.self.value.weight', 'sci_embeddings.encoder.layer.9.attention.self.value.bias', 'sci_embeddings.encoder.layer.9.attention.output.dense.weight', 'sci_embeddings.encoder.layer.9.attention.output.dense.bias', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.intermediate.dense.weight', 'sci_embeddings.encoder.layer.9.intermediate.dense.bias', 'sci_embeddings.encoder.layer.9.output.dense.weight', 'sci_embeddings.encoder.layer.9.output.dense.bias', 'sci_embeddings.encoder.layer.9.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.attention.self.query.weight', 'sci_embeddings.encoder.layer.10.attention.self.query.bias', 'sci_embeddings.encoder.layer.10.attention.self.key.weight', 'sci_embeddings.encoder.layer.10.attention.self.key.bias', 'sci_embeddings.encoder.layer.10.attention.self.value.weight', 'sci_embeddings.encoder.layer.10.attention.self.value.bias', 'sci_embeddings.encoder.layer.10.attention.output.dense.weight', 'sci_embeddings.encoder.layer.10.attention.output.dense.bias', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.intermediate.dense.weight', 'sci_embeddings.encoder.layer.10.intermediate.dense.bias', 'sci_embeddings.encoder.layer.10.output.dense.weight', 'sci_embeddings.encoder.layer.10.output.dense.bias', 'sci_embeddings.encoder.layer.10.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.attention.self.query.weight', 'sci_embeddings.encoder.layer.11.attention.self.query.bias', 'sci_embeddings.encoder.layer.11.attention.self.key.weight', 'sci_embeddings.encoder.layer.11.attention.self.key.bias', 'sci_embeddings.encoder.layer.11.attention.self.value.weight', 'sci_embeddings.encoder.layer.11.attention.self.value.bias', 'sci_embeddings.encoder.layer.11.attention.output.dense.weight', 'sci_embeddings.encoder.layer.11.attention.output.dense.bias', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.intermediate.dense.weight', 'sci_embeddings.encoder.layer.11.intermediate.dense.bias', 'sci_embeddings.encoder.layer.11.output.dense.weight', 'sci_embeddings.encoder.layer.11.output.dense.bias', 'sci_embeddings.encoder.layer.11.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.output.LayerNorm.bias', 'sci_embeddings.pooler.dense.weight', 'sci_embeddings.pooler.dense.bias', 'ff.weight', 'ff.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias'])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCFtuyBv3ADq","executionInfo":{"status":"ok","timestamp":1638831550219,"user_tz":360,"elapsed":209,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"40928121-bd3b-422d-e0c7-53907b33e69b"},"source":["ner_model = NerModel(BertEmbModel).to('cuda')\n","ner_model.load_state_dict(state_dict)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"ymiejDO5srYE"},"source":["# Obtain datasets' weights values (Do not run - fixed values)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GsRs55zcsxhk","executionInfo":{"status":"ok","timestamp":1638631909440,"user_tz":360,"elapsed":30431660,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"3e40179a-10be-42e6-bcf8-aa02fc919459"},"source":["zero = 0\n","one=0\n","two=0 \n","three=0\n","four=0\n","five=0\n","six=0\n","local_set = train\n","for i in range(len(local_set)):\n","  for j in range(len(local_set[i]['labels'])):\n","    if local_set[i]['labels'][j] == 0:\n","      zero += 1\n","    elif local_set[i]['labels'][j] == 1:\n","      one += 1\n","    elif local_set[i]['labels'][j] == 2:\n","      two += 1\n","    elif local_set[i]['labels'][j] == 3:\n","      three += 1\n","    elif local_set[i]['labels'][j] == 4:\n","      four += 1\n","    elif local_set[i]['labels'][j] == 5:\n","      five += 1\n","    elif local_set[i]['labels'][j] == 6:\n","      six += 1\n","    \n","print('0: ', zero, '1: ', one, '2: ', two, '3: ', three, '4: ', four, '5: ', five, '6: ', six)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:  48170 1:  4364 2:  1859 3:  1133 4:  0 5:  0 6:  0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzN52yRfsym4","executionInfo":{"status":"ok","timestamp":1638635605524,"user_tz":360,"elapsed":3696093,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"3569920b-8cc2-4634-e9a1-8dc024372263"},"source":["zero = 0\n","one=0\n","two=0 \n","three=0\n","four=0\n","five=0\n","six=0\n","local_set = test\n","for i in range(len(local_set)):\n","  for j in range(len(local_set[i]['labels'])):\n","    if local_set[i]['labels'][j] == 0:\n","      zero += 1\n","    elif local_set[i]['labels'][j] == 1:\n","      one += 1\n","    elif local_set[i]['labels'][j] == 2:\n","      two += 1\n","    elif local_set[i]['labels'][j] == 3:\n","      three += 1\n","    elif local_set[i]['labels'][j] == 4:\n","      four += 1\n","    elif local_set[i]['labels'][j] == 5:\n","      five += 1\n","    elif local_set[i]['labels'][j] == 6:\n","      six += 1\n","    \n","print('0: ', zero, '1: ', one, '2: ', two, '3: ', three, '4: ', four, '5: ', five, '6: ', six)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:  14719 1:  1337 2:  854 3:  386 4:  0 5:  0 6:  0\n"]}]},{"cell_type":"code","metadata":{"id":"eEKHdlDyy1T2"},"source":[""],"execution_count":null,"outputs":[]}]}