{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SciTDM+SciModel-Spans.ipynb","provenance":[{"file_id":"1apRBKJ_i2t4gsKy0TfRAM0Gih4gDkI15","timestamp":1634538734401}],"collapsed_sections":["LyMVIljFrioE","52t8l945rf1Y","-DBrafz0KqEq","oGYV8ipcJ5fU","7LthVVOhx_Sz","1Nh8Lcpvb_F4","kv6pGzbGpv2D","BBA4owXH8oS1","1DkDHptAhniL","dcdDIwq0kXy2","k0w9Lc0d3iOT","hC35X0v72kXB","ymiejDO5srYE"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e4fe39578a9c4197ae10b9838544d49e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9eb65347ebfb4b778c5ed5e308546633","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d38c4a7e890240d0b91b612f306b97ad","IPY_MODEL_fa4ee475df4445a9883454c112597766","IPY_MODEL_1584701f16a842b8bd217008dea42e25"]}},"9eb65347ebfb4b778c5ed5e308546633":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d38c4a7e890240d0b91b612f306b97ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d841314f2b99483a8d0713dae4f4f5e0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7ab91f6538584783bf12e5f551a8966c"}},"fa4ee475df4445a9883454c112597766":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_01a6db4f713f4cdfa20bc776b5271843","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":385,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":385,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e806370238a54c5f8bb6ecf430d6acdf"}},"1584701f16a842b8bd217008dea42e25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_07e8515ad1e84a3182817e0f629c3e1c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 385/385 [00:00&lt;00:00, 4.18kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1e4fcf23777a42d0b9aa9280bfb40364"}},"d841314f2b99483a8d0713dae4f4f5e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7ab91f6538584783bf12e5f551a8966c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01a6db4f713f4cdfa20bc776b5271843":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e806370238a54c5f8bb6ecf430d6acdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07e8515ad1e84a3182817e0f629c3e1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1e4fcf23777a42d0b9aa9280bfb40364":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6a2ee9f9e57c429cbd4da16d735048f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3975443269764fb4b1038766d3ab97c4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b4e7e29a699c461ea1794ebbc4ef433d","IPY_MODEL_dd7d0e0a11ae4ab68d7dac5e11d444c8","IPY_MODEL_eeb592ed1866480ba7480f92ace488b7"]}},"3975443269764fb4b1038766d3ab97c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4e7e29a699c461ea1794ebbc4ef433d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_eb7ee803a8804cca9c08092ed6fe5249","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6cff7a8475324c7fb6f984d0f1c2aad7"}},"dd7d0e0a11ae4ab68d7dac5e11d444c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_83cfc721c13b4cfba7f991992d32e5a1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":227845,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":227845,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3323b7a9fe884fe79f9b583157c68ce0"}},"eeb592ed1866480ba7480f92ace488b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aeb37b00c6b546c189c09b528dc4bfac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 223k/223k [00:00&lt;00:00, 508kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b0138a8f7d0f4960b18d498169bf5412"}},"eb7ee803a8804cca9c08092ed6fe5249":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6cff7a8475324c7fb6f984d0f1c2aad7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"83cfc721c13b4cfba7f991992d32e5a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3323b7a9fe884fe79f9b583157c68ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aeb37b00c6b546c189c09b528dc4bfac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b0138a8f7d0f4960b18d498169bf5412":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d60f42ce1a3442bbdf161bb0e82669c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dcc525a7e3f6430687c1f54f3c0426b1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c38b884c87f348c49030acd36fa36318","IPY_MODEL_e339471c5c4c4bc2b39634332b690da3","IPY_MODEL_21ac09747b144e489cc221496cdbf7df"]}},"dcc525a7e3f6430687c1f54f3c0426b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c38b884c87f348c49030acd36fa36318":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7a10e868decc4fb99fe04f483b5c5031","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d73fbcd432f544de8a1380320bfe6726"}},"e339471c5c4c4bc2b39634332b690da3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e9fd7df886a74b55b3f15222f212eb3e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":442221694,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442221694,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_954b121b96c048da8e06fabd2521f7dc"}},"21ac09747b144e489cc221496cdbf7df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f4974f69f1c24f83bf2ddbe5e3d6356c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 422M/422M [00:14&lt;00:00, 55.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1731044138bf47c8b3a76aa2edaa331d"}},"7a10e868decc4fb99fe04f483b5c5031":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d73fbcd432f544de8a1380320bfe6726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e9fd7df886a74b55b3f15222f212eb3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"954b121b96c048da8e06fabd2521f7dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f4974f69f1c24f83bf2ddbe5e3d6356c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1731044138bf47c8b3a76aa2edaa331d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQ6H5sYFfFCM","executionInfo":{"status":"ok","timestamp":1638577821479,"user_tz":360,"elapsed":242,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"1c6f9353-196c-43fa-a996-765dca75925e"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data\n"]}]},{"cell_type":"markdown","metadata":{"id":"LyMVIljFrioE"},"source":["#External Databases"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeeP8a6fKXOX","executionInfo":{"status":"ok","timestamp":1646982143188,"user_tz":360,"elapsed":38759,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"9ba30e89-3eae-409d-b878-93edd8f1554d"},"source":["! pip install git+https://github.com/huggingface/transformers.git"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-x_9zp_z7\n","  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-x_9zp_z7\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 9.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 45.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.9 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 66.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3835851 sha256=cf8f9b2c73ff8a9aa154194feb583c59d65b9c36cb3a4b9b870108ea3e45f07f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-njbw699c/wheels/90/a5/44/6bcd83827c8a60628c5ad602f429cd5076bcce5f2a90054947\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.18.0.dev0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_L4bHxBGsZk_","executionInfo":{"status":"ok","timestamp":1646982167818,"user_tz":360,"elapsed":24640,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"329237da-62da-4a87-d4f7-77f094086fae"},"source":["! pip install ray[tune]"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ray[tune]\n","  Downloading ray-1.11.0-cp37-cp37m-manylinux2014_x86_64.whl (52.7 MB)\n","\u001b[K     |████████████████████████████████| 52.7 MB 62 kB/s \n","\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (6.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (21.4.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.17.3)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.6.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.3.3)\n","Collecting grpcio<=1.43.0,>=1.28.1\n","  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 52.7 MB/s \n","\u001b[?25hCollecting redis>=3.5.0\n","  Downloading redis-4.1.4-py3-none-any.whl (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 7.1 MB/s \n","\u001b[?25hCollecting tensorboardX>=1.9\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 55.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.5)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.9)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.15.0)\n","Collecting deprecated>=1.2.3\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (4.11.2)\n","Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (21.3)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[tune]) (1.13.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis>=3.5.0->ray[tune]) (3.0.7)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (5.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (0.18.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2021.10.8)\n","Installing collected packages: deprecated, redis, grpcio, tensorboardX, ray\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.44.0\n","    Uninstalling grpcio-1.44.0:\n","      Successfully uninstalled grpcio-1.44.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n","Successfully installed deprecated-1.2.13 grpcio-1.43.0 ray-1.11.0 redis-4.1.4 tensorboardX-2.5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UI0tseYdav7D","executionInfo":{"status":"ok","timestamp":1646982182792,"user_tz":360,"elapsed":14981,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"3006595f-d2e2-4899-829d-df3dee8516f9"},"source":["!pip install datasets"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n","\u001b[K     |████████████████████████████████| 312 kB 8.1 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n","\u001b[K     |████████████████████████████████| 134 kB 67.4 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 52.7 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 52.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 71.3 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 42.3 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 50.8 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.4 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install seqeval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h-TchuTgDbbJ","executionInfo":{"status":"ok","timestamp":1646982192763,"user_tz":360,"elapsed":9979,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"2a135c4c-4233-4524-9821-c38b1613c6bc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=6b9874e5ed549c9949d3c1476544b5a01defc21d2779cca015b1100b6efb7dc4\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"52t8l945rf1Y"},"source":["#Libraries"]},{"cell_type":"code","metadata":{"id":"qSbdGMYCgquq","colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["e4fe39578a9c4197ae10b9838544d49e","9eb65347ebfb4b778c5ed5e308546633","d38c4a7e890240d0b91b612f306b97ad","fa4ee475df4445a9883454c112597766","1584701f16a842b8bd217008dea42e25","d841314f2b99483a8d0713dae4f4f5e0","7ab91f6538584783bf12e5f551a8966c","01a6db4f713f4cdfa20bc776b5271843","e806370238a54c5f8bb6ecf430d6acdf","07e8515ad1e84a3182817e0f629c3e1c","1e4fcf23777a42d0b9aa9280bfb40364","6a2ee9f9e57c429cbd4da16d735048f9","3975443269764fb4b1038766d3ab97c4","b4e7e29a699c461ea1794ebbc4ef433d","dd7d0e0a11ae4ab68d7dac5e11d444c8","eeb592ed1866480ba7480f92ace488b7","eb7ee803a8804cca9c08092ed6fe5249","6cff7a8475324c7fb6f984d0f1c2aad7","83cfc721c13b4cfba7f991992d32e5a1","3323b7a9fe884fe79f9b583157c68ce0","aeb37b00c6b546c189c09b528dc4bfac","b0138a8f7d0f4960b18d498169bf5412","7d60f42ce1a3442bbdf161bb0e82669c","dcc525a7e3f6430687c1f54f3c0426b1","c38b884c87f348c49030acd36fa36318","e339471c5c4c4bc2b39634332b690da3","21ac09747b144e489cc221496cdbf7df","7a10e868decc4fb99fe04f483b5c5031","d73fbcd432f544de8a1380320bfe6726","e9fd7df886a74b55b3f15222f212eb3e","954b121b96c048da8e06fabd2521f7dc","f4974f69f1c24f83bf2ddbe5e3d6356c","1731044138bf47c8b3a76aa2edaa331d"]},"executionInfo":{"status":"ok","timestamp":1646982226445,"user_tz":360,"elapsed":33694,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"3b59f0cf-e1b7-4ff1-d40a-ada388f34c96"},"source":["from google.colab import files\n","\n","import os\n","import re\n","import json\n","import string\n","\n","import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","#import tensorflow as tf\n","#from tensorflow import keras\n","#from tensorflow.keras import layers\n","\n","from datasets import load_metric\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from seqeval.metrics import classification_report\n","from sklearn.metrics import precision_recall_fscore_support as prfs\n","\n","import tensorflow_hub as hub\n","from keras import backend as K\n","\n","import transformers\n","print(transformers.__version__)\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers.trainer_utils import EvalLoopOutput\n","\n","from transformers import AutoTokenizer, AutoModel\n","BertTokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n","BertEmbModel = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["4.18.0.dev0\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4fe39578a9c4197ae10b9838544d49e","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a2ee9f9e57c429cbd4da16d735048f9","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/223k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d60f42ce1a3442bbdf161bb0e82669c","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/422M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","metadata":{"id":"-DBrafz0KqEq"},"source":["#Loading Dataset"]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":0},"id":"ICN0rgXve6o1","executionInfo":{"status":"ok","timestamp":1646982270109,"user_tz":360,"elapsed":43676,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"35fbfc2e-b740-47ef-f7ac-bd8ee190ceae"},"source":["uploaded = files.upload()"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-1fbbfa7b-4b55-425b-b990-2db81925ac01\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1fbbfa7b-4b55-425b-b990-2db81925ac01\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving test_500_v2.conll to test_500_v2.conll\n","Saving train_1500_v2.conll to train_1500_v2.conll\n"]}]},{"cell_type":"code","metadata":{"id":"Yf9yoQx4D7r9","executionInfo":{"status":"ok","timestamp":1646982270110,"user_tz":360,"elapsed":6,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["class TDMSciDataset(torch.utils.data.Dataset):\n","  def __init__(self, raw_x, raw_y, max_length=250):\n","    self.raw_x = raw_x\n","    self.raw_y = raw_y\n","    \n","    self.max_length = max_length\n","\n","  def tokenize_and_preserve_labels(self, sentence, text_labels):\n","    \"\"\"\n","    The tokenizer can split single words into multiple tokens - this breaks\n","    the labels, so we need to keep track of this!\n","    \"\"\"\n","    tokenized_sentence = []\n","    labels = []\n","\n","    for word, label in zip(sentence, text_labels):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = BertTokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels\n","\n","  def __getitem__(self, idx):\n","    tokens = self.raw_x[idx]\n","    labels = np.zeros((len(tokens)))\n","    for i, label in enumerate(self.raw_y[idx]):\n","      labels[i] = label\n","\n","    # This could be moved to __init__ to save time?\n","    tokens, labels = self.tokenize_and_preserve_labels(tokens, labels)\n","\n","    # Convert each token to an id number \n","    input_ids = BertTokenizer.convert_tokens_to_ids(tokens)\n","    \n","    # add and adjust for special tokens\n","    input_ids = [BertTokenizer.cls_token_id] + input_ids + [BertTokenizer.sep_token_id]  \n","    labels = [0] + labels + [0]\n","\n","    # Pad inputs\n","    input_ids = torch.tensor(np.pad(input_ids, [0, self.max_length-len(input_ids)]))\n","    labels = torch.tensor(np.pad(labels, [0, self.max_length-len(labels)], constant_values=-100))\n","\n","    attention_mask = torch.tensor([int(i != 0) for i in input_ids])\n","\n","    #return {input_ids, labels}\n","    #return {'input_ids': [input_ids], 'labels': labels}\n","    return {'input_ids': [input_ids], 'attention_mask': [attention_mask], 'labels': labels}\n","\n","  def __len__(self):\n","    return len(self.raw_y)\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYy-iTnlEU8K","executionInfo":{"status":"ok","timestamp":1646982270110,"user_tz":360,"elapsed":4,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["# Entity maps\n","def Entity2ID(Sc_Entity):\n","  return {\n","        'TASK': 1,\n","        'DATASET': 2,\n","        'METRIC': 3,\n","        'None': 0,\n","        'O': 0,\n","    }[Sc_Entity]\n","\n","def ID2Entity(Sc_Entity):\n","  return {\n","        1: 'TASK',\n","        2: 'DATASET',\n","        3: 'METRIC',\n","        0: 'None',\n","    }[Sc_Entity]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NxRNljBqETC0","executionInfo":{"status":"ok","timestamp":1646982270302,"user_tz":360,"elapsed":196,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"b52f79ed-a369-4ebb-aeca-f1d4d3cf018b"},"source":["# Load raw data train\n","raw_x = []\n","raw_y = []\n","\n","cur_x = []\n","cur_y = []\n","with open('train_1500_v2.conll', 'r') as f:\n","  for line in f:\n","    if len(line.strip()) <= 3:\n","      raw_x.append(cur_x)\n","      raw_y.append(cur_y)\n","      cur_x = []\n","      cur_y = []\n","    else:\n","      line = line.strip().split('\\t')\n","      cur_x.append(line[0])\n","      cur_y.append(Entity2ID(line[-1].split('-')[-1]))\n","\n","print(raw_x[10])\n","print(raw_y[10])\n","\n","# Create actual dataset\n","train = TDMSciDataset(raw_x, raw_y)\n","\n","#print(train[0])"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['The', 'results', 'show', 'that', 'proposed', 'shallow', 'representations', 'of', 'sentence', 'structure', 'are', 'robust', 'to', 'reductions', 'in', 'parsing', 'accuracy', ',', 'and', 'that', 'the', 'contribution', 'of', 'alternative', 'representations', 'of', 'sentence', 'structure', 'to', 'successful', 'semantic', 'role', 'labeling', 'varies', 'with', 'the', 'integrity', 'of', 'the', 'parsing', 'and', 'argument', '-', 'identification', 'stages', '.', '.']\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W--BOb0bEaUH","executionInfo":{"status":"ok","timestamp":1646982270303,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"9ad6bd79-6e70-40b7-a543-998f296ddaf7"},"source":["# Load raw data test\n","raw_x = []\n","raw_y = []\n","\n","cur_x = []\n","cur_y = []\n","with open('test_500_v2.conll', 'r') as f:\n","  for line in f:\n","    if len(line.strip()) <= 3:\n","      raw_x.append(cur_x)\n","      raw_y.append(cur_y)\n","      cur_x = []\n","      cur_y = []\n","    else:\n","      line = line.strip().split('\\t')\n","      cur_x.append(line[0])\n","      cur_y.append(Entity2ID(line[-1].split('-')[-1]))\n","\n","print(raw_x[10])\n","print(raw_y[10])\n","\n","# Create actual dataset\n","test = TDMSciDataset(raw_x, raw_y)\n","\n","#print(test[0])"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["['We', 'evaluate', 'our', 'approach', 'on', 'three', 'datasets', ':', 'TriviaQA', 'unfiltered', '(', ',', 'a', 'dataset', 'of', 'questions', 'from', 'trivia', 'databases', 'paired', 'with', 'documents', 'found', 'by', 'completing', 'a', 'web', 'search', 'of', 'the', 'questions', ';', 'TriviaQA', 'web', ',', 'a', 'dataset', 'derived', 'from', 'TriviaQA', 'unfiltered', 'by', 'treating', 'each', 'question', 'document', 'pair', 'where', 'the', 'document', 'contains', 'the', 'question', 'answer', 'as', 'an', 'individual', 'training', 'point', ';', 'and', 'SQuAD', '(', ',', 'a', 'collection', 'of', 'Wikipedia', 'articles', 'and', 'crowdsourced', 'questions', '.']\n","[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iH9xrFbXFG0g","executionInfo":{"status":"ok","timestamp":1646982270303,"user_tz":360,"elapsed":4,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"a5623eee-230c-4a95-fb65-67229957a960"},"source":["len(test), len(train)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(486, 1522)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"3xW0jveILkHM","executionInfo":{"status":"ok","timestamp":1646982270303,"user_tz":360,"elapsed":4,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["#train[1]"],"execution_count":12,"outputs":[]},{"cell_type":"code","source":["local_hp_train = []\n","local_hp_val = []\n","for x in range(500):\n","  local_hp_train.append(train[x])\n","\n","for x in range(180):\n","  local_hp_val.append(test[x])\n","\n","#print(len(local_hp_train))\n","#print(local_hp_train[1])"],"metadata":{"id":"06Jf8634_5Bz","executionInfo":{"status":"ok","timestamp":1646982280718,"user_tz":360,"elapsed":10419,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGYV8ipcJ5fU"},"source":["# Pytorch Model Definition"]},{"cell_type":"code","metadata":{"id":"eFp2ZFd25KwK","executionInfo":{"status":"ok","timestamp":1646982280718,"user_tz":360,"elapsed":14,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["class NerModel(nn.Module):\n","  def __init__(self, b_embeddings, emb_dims=768, ff_dims=14, out_dims=4):\n","    super(NerModel, self).__init__()\n","    self.sci_embeddings = b_embeddings\n","    self.embd_dropout = nn.Dropout(0.1)\n","    self.ff_dropout = nn.Dropout(0.1)\n","    self.ff = nn.Linear(emb_dims, ff_dims)\n","    self.tanh = nn.Tanh()\n","    self.lstm = nn.LSTM(768, 100, 1, bidirectional=True)\n","    self.lstm_drop = nn.Dropout(0.4)\n","    self.ff = nn.Linear(200, 14)\n","    self.ff_act = nn.ReLU()\n","    self.classifier = nn.Linear(ff_dims, out_dims)\n","  def forward(self, **inputs):\n","    embds = self.sci_embeddings(**inputs)['last_hidden_state']\n","    out = self.embd_dropout(embds)\n","    out, _ = self.lstm(out)\n","    out = self.tanh(out)\n","    out = self.lstm_drop(out)\n","    out = self.ff(out)\n","    out = self.ff_act(out)\n","    out = self.ff_dropout(out)\n","    out = self.classifier(out)\n","    return out\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LthVVOhx_Sz"},"source":["# Metrics and Configs"]},{"cell_type":"code","metadata":{"id":"VZJjqYf2AKA8","executionInfo":{"status":"ok","timestamp":1646982280719,"user_tz":360,"elapsed":14,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["def reset_weights(m):\n","  '''\n","    Try resetting model weights to avoid\n","    weight leakage.\n","  '''\n","  for layer in m.children():\n","   if hasattr(layer, 'reset_parameters'):\n","    #print(f'Reset trainable parameters of layer = {layer}')\n","    layer.reset_parameters()\n","\n","import math\n","\n","def load_param():\n","  for n, v in best_run.hyperparameters.items():\n","    if n == 'seed':\n","      setattr(trainer.args, n, math.ceil(v))\n","      print(n, math.ceil(v))\n","    else:\n","      setattr(trainer.args, n, v)\n","      print(n, v)   "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjMXqFluyLCW","executionInfo":{"status":"ok","timestamp":1646982280719,"user_tz":360,"elapsed":13,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["batch_size = 4\n","training_args = TrainingArguments(\n","    \"trained_scibert_ner_model\", # output dir\n","    learning_rate=1e-5, \n","    num_train_epochs=10, \n","    dataloader_drop_last=True,\n","    per_device_eval_batch_size=batch_size, \n","    per_device_train_batch_size=batch_size,\n","    save_steps=len(train) // batch_size,\n","    eval_steps=len(train) // batch_size,\n","    lr_scheduler_type='cosine',\n","    evaluation_strategy='steps'\n","    )\n","#print(training_args)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"fabdHzMRybBR","executionInfo":{"status":"ok","timestamp":1646982280719,"user_tz":360,"elapsed":13,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["def collator(batch):\n","  out =  {\n","      'input_ids': torch.stack([(x['input_ids'][0]) for x in batch]),\n","      'attention_mask': torch.stack([x['attention_mask'][0] for x in batch]),\n","      'labels': torch.stack([x['labels'].clone().detach() for x in batch])\n","  }\n","  return out"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"dZluJPsy2eb5","executionInfo":{"status":"ok","timestamp":1646982280899,"user_tz":360,"elapsed":193,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["import torch.nn.functional as F\n","\n","class FocalLoss(nn.modules.loss._WeightedLoss):\n","    def __init__(self, weight, gamma, reduction='mean'):\n","        super(FocalLoss, self).__init__(weight,reduction=reduction)\n","        self.gamma = gamma\n","        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n","\n","    def forward(self, input, target):\n","        ce_loss = F.cross_entropy(input, target, ignore_index=-100, reduction=self.reduction, weight=self.weight)\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n","        return focal_loss"],"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"QLbpwNckLvV6","executionInfo":{"status":"ok","timestamp":1646982280900,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"outputs":[],"source":["class MultilabelTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False, num_labels=4):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs\n","\n","        weights = torch.tensor([0.85, 1.55, 1.65, 1.85]).cuda()  # The no-class label has too many examples, we need to weight the loss - this probably needs further tuning \n","        gamma=5\n","        loss_fct = FocalLoss(weight=weights, gamma=gamma)\n","        loss = loss_fct(logits.view(-1, num_labels), labels.long().view(-1))\n","        return (loss, outputs) if return_outputs else loss\n","\n","    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, ignore_keys=None, metric_key_prefix=\"eval\", num_labels=4):\n","      args = self.args\n","      prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n","\n","      self.model.eval()\n","\n","      all_losses = []\n","      all_preds = []\n","      all_labels = []\n","      for step, sample in enumerate(dataloader):\n","        for i in range(0, len(sample['labels'])):\n","          inputs = {}\n","          inputs['input_ids'] = torch.stack([sample['input_ids'][i].cuda()])\n","          inputs['attention_mask'] = torch.stack([sample['attention_mask'][i].cuda()])\n","          inputs['labels'] = torch.stack([sample['labels'][i].cuda()])\n","          labels = inputs['labels'][0].cpu().numpy()\n","          \n","          (loss, logits) = self.compute_loss(self.model, inputs, return_outputs=True)\n","          logits = logits[0].cpu().detach().numpy()\n","          preds = np.argmax(nn.Softmax(dim=-1)(torch.tensor(logits)).numpy(), axis=-1)\n","\n","          all_losses = np.concatenate((all_losses, [loss.detach().cpu().numpy()]), axis=0)\n","\n","          preds = preds[labels != -100]\n","          labels = labels[labels != -100]\n","          all_preds = np.concatenate((all_preds, preds))\n","          all_labels = np.concatenate((all_labels, labels))\n","\n","      metrics = {}\n","      metrics['macro_f1'] = f1_score(all_labels, all_preds, average='macro')\n","      metrics['macro_precision'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n","      metrics['macro_recall'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n","      metrics['micro_f1'] = f1_score(all_labels, all_preds, average='micro')\n","      metrics['micro_precision'] = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n","      metrics['micro_recall'] = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n","\n","      metrics['macro_f1_no_o'] = f1_score(all_labels, all_preds, average='macro', labels=[1, 2, 3])\n","      metrics['macro_precision_no_o'] = precision_score(all_labels, all_preds, average='macro', labels=[1, 2, 3], zero_division=0)\n","      metrics['macro_recall_no_o'] = recall_score(all_labels, all_preds, average='macro', labels=[1, 2, 3], zero_division=0)\n","      metrics['micro_f1_no_o'] = f1_score(all_labels, all_preds, average='micro', labels=[1, 2, 3])\n","      metrics['micro_precision_no_o'] = precision_score(all_labels, all_preds, average='micro', labels=[1, 2, 3], zero_division=0)\n","      metrics['micro_recall_no_o'] = recall_score(all_labels, all_preds, average='micro', labels=[1, 2, 3], zero_division=0)\n","\n","      for key in list(metrics.keys()):\n","        if not key.startswith(metric_key_prefix):\n","          metrics[metric_key_prefix + '_' + key] = metrics.pop(key)\n","      \n","      metrics[metric_key_prefix + '_loss'] = all_losses.mean().item()\n","\n","      return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=len(dataloader))"]},{"cell_type":"code","metadata":{"id":"infNapI_OmFr","executionInfo":{"status":"ok","timestamp":1646982280901,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["def my_hp_space_ray(trial):\n","    from ray import tune\n","\n","    return {\n","        \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n","        \"num_train_epochs\": tune.choice(range(8, 20)),\n","        \"weight_decay\": tune.uniform(0.0, 0.5),\n","        \"per_device_train_batch_size\": tune.choice([4, 8, 16]),  #<16 definetly not working\n","    }"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Nh8Lcpvb_F4"},"source":["# Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"id":"w6AqbEPyPK7O"},"source":["def model_init():\n","    x = NerModel(BertEmbModel)\n","    x.sci_embeddings.requires_grad = False\n","    return x\n","\n","trainer = MultilabelTrainer(model_init=model_init,\n","                            args=training_args,\n","                            train_dataset=local_hp_train,\n","                            eval_dataset=local_hp_val,\n","                            data_collator=collator  # defines how to merge data into batches, using the collator function above\n","                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGyHLY640NRz","colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1yVL6e4_xZg47FGke4cETp8jQ1hFwy4Wv"},"executionInfo":{"status":"ok","timestamp":1645127711460,"user_tz":360,"elapsed":3009533,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"31108eec-27cd-4804-adfa-119a5a52dff9"},"source":["from ray.tune.suggest.hyperopt import HyperOptSearch\n","from ray.tune.schedulers import ASHAScheduler\n","\n","best_run = trainer.hyperparameter_search(backend=\"ray\", \n","                                         resources_per_trial={\"gpu\": 1, \"cpu\": 0},\n","                                         n_trials=16, \n","                                         direction=\"maximize\", \n","                                         hp_space=my_hp_space_ray,\n","                                         search_alg=HyperOptSearch(metric='eval_micro_recall_no_o', mode=\"max\"),  #'eval_*f1_micro'\n","                                         scheduler=ASHAScheduler(metric='eval_micro_recall_no_o', mode=\"max\")\n","                                         )"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zH0H8agAbzZi","executionInfo":{"status":"ok","timestamp":1645127711460,"user_tz":360,"elapsed":12,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"0c627f34-8607-4610-c625-3001e637a065"},"source":["best_run"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='63b4738c', objective=9.156609578992219, hyperparameters={'learning_rate': 7.648631456296879e-06, 'num_train_epochs': 19, 'weight_decay': 0.44441579284898636, 'per_device_train_batch_size': 8})"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"kv6pGzbGpv2D"},"source":["# Cossvalidation"]},{"cell_type":"code","metadata":{"id":"LhipElJSzRUF"},"source":["#import tensorflow as tf\n","from torch.utils.data import DataLoader, ConcatDataset\n","from sklearn.model_selection import KFold\n","from torch import nn\n","from transformers import Trainer\n","\n","#print('GPU detected:', tf.config.list_physical_devices('GPU'))\n","k_folds = 5\n","kfold = KFold(n_splits=k_folds, shuffle=True)\n","results = np.zeros(5)\n","resultss = np.zeros(5)\n","dataset = ConcatDataset([train, test])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zQyLOxyMjZq2","executionInfo":{"status":"ok","timestamp":1646797233555,"user_tz":360,"elapsed":4939043,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"6fbfb2bf-4b30-4b84-f683-2cb71bbaf1c6"},"source":["for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n","    \n","    # Print\n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","    \n","    # Sample elements randomly from a given list of ids, no replacement.\n","    #train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n","    #test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n","    \n","    local_train = []\n","    #i = 0\n","    for idx in train_ids:\n","      #if i <= 1920:\n","        local_train.append(dataset[idx])\n","      #i += 1\n","    \n","    local_test = []\n","    #i = 0\n","    for idx in test_ids:\n","      #if i <= 480:\n","        local_test.append(dataset[idx])\n","      #i += 1\n","\n","    training_args = TrainingArguments(\"trained_scibert_ner_model\", # output dir\n","                                      learning_rate=7.648631456296879e-06, \n","                                      weight_decay=0.4444157928489863,\n","                                      num_train_epochs=19, \n","                                      dataloader_drop_last=True,\n","                                      per_device_eval_batch_size=8, \n","                                      per_device_train_batch_size=8,\n","                                      logging_steps=50,\n","                                      save_steps=len(local_train) // batch_size,\n","                                      lr_scheduler_type='cosine',\n","                                      evaluation_strategy='steps',\n","                                      eval_steps=len(local_train) // batch_size\n","                                      )\n","    #learning_rate 7.648631456296879e-06\n","    #num_train_epochs 19\n","    #weight_decay 0.44441579284898636\n","    #per_device_train_batch_size 8\n","\n","    # Init the neural network\n","    ner_model = NerModel(BertEmbModel).to('cuda')  # make sure we move the model to the GPU for training\n","    \n","    trainer = MultilabelTrainer(model=ner_model,\n","                                args=training_args,\n","                                train_dataset=local_train,\n","                                eval_dataset=local_test,\n","                                data_collator=collator  # defines how to merge data into batches, using the collator function above\n","                                )\n","\n","    #Loading Best parameters\n","    #load_param()\n","\n","    #Training\n","    trainer.train()\n","          \n","    # Process is complete.\n","    print('Training process has finished. Saving trained model.')\n","\n","    # Print about testing\n","    print('Starting testing')\n","    \n","    # Saving the model\n","    save_path = f'./model-fold-{fold}.pth'\n","    torch.save(ner_model.state_dict(), save_path)\n","\n","    # Evaluationfor this fold\n","    #correct, total = 0, 0\n","    with torch.no_grad():\n","      result = trainer.evaluate(local_test)\n","      print(result)\n","\n","      # Print accuracy\n","      print('Accuracy for fold ', fold, ': ', result['eval_micro_f1_no_o'])\n","      print('--------------------------------')\n","      results[fold] = result['eval_micro_f1_no_o']\n","      resultss[fold] = result['eval_micro_f1']\n","      del result\n","    \n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FOLD 0\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1606\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3800\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3800' max='3800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3800/3800 16:01, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.009700</td>\n","      <td>0.038744</td>\n","      <td>0.845879</td>\n","      <td>0.784856</td>\n","      <td>0.933405</td>\n","      <td>0.942737</td>\n","      <td>0.942737</td>\n","      <td>0.942737</td>\n","      <td>0.805170</td>\n","      <td>0.715480</td>\n","      <td>0.929788</td>\n","      <td>0.812942</td>\n","      <td>0.719924</td>\n","      <td>0.933563</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.004900</td>\n","      <td>0.021895</td>\n","      <td>0.895151</td>\n","      <td>0.854653</td>\n","      <td>0.943002</td>\n","      <td>0.964804</td>\n","      <td>0.964804</td>\n","      <td>0.964804</td>\n","      <td>0.866527</td>\n","      <td>0.808648</td>\n","      <td>0.934121</td>\n","      <td>0.874224</td>\n","      <td>0.820457</td>\n","      <td>0.935531</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.002700</td>\n","      <td>0.022043</td>\n","      <td>0.884135</td>\n","      <td>0.845315</td>\n","      <td>0.932203</td>\n","      <td>0.959148</td>\n","      <td>0.959148</td>\n","      <td>0.959148</td>\n","      <td>0.852921</td>\n","      <td>0.796762</td>\n","      <td>0.921296</td>\n","      <td>0.856166</td>\n","      <td>0.797452</td>\n","      <td>0.924213</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.002400</td>\n","      <td>0.021728</td>\n","      <td>0.892430</td>\n","      <td>0.883134</td>\n","      <td>0.904459</td>\n","      <td>0.967388</td>\n","      <td>0.967388</td>\n","      <td>0.967388</td>\n","      <td>0.862054</td>\n","      <td>0.849606</td>\n","      <td>0.878145</td>\n","      <td>0.869715</td>\n","      <td>0.868861</td>\n","      <td>0.870571</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.001300</td>\n","      <td>0.018123</td>\n","      <td>0.893827</td>\n","      <td>0.864173</td>\n","      <td>0.928615</td>\n","      <td>0.965503</td>\n","      <td>0.965503</td>\n","      <td>0.965503</td>\n","      <td>0.864573</td>\n","      <td>0.822452</td>\n","      <td>0.913500</td>\n","      <td>0.873325</td>\n","      <td>0.835807</td>\n","      <td>0.914370</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.001600</td>\n","      <td>0.018125</td>\n","      <td>0.893173</td>\n","      <td>0.866766</td>\n","      <td>0.923543</td>\n","      <td>0.963757</td>\n","      <td>0.963757</td>\n","      <td>0.963757</td>\n","      <td>0.864011</td>\n","      <td>0.825808</td>\n","      <td>0.907444</td>\n","      <td>0.867678</td>\n","      <td>0.825122</td>\n","      <td>0.914862</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.001700</td>\n","      <td>0.020007</td>\n","      <td>0.890761</td>\n","      <td>0.857275</td>\n","      <td>0.930664</td>\n","      <td>0.960335</td>\n","      <td>0.960335</td>\n","      <td>0.960335</td>\n","      <td>0.861492</td>\n","      <td>0.812539</td>\n","      <td>0.918892</td>\n","      <td>0.859361</td>\n","      <td>0.801533</td>\n","      <td>0.926181</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.001100</td>\n","      <td>0.017081</td>\n","      <td>0.896352</td>\n","      <td>0.871960</td>\n","      <td>0.924358</td>\n","      <td>0.965573</td>\n","      <td>0.965573</td>\n","      <td>0.965573</td>\n","      <td>0.867885</td>\n","      <td>0.832779</td>\n","      <td>0.907771</td>\n","      <td>0.872855</td>\n","      <td>0.835358</td>\n","      <td>0.913878</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.001100</td>\n","      <td>0.016901</td>\n","      <td>0.900061</td>\n","      <td>0.878667</td>\n","      <td>0.924373</td>\n","      <td>0.966550</td>\n","      <td>0.966550</td>\n","      <td>0.966550</td>\n","      <td>0.872636</td>\n","      <td>0.841718</td>\n","      <td>0.907411</td>\n","      <td>0.875737</td>\n","      <td>0.840652</td>\n","      <td>0.913878</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.8995894858631727, 'eval_macro_precision': 0.8778613590866896, 'eval_macro_recall': 0.9243327436723349, 'eval_micro_f1': 0.9664106145251397, 'eval_micro_precision': 0.9664106145251397, 'eval_micro_recall': 0.9664106145251397, 'eval_macro_f1_no_o': 0.8720353424111611, 'eval_macro_precision_no_o': 0.840644501024662, 'eval_macro_recall_no_o': 0.9074111061464465, 'eval_micro_f1_no_o': 0.8753240631628564, 'eval_micro_precision_no_o': 0.8398914518317503, 'eval_micro_recall_no_o': 0.9138779527559056, 'eval_loss': 0.016934644687280523, 'eval_runtime': 4.9843, 'eval_samples_per_second': 10.031, 'eval_steps_per_second': 1.404, 'epoch': 19.0}\n","Accuracy for fold  0 :  0.8753240631628564\n","--------------------------------\n","FOLD 1\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1606\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3800\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3800' max='3800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3800/3800 16:01, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.010400</td>\n","      <td>0.012499</td>\n","      <td>0.976436</td>\n","      <td>0.964471</td>\n","      <td>0.988910</td>\n","      <td>0.991931</td>\n","      <td>0.991931</td>\n","      <td>0.991931</td>\n","      <td>0.970067</td>\n","      <td>0.953158</td>\n","      <td>0.987650</td>\n","      <td>0.971549</td>\n","      <td>0.956039</td>\n","      <td>0.987572</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.004200</td>\n","      <td>0.007497</td>\n","      <td>0.971135</td>\n","      <td>0.955485</td>\n","      <td>0.987647</td>\n","      <td>0.990303</td>\n","      <td>0.990303</td>\n","      <td>0.990303</td>\n","      <td>0.963265</td>\n","      <td>0.941177</td>\n","      <td>0.986493</td>\n","      <td>0.965356</td>\n","      <td>0.945872</td>\n","      <td>0.985660</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.003200</td>\n","      <td>0.005228</td>\n","      <td>0.973421</td>\n","      <td>0.960208</td>\n","      <td>0.987245</td>\n","      <td>0.990940</td>\n","      <td>0.990940</td>\n","      <td>0.990940</td>\n","      <td>0.966187</td>\n","      <td>0.947557</td>\n","      <td>0.985625</td>\n","      <td>0.967348</td>\n","      <td>0.951039</td>\n","      <td>0.984226</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.002000</td>\n","      <td>0.004037</td>\n","      <td>0.975116</td>\n","      <td>0.963054</td>\n","      <td>0.987691</td>\n","      <td>0.991789</td>\n","      <td>0.991789</td>\n","      <td>0.991789</td>\n","      <td>0.968251</td>\n","      <td>0.951268</td>\n","      <td>0.985914</td>\n","      <td>0.969640</td>\n","      <td>0.955030</td>\n","      <td>0.984704</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.001500</td>\n","      <td>0.003477</td>\n","      <td>0.976415</td>\n","      <td>0.965876</td>\n","      <td>0.987340</td>\n","      <td>0.991931</td>\n","      <td>0.991931</td>\n","      <td>0.991931</td>\n","      <td>0.969983</td>\n","      <td>0.955142</td>\n","      <td>0.985335</td>\n","      <td>0.970526</td>\n","      <td>0.957655</td>\n","      <td>0.983748</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.001600</td>\n","      <td>0.003121</td>\n","      <td>0.975748</td>\n","      <td>0.964262</td>\n","      <td>0.987691</td>\n","      <td>0.991789</td>\n","      <td>0.991789</td>\n","      <td>0.991789</td>\n","      <td>0.969122</td>\n","      <td>0.952933</td>\n","      <td>0.985914</td>\n","      <td>0.970097</td>\n","      <td>0.955916</td>\n","      <td>0.984704</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.001000</td>\n","      <td>0.002875</td>\n","      <td>0.973793</td>\n","      <td>0.961157</td>\n","      <td>0.987081</td>\n","      <td>0.991719</td>\n","      <td>0.991719</td>\n","      <td>0.991719</td>\n","      <td>0.966487</td>\n","      <td>0.948793</td>\n","      <td>0.985046</td>\n","      <td>0.969140</td>\n","      <td>0.955411</td>\n","      <td>0.983270</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.001600</td>\n","      <td>0.002806</td>\n","      <td>0.975304</td>\n","      <td>0.963422</td>\n","      <td>0.987691</td>\n","      <td>0.991789</td>\n","      <td>0.991789</td>\n","      <td>0.991789</td>\n","      <td>0.968529</td>\n","      <td>0.951813</td>\n","      <td>0.985914</td>\n","      <td>0.970097</td>\n","      <td>0.955916</td>\n","      <td>0.984704</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.001300</td>\n","      <td>0.002781</td>\n","      <td>0.975083</td>\n","      <td>0.963007</td>\n","      <td>0.987691</td>\n","      <td>0.991789</td>\n","      <td>0.991789</td>\n","      <td>0.991789</td>\n","      <td>0.968235</td>\n","      <td>0.951261</td>\n","      <td>0.985914</td>\n","      <td>0.970097</td>\n","      <td>0.955916</td>\n","      <td>0.984704</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9750827002880741, 'eval_macro_precision': 0.9630069511846001, 'eval_macro_recall': 0.9876906419833318, 'eval_micro_f1': 0.9917893544733862, 'eval_micro_precision': 0.9917893544733862, 'eval_micro_recall': 0.9917893544733862, 'eval_macro_f1_no_o': 0.9682347203807667, 'eval_macro_precision_no_o': 0.9512605837059179, 'eval_macro_recall_no_o': 0.9859138769149641, 'eval_micro_f1_no_o': 0.9700965387332234, 'eval_micro_precision_no_o': 0.9559164733178654, 'eval_micro_recall_no_o': 0.9847036328871893, 'eval_loss': 0.0027815851680497872, 'eval_runtime': 4.9796, 'eval_samples_per_second': 10.041, 'eval_steps_per_second': 1.406, 'epoch': 19.0}\n","Accuracy for fold  1 :  0.9700965387332234\n","--------------------------------\n","FOLD 2\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1606\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3800\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3800' max='3800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3800/3800 16:01, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.007800</td>\n","      <td>0.012269</td>\n","      <td>0.972855</td>\n","      <td>0.965450</td>\n","      <td>0.980745</td>\n","      <td>0.992277</td>\n","      <td>0.992277</td>\n","      <td>0.992277</td>\n","      <td>0.965165</td>\n","      <td>0.954797</td>\n","      <td>0.976177</td>\n","      <td>0.966799</td>\n","      <td>0.956887</td>\n","      <td>0.976919</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.004100</td>\n","      <td>0.007675</td>\n","      <td>0.972917</td>\n","      <td>0.963181</td>\n","      <td>0.983149</td>\n","      <td>0.991877</td>\n","      <td>0.991877</td>\n","      <td>0.991877</td>\n","      <td>0.965362</td>\n","      <td>0.951671</td>\n","      <td>0.979711</td>\n","      <td>0.966155</td>\n","      <td>0.952058</td>\n","      <td>0.980676</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.002500</td>\n","      <td>0.005720</td>\n","      <td>0.974336</td>\n","      <td>0.966848</td>\n","      <td>0.982055</td>\n","      <td>0.992077</td>\n","      <td>0.992077</td>\n","      <td>0.992077</td>\n","      <td>0.967215</td>\n","      <td>0.956636</td>\n","      <td>0.978100</td>\n","      <td>0.966870</td>\n","      <td>0.954974</td>\n","      <td>0.979066</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.001800</td>\n","      <td>0.004737</td>\n","      <td>0.976019</td>\n","      <td>0.970482</td>\n","      <td>0.981832</td>\n","      <td>0.992810</td>\n","      <td>0.992810</td>\n","      <td>0.992810</td>\n","      <td>0.969319</td>\n","      <td>0.961532</td>\n","      <td>0.977474</td>\n","      <td>0.969665</td>\n","      <td>0.961478</td>\n","      <td>0.977992</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.001200</td>\n","      <td>0.004434</td>\n","      <td>0.974006</td>\n","      <td>0.966749</td>\n","      <td>0.981661</td>\n","      <td>0.992210</td>\n","      <td>0.992210</td>\n","      <td>0.992210</td>\n","      <td>0.966763</td>\n","      <td>0.956580</td>\n","      <td>0.977474</td>\n","      <td>0.967605</td>\n","      <td>0.957436</td>\n","      <td>0.977992</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.001400</td>\n","      <td>0.004120</td>\n","      <td>0.975772</td>\n","      <td>0.970615</td>\n","      <td>0.981133</td>\n","      <td>0.992610</td>\n","      <td>0.992610</td>\n","      <td>0.992610</td>\n","      <td>0.969040</td>\n","      <td>0.961734</td>\n","      <td>0.976617</td>\n","      <td>0.969149</td>\n","      <td>0.960464</td>\n","      <td>0.977992</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.000900</td>\n","      <td>0.003901</td>\n","      <td>0.976258</td>\n","      <td>0.970752</td>\n","      <td>0.982010</td>\n","      <td>0.992676</td>\n","      <td>0.992676</td>\n","      <td>0.992676</td>\n","      <td>0.969663</td>\n","      <td>0.961866</td>\n","      <td>0.977787</td>\n","      <td>0.969165</td>\n","      <td>0.959979</td>\n","      <td>0.978529</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.001200</td>\n","      <td>0.003812</td>\n","      <td>0.975780</td>\n","      <td>0.970667</td>\n","      <td>0.981114</td>\n","      <td>0.992543</td>\n","      <td>0.992543</td>\n","      <td>0.992543</td>\n","      <td>0.969063</td>\n","      <td>0.961804</td>\n","      <td>0.976617</td>\n","      <td>0.968891</td>\n","      <td>0.959958</td>\n","      <td>0.977992</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.000800</td>\n","      <td>0.003817</td>\n","      <td>0.976631</td>\n","      <td>0.972292</td>\n","      <td>0.981152</td>\n","      <td>0.992676</td>\n","      <td>0.992676</td>\n","      <td>0.992676</td>\n","      <td>0.970173</td>\n","      <td>0.963970</td>\n","      <td>0.976617</td>\n","      <td>0.969407</td>\n","      <td>0.960970</td>\n","      <td>0.977992</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9766307611201739, 'eval_macro_precision': 0.9722919279966526, 'eval_macro_recall': 0.9811519995014284, 'eval_micro_f1': 0.992676431424767, 'eval_micro_precision': 0.992676431424767, 'eval_micro_recall': 0.992676431424767, 'eval_macro_f1_no_o': 0.9701727754143952, 'eval_macro_precision_no_o': 0.9639702593749945, 'eval_macro_recall_no_o': 0.9766174515406544, 'eval_micro_f1_no_o': 0.9694067571162543, 'eval_micro_precision_no_o': 0.9609704641350211, 'eval_micro_recall_no_o': 0.9779924852388621, 'eval_loss': 0.003816430266087991, 'eval_runtime': 4.9838, 'eval_samples_per_second': 10.032, 'eval_steps_per_second': 1.405, 'epoch': 19.0}\n","Accuracy for fold  2 :  0.9694067571162543\n","--------------------------------\n","FOLD 3\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1607\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3800\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3800' max='3800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3800/3800 16:03, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.006400</td>\n","      <td>0.011333</td>\n","      <td>0.982279</td>\n","      <td>0.976999</td>\n","      <td>0.987697</td>\n","      <td>0.995258</td>\n","      <td>0.995258</td>\n","      <td>0.995258</td>\n","      <td>0.977286</td>\n","      <td>0.969916</td>\n","      <td>0.984840</td>\n","      <td>0.982518</td>\n","      <td>0.976334</td>\n","      <td>0.988781</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.003500</td>\n","      <td>0.006679</td>\n","      <td>0.981697</td>\n","      <td>0.975507</td>\n","      <td>0.988111</td>\n","      <td>0.995258</td>\n","      <td>0.995258</td>\n","      <td>0.995258</td>\n","      <td>0.976510</td>\n","      <td>0.967899</td>\n","      <td>0.985419</td>\n","      <td>0.982527</td>\n","      <td>0.975855</td>\n","      <td>0.989291</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.002500</td>\n","      <td>0.004643</td>\n","      <td>0.982906</td>\n","      <td>0.978215</td>\n","      <td>0.987717</td>\n","      <td>0.995327</td>\n","      <td>0.995327</td>\n","      <td>0.995327</td>\n","      <td>0.978108</td>\n","      <td>0.971537</td>\n","      <td>0.984840</td>\n","      <td>0.982767</td>\n","      <td>0.976826</td>\n","      <td>0.988781</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.001600</td>\n","      <td>0.003664</td>\n","      <td>0.983130</td>\n","      <td>0.978645</td>\n","      <td>0.987737</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.978394</td>\n","      <td>0.972110</td>\n","      <td>0.984840</td>\n","      <td>0.983016</td>\n","      <td>0.977319</td>\n","      <td>0.988781</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.001100</td>\n","      <td>0.003138</td>\n","      <td>0.983130</td>\n","      <td>0.978645</td>\n","      <td>0.987737</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.978394</td>\n","      <td>0.972110</td>\n","      <td>0.984840</td>\n","      <td>0.983016</td>\n","      <td>0.977319</td>\n","      <td>0.988781</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.000900</td>\n","      <td>0.002839</td>\n","      <td>0.982464</td>\n","      <td>0.977405</td>\n","      <td>0.987677</td>\n","      <td>0.995189</td>\n","      <td>0.995189</td>\n","      <td>0.995189</td>\n","      <td>0.977547</td>\n","      <td>0.970456</td>\n","      <td>0.984840</td>\n","      <td>0.982270</td>\n","      <td>0.975843</td>\n","      <td>0.988781</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.000800</td>\n","      <td>0.002660</td>\n","      <td>0.983130</td>\n","      <td>0.978645</td>\n","      <td>0.987737</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.978394</td>\n","      <td>0.972110</td>\n","      <td>0.984840</td>\n","      <td>0.983016</td>\n","      <td>0.977319</td>\n","      <td>0.988781</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.001100</td>\n","      <td>0.002571</td>\n","      <td>0.983130</td>\n","      <td>0.978645</td>\n","      <td>0.987737</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.978394</td>\n","      <td>0.972110</td>\n","      <td>0.984840</td>\n","      <td>0.983016</td>\n","      <td>0.977319</td>\n","      <td>0.988781</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.000900</td>\n","      <td>0.002544</td>\n","      <td>0.983130</td>\n","      <td>0.978645</td>\n","      <td>0.987737</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.995396</td>\n","      <td>0.978394</td>\n","      <td>0.972110</td>\n","      <td>0.984840</td>\n","      <td>0.983016</td>\n","      <td>0.977319</td>\n","      <td>0.988781</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9831296528527464, 'eval_macro_precision': 0.9786447502591404, 'eval_macro_recall': 0.9877365024833329, 'eval_micro_f1': 0.995395505463542, 'eval_micro_precision': 0.995395505463542, 'eval_micro_recall': 0.995395505463542, 'eval_macro_f1_no_o': 0.9783939620154255, 'eval_macro_precision_no_o': 0.9721098725770263, 'eval_macro_recall_no_o': 0.9848400917410812, 'eval_micro_f1_no_o': 0.9830164765525982, 'eval_micro_precision_no_o': 0.9773185483870968, 'eval_micro_recall_no_o': 0.9887812340642529, 'eval_loss': 0.002542914872610709, 'eval_runtime': 5.0006, 'eval_samples_per_second': 9.999, 'eval_steps_per_second': 1.4, 'epoch': 19.0}\n","Accuracy for fold  3 :  0.9830164765525982\n","--------------------------------\n","FOLD 4\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1607\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3800\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3800' max='3800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3800/3800 16:04, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>401</td>\n","      <td>0.006400</td>\n","      <td>0.009797</td>\n","      <td>0.988116</td>\n","      <td>0.984581</td>\n","      <td>0.991722</td>\n","      <td>0.996415</td>\n","      <td>0.996415</td>\n","      <td>0.996415</td>\n","      <td>0.984820</td>\n","      <td>0.979761</td>\n","      <td>0.989972</td>\n","      <td>0.986350</td>\n","      <td>0.979910</td>\n","      <td>0.992875</td>\n","    </tr>\n","    <tr>\n","      <td>802</td>\n","      <td>0.003400</td>\n","      <td>0.006331</td>\n","      <td>0.985318</td>\n","      <td>0.980972</td>\n","      <td>0.989752</td>\n","      <td>0.995174</td>\n","      <td>0.995174</td>\n","      <td>0.995174</td>\n","      <td>0.981355</td>\n","      <td>0.975135</td>\n","      <td>0.987692</td>\n","      <td>0.982332</td>\n","      <td>0.974462</td>\n","      <td>0.990331</td>\n","    </tr>\n","    <tr>\n","      <td>1203</td>\n","      <td>0.001900</td>\n","      <td>0.004363</td>\n","      <td>0.989631</td>\n","      <td>0.989246</td>\n","      <td>0.990071</td>\n","      <td>0.996277</td>\n","      <td>0.996277</td>\n","      <td>0.996277</td>\n","      <td>0.986892</td>\n","      <td>0.986167</td>\n","      <td>0.987692</td>\n","      <td>0.986315</td>\n","      <td>0.982332</td>\n","      <td>0.990331</td>\n","    </tr>\n","    <tr>\n","      <td>1604</td>\n","      <td>0.001400</td>\n","      <td>0.003496</td>\n","      <td>0.989620</td>\n","      <td>0.989053</td>\n","      <td>0.990233</td>\n","      <td>0.996277</td>\n","      <td>0.996277</td>\n","      <td>0.996277</td>\n","      <td>0.986852</td>\n","      <td>0.985884</td>\n","      <td>0.987881</td>\n","      <td>0.985808</td>\n","      <td>0.981827</td>\n","      <td>0.989822</td>\n","    </tr>\n","    <tr>\n","      <td>2005</td>\n","      <td>0.001100</td>\n","      <td>0.002986</td>\n","      <td>0.990817</td>\n","      <td>0.991328</td>\n","      <td>0.990381</td>\n","      <td>0.996484</td>\n","      <td>0.996484</td>\n","      <td>0.996484</td>\n","      <td>0.988395</td>\n","      <td>0.988889</td>\n","      <td>0.987999</td>\n","      <td>0.986308</td>\n","      <td>0.982820</td>\n","      <td>0.989822</td>\n","    </tr>\n","    <tr>\n","      <td>2406</td>\n","      <td>0.000800</td>\n","      <td>0.002710</td>\n","      <td>0.991196</td>\n","      <td>0.991892</td>\n","      <td>0.990563</td>\n","      <td>0.996484</td>\n","      <td>0.996484</td>\n","      <td>0.996484</td>\n","      <td>0.988939</td>\n","      <td>0.989722</td>\n","      <td>0.988241</td>\n","      <td>0.987059</td>\n","      <td>0.984312</td>\n","      <td>0.989822</td>\n","    </tr>\n","    <tr>\n","      <td>2807</td>\n","      <td>0.001500</td>\n","      <td>0.002560</td>\n","      <td>0.990147</td>\n","      <td>0.989864</td>\n","      <td>0.990483</td>\n","      <td>0.996208</td>\n","      <td>0.996208</td>\n","      <td>0.996208</td>\n","      <td>0.987594</td>\n","      <td>0.987018</td>\n","      <td>0.988241</td>\n","      <td>0.986058</td>\n","      <td>0.982323</td>\n","      <td>0.989822</td>\n","    </tr>\n","    <tr>\n","      <td>3208</td>\n","      <td>0.001300</td>\n","      <td>0.002487</td>\n","      <td>0.990227</td>\n","      <td>0.990507</td>\n","      <td>0.990016</td>\n","      <td>0.996208</td>\n","      <td>0.996208</td>\n","      <td>0.996208</td>\n","      <td>0.987688</td>\n","      <td>0.987875</td>\n","      <td>0.987592</td>\n","      <td>0.985801</td>\n","      <td>0.982314</td>\n","      <td>0.989313</td>\n","    </tr>\n","    <tr>\n","      <td>3609</td>\n","      <td>0.001000</td>\n","      <td>0.002466</td>\n","      <td>0.990579</td>\n","      <td>0.990721</td>\n","      <td>0.990503</td>\n","      <td>0.996277</td>\n","      <td>0.996277</td>\n","      <td>0.996277</td>\n","      <td>0.988156</td>\n","      <td>0.988160</td>\n","      <td>0.988241</td>\n","      <td>0.986308</td>\n","      <td>0.982820</td>\n","      <td>0.989822</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-401\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1203\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1604\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2005\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2406\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2807\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3208\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3609\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9905787393405279, 'eval_macro_precision': 0.9907206363454815, 'eval_macro_recall': 0.990502841355944, 'eval_micro_f1': 0.9962774024541569, 'eval_micro_precision': 0.9962774024541569, 'eval_micro_recall': 0.9962774024541569, 'eval_macro_f1_no_o': 0.9881563660331021, 'eval_macro_precision_no_o': 0.9881596989435989, 'eval_macro_recall_no_o': 0.9882408243303186, 'eval_micro_f1_no_o': 0.9863083164300204, 'eval_micro_precision_no_o': 0.9828196058615463, 'eval_micro_recall_no_o': 0.989821882951654, 'eval_loss': 0.0024650488651241174, 'eval_runtime': 4.9997, 'eval_samples_per_second': 10.001, 'eval_steps_per_second': 1.4, 'epoch': 19.0}\n","Accuracy for fold  4 :  0.9863083164300204\n","--------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"BBA4owXH8oS1"},"source":["# Results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhusoblW-oCA","executionInfo":{"status":"ok","timestamp":1646797233555,"user_tz":360,"elapsed":16,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"31183c19-4f43-4fac-9de8-457a95f7fcb3"},"source":["results"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.87532406, 0.97009654, 0.96940676, 0.98301648, 0.98630832])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPIH-gt_-foF","executionInfo":{"status":"ok","timestamp":1646797233556,"user_tz":360,"elapsed":8,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"db273776-97e3-4db8-dd90-6581670bdd71"},"source":["# Print fold results\n","print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n","print('--------------------------------')\n","sum = 0.0\n","key = 0\n","for value in results:\n","  print(f'Fold {key}: {value} %')\n","  sum += value\n","  key += 1\n","print(f'Average: {sum/len(results)} %')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n","--------------------------------\n","Fold 0: 0.8753240631628564 %\n","Fold 1: 0.9700965387332234 %\n","Fold 2: 0.9694067571162543 %\n","Fold 3: 0.9830164765525982 %\n","Fold 4: 0.9863083164300204 %\n","Average: 0.9568304303989905 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"1DkDHptAhniL"},"source":["# Pytorch Training - Loop UPDT"]},{"cell_type":"code","metadata":{"id":"rirS-e83GVn0","executionInfo":{"status":"ok","timestamp":1646982284160,"user_tz":360,"elapsed":2,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["device = 'cuda'"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"HrsfoSRSI94T","executionInfo":{"status":"ok","timestamp":1646982284318,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["#'learning_rate': 7.416939461964081e-06, 'num_train_epochs': 12, 'weight_decay': 0.22055622812922648, 'per_device_train_batch_size': 8}"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSDdzo_so0vk","executionInfo":{"status":"ok","timestamp":1646982284318,"user_tz":360,"elapsed":4,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["training_args = TrainingArguments(\"trained_scibert_ner_model\", # output dir\n","                                      learning_rate=7.648631456296879e-06, \n","                                      num_train_epochs=19, \n","                                      dataloader_drop_last=True,\n","                                      per_device_eval_batch_size=8, \n","                                      per_device_train_batch_size=8,\n","                                      logging_steps=50,\n","                                      save_steps=len(train) // batch_size,\n","                                      lr_scheduler_type='cosine',\n","                                      evaluation_strategy='steps',\n","                                      weight_decay=0.4444157928489863,\n","                                      eval_steps=len(train) // batch_size\n","                                      )\n","\n","#learning_rate 7.648631456296879e-06\n","#num_train_epochs 19\n","#weight_decay 0.44441579284898636\n","#per_device_train_batch_size 8\n","\n","#load_param()"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"8ea72ecd-b643-42d0-d6da-0326a5c6ef76","id":"nZR3aDuKyUqy","executionInfo":{"status":"ok","timestamp":1647038132900,"user_tz":360,"elapsed":282629,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["loop_val = 30\n","loop_results = np.zeros(loop_val)\n","loop_resultss = np.zeros(loop_val)\n","for r in range(loop_val):\n","\n","  ner_model = NerModel(BertEmbModel).to('cuda')  # make sure we move the model to the GPU for training\n","\n","  trainer = MultilabelTrainer(\n","      model=ner_model, \n","      args=training_args, \n","      train_dataset=train, \n","      eval_dataset=test,\n","      data_collator=collator  # defines how to merge data into batches, using the collator function above\n","  )\n","\n","  # Print\n","  print(f'Train run #{r}')\n","  print('--------------------------------')\n","\n","  trainer.train()\n","\n","  # Process is complete.\n","  print('Training process has finished.')\n","\n","  # Print about testing\n","  print('Starting testing')\n","\n","  with torch.no_grad():\n","    result = trainer.evaluate(test)\n","    print(result)\n","\n","    # Print accuracy\n","    print('Accuracy for fold ', r, ': ', result['eval_micro_f1_no_o'], ' -- ', result['eval_micro_f1'])\n","    print('--------------------------------')\n","    loop_results[r] = result['eval_micro_f1_no_o']\n","    loop_resultss[r] = result['eval_micro_f1']\n","    del result\n","\n","  if r > 0:\n","    if loop_results[r] < loop_results[r-1]:\n","      save_path = f'./model-fold-{r}.pth'\n","      torch.save(ner_model.state_dict(), save_path)\n","\n","  print('Testing process has finished.')"],"execution_count":25,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train run #0\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:56, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.013600</td>\n","      <td>0.031001</td>\n","      <td>0.431051</td>\n","      <td>0.499299</td>\n","      <td>0.455481</td>\n","      <td>0.865356</td>\n","      <td>0.865356</td>\n","      <td>0.865356</td>\n","      <td>0.265358</td>\n","      <td>0.375869</td>\n","      <td>0.275602</td>\n","      <td>0.215732</td>\n","      <td>0.711160</td>\n","      <td>0.127152</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.004700</td>\n","      <td>0.025884</td>\n","      <td>0.559517</td>\n","      <td>0.716302</td>\n","      <td>0.573667</td>\n","      <td>0.886588</td>\n","      <td>0.886588</td>\n","      <td>0.886588</td>\n","      <td>0.430383</td>\n","      <td>0.652946</td>\n","      <td>0.434467</td>\n","      <td>0.396800</td>\n","      <td>0.623116</td>\n","      <td>0.291080</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.002800</td>\n","      <td>0.021995</td>\n","      <td>0.618203</td>\n","      <td>0.669953</td>\n","      <td>0.665576</td>\n","      <td>0.892262</td>\n","      <td>0.892262</td>\n","      <td>0.892262</td>\n","      <td>0.506144</td>\n","      <td>0.580084</td>\n","      <td>0.564210</td>\n","      <td>0.497202</td>\n","      <td>0.552632</td>\n","      <td>0.451878</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001800</td>\n","      <td>0.022825</td>\n","      <td>0.720182</td>\n","      <td>0.734925</td>\n","      <td>0.750691</td>\n","      <td>0.918231</td>\n","      <td>0.918231</td>\n","      <td>0.918231</td>\n","      <td>0.638750</td>\n","      <td>0.661166</td>\n","      <td>0.676620</td>\n","      <td>0.638946</td>\n","      <td>0.674196</td>\n","      <td>0.607199</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002400</td>\n","      <td>0.020452</td>\n","      <td>0.813947</td>\n","      <td>0.794510</td>\n","      <td>0.843633</td>\n","      <td>0.931918</td>\n","      <td>0.931918</td>\n","      <td>0.931918</td>\n","      <td>0.763361</td>\n","      <td>0.736352</td>\n","      <td>0.804027</td>\n","      <td>0.743954</td>\n","      <td>0.730219</td>\n","      <td>0.758216</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001700</td>\n","      <td>0.020884</td>\n","      <td>0.808956</td>\n","      <td>0.782023</td>\n","      <td>0.845339</td>\n","      <td>0.929169</td>\n","      <td>0.929169</td>\n","      <td>0.929169</td>\n","      <td>0.757421</td>\n","      <td>0.719746</td>\n","      <td>0.807677</td>\n","      <td>0.740417</td>\n","      <td>0.718865</td>\n","      <td>0.763302</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.024039</td>\n","      <td>0.815905</td>\n","      <td>0.812345</td>\n","      <td>0.823074</td>\n","      <td>0.933848</td>\n","      <td>0.933848</td>\n","      <td>0.933848</td>\n","      <td>0.765691</td>\n","      <td>0.761958</td>\n","      <td>0.774232</td>\n","      <td>0.743825</td>\n","      <td>0.757711</td>\n","      <td>0.730438</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.001400</td>\n","      <td>0.021208</td>\n","      <td>0.812136</td>\n","      <td>0.788770</td>\n","      <td>0.841759</td>\n","      <td>0.932035</td>\n","      <td>0.932035</td>\n","      <td>0.932035</td>\n","      <td>0.761035</td>\n","      <td>0.728574</td>\n","      <td>0.801827</td>\n","      <td>0.746988</td>\n","      <td>0.730640</td>\n","      <td>0.764085</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.001400</td>\n","      <td>0.020971</td>\n","      <td>0.813548</td>\n","      <td>0.789555</td>\n","      <td>0.844056</td>\n","      <td>0.931976</td>\n","      <td>0.931976</td>\n","      <td>0.931976</td>\n","      <td>0.762966</td>\n","      <td>0.729559</td>\n","      <td>0.805050</td>\n","      <td>0.747853</td>\n","      <td>0.730153</td>\n","      <td>0.766432</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8136570742379785, 'eval_macro_precision': 0.7901239192986469, 'eval_macro_recall': 0.8435684245335198, 'eval_micro_f1': 0.9322103292975376, 'eval_micro_precision': 0.9322103292975376, 'eval_micro_recall': 0.9322103292975376, 'eval_macro_f1_no_o': 0.7630409873971247, 'eval_macro_precision_no_o': 0.7303131684037437, 'eval_macro_recall_no_o': 0.8042618307903546, 'eval_micro_f1_no_o': 0.7479457290273266, 'eval_micro_precision_no_o': 0.731042211430706, 'eval_micro_recall_no_o': 0.7656494522691706, 'eval_loss': 0.02102994751051786, 'eval_runtime': 21.4694, 'eval_samples_per_second': 2.795, 'eval_steps_per_second': 0.373, 'epoch': 19.0}\n","Accuracy for fold  0 :  0.7479457290273266  --  0.9322103292975376\n","--------------------------------\n","Testing process has finished.\n","Train run #1\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 34:35, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.020100</td>\n","      <td>0.056828</td>\n","      <td>0.788284</td>\n","      <td>0.719662</td>\n","      <td>0.893936</td>\n","      <td>0.911446</td>\n","      <td>0.911446</td>\n","      <td>0.911446</td>\n","      <td>0.733522</td>\n","      <td>0.630590</td>\n","      <td>0.885058</td>\n","      <td>0.721629</td>\n","      <td>0.621851</td>\n","      <td>0.859546</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.009700</td>\n","      <td>0.045812</td>\n","      <td>0.812271</td>\n","      <td>0.773266</td>\n","      <td>0.860349</td>\n","      <td>0.929695</td>\n","      <td>0.929695</td>\n","      <td>0.929695</td>\n","      <td>0.761592</td>\n","      <td>0.704817</td>\n","      <td>0.830327</td>\n","      <td>0.750316</td>\n","      <td>0.697479</td>\n","      <td>0.811815</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.005400</td>\n","      <td>0.044274</td>\n","      <td>0.807841</td>\n","      <td>0.763882</td>\n","      <td>0.866405</td>\n","      <td>0.927882</td>\n","      <td>0.927882</td>\n","      <td>0.927882</td>\n","      <td>0.756332</td>\n","      <td>0.692127</td>\n","      <td>0.839823</td>\n","      <td>0.751874</td>\n","      <td>0.691399</td>\n","      <td>0.823944</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.003600</td>\n","      <td>0.045088</td>\n","      <td>0.812120</td>\n","      <td>0.770745</td>\n","      <td>0.863265</td>\n","      <td>0.926069</td>\n","      <td>0.926069</td>\n","      <td>0.926069</td>\n","      <td>0.762477</td>\n","      <td>0.701501</td>\n","      <td>0.836277</td>\n","      <td>0.748265</td>\n","      <td>0.686134</td>\n","      <td>0.822770</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.003000</td>\n","      <td>0.042009</td>\n","      <td>0.819507</td>\n","      <td>0.787257</td>\n","      <td>0.857576</td>\n","      <td>0.934140</td>\n","      <td>0.934140</td>\n","      <td>0.934140</td>\n","      <td>0.770557</td>\n","      <td>0.724406</td>\n","      <td>0.824406</td>\n","      <td>0.762011</td>\n","      <td>0.724515</td>\n","      <td>0.803599</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.002400</td>\n","      <td>0.043071</td>\n","      <td>0.820315</td>\n","      <td>0.781091</td>\n","      <td>0.868288</td>\n","      <td>0.930631</td>\n","      <td>0.930631</td>\n","      <td>0.930631</td>\n","      <td>0.772645</td>\n","      <td>0.715196</td>\n","      <td>0.841600</td>\n","      <td>0.762401</td>\n","      <td>0.705120</td>\n","      <td>0.829812</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001900</td>\n","      <td>0.043504</td>\n","      <td>0.820843</td>\n","      <td>0.783518</td>\n","      <td>0.865471</td>\n","      <td>0.932444</td>\n","      <td>0.932444</td>\n","      <td>0.932444</td>\n","      <td>0.772855</td>\n","      <td>0.718749</td>\n","      <td>0.836583</td>\n","      <td>0.763378</td>\n","      <td>0.713751</td>\n","      <td>0.820423</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.002100</td>\n","      <td>0.043618</td>\n","      <td>0.821779</td>\n","      <td>0.785593</td>\n","      <td>0.864849</td>\n","      <td>0.933146</td>\n","      <td>0.933146</td>\n","      <td>0.933146</td>\n","      <td>0.774029</td>\n","      <td>0.721558</td>\n","      <td>0.835570</td>\n","      <td>0.766229</td>\n","      <td>0.717555</td>\n","      <td>0.821987</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.002200</td>\n","      <td>0.043355</td>\n","      <td>0.821576</td>\n","      <td>0.786008</td>\n","      <td>0.863722</td>\n","      <td>0.933380</td>\n","      <td>0.933380</td>\n","      <td>0.933380</td>\n","      <td>0.773660</td>\n","      <td>0.722175</td>\n","      <td>0.833815</td>\n","      <td>0.765631</td>\n","      <td>0.718600</td>\n","      <td>0.819249</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8222484006531178, 'eval_macro_precision': 0.7871193431830275, 'eval_macro_recall': 0.8637387232967211, 'eval_micro_f1': 0.9334386149616892, 'eval_micro_precision': 0.9334386149616892, 'eval_micro_recall': 0.9334386149616892, 'eval_macro_f1_no_o': 0.774544139658175, 'eval_macro_precision_no_o': 0.7236558341519141, 'eval_macro_recall_no_o': 0.8338147101719389, 'eval_micro_f1_no_o': 0.7657707076247943, 'eval_micro_precision_no_o': 0.7188465499485067, 'eval_micro_recall_no_o': 0.8192488262910798, 'eval_loss': 0.04336144351824866, 'eval_runtime': 22.6232, 'eval_samples_per_second': 2.652, 'eval_steps_per_second': 0.354, 'epoch': 19.0}\n","Accuracy for fold  1 :  0.7657707076247943  --  0.9334386149616892\n","--------------------------------\n","Testing process has finished.\n","Train run #2\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 20:52, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.009400</td>\n","      <td>0.047705</td>\n","      <td>0.825715</td>\n","      <td>0.797839</td>\n","      <td>0.857835</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.778978</td>\n","      <td>0.738855</td>\n","      <td>0.824705</td>\n","      <td>0.767399</td>\n","      <td>0.731725</td>\n","      <td>0.806729</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.004900</td>\n","      <td>0.045974</td>\n","      <td>0.817418</td>\n","      <td>0.767592</td>\n","      <td>0.880652</td>\n","      <td>0.927882</td>\n","      <td>0.927882</td>\n","      <td>0.927882</td>\n","      <td>0.768999</td>\n","      <td>0.696245</td>\n","      <td>0.859391</td>\n","      <td>0.752605</td>\n","      <td>0.685871</td>\n","      <td>0.833725</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.002600</td>\n","      <td>0.046966</td>\n","      <td>0.821542</td>\n","      <td>0.792542</td>\n","      <td>0.855345</td>\n","      <td>0.933907</td>\n","      <td>0.933907</td>\n","      <td>0.933907</td>\n","      <td>0.773381</td>\n","      <td>0.732013</td>\n","      <td>0.821110</td>\n","      <td>0.760837</td>\n","      <td>0.728183</td>\n","      <td>0.796557</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001800</td>\n","      <td>0.046794</td>\n","      <td>0.815207</td>\n","      <td>0.780137</td>\n","      <td>0.857693</td>\n","      <td>0.931567</td>\n","      <td>0.931567</td>\n","      <td>0.931567</td>\n","      <td>0.765344</td>\n","      <td>0.715117</td>\n","      <td>0.825388</td>\n","      <td>0.755121</td>\n","      <td>0.714635</td>\n","      <td>0.800469</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001600</td>\n","      <td>0.046865</td>\n","      <td>0.818311</td>\n","      <td>0.783413</td>\n","      <td>0.859839</td>\n","      <td>0.931918</td>\n","      <td>0.931918</td>\n","      <td>0.931918</td>\n","      <td>0.769439</td>\n","      <td>0.719348</td>\n","      <td>0.828294</td>\n","      <td>0.756956</td>\n","      <td>0.715430</td>\n","      <td>0.803599</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001200</td>\n","      <td>0.046644</td>\n","      <td>0.830032</td>\n","      <td>0.796165</td>\n","      <td>0.869449</td>\n","      <td>0.932737</td>\n","      <td>0.932737</td>\n","      <td>0.932737</td>\n","      <td>0.785254</td>\n","      <td>0.735844</td>\n","      <td>0.841955</td>\n","      <td>0.767268</td>\n","      <td>0.718185</td>\n","      <td>0.823552</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.001000</td>\n","      <td>0.047080</td>\n","      <td>0.825352</td>\n","      <td>0.803188</td>\n","      <td>0.850460</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.778426</td>\n","      <td>0.746889</td>\n","      <td>0.813864</td>\n","      <td>0.763093</td>\n","      <td>0.738383</td>\n","      <td>0.789515</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.001100</td>\n","      <td>0.047228</td>\n","      <td>0.829325</td>\n","      <td>0.804186</td>\n","      <td>0.857953</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.783747</td>\n","      <td>0.747538</td>\n","      <td>0.824565</td>\n","      <td>0.765838</td>\n","      <td>0.733095</td>\n","      <td>0.801643</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.001200</td>\n","      <td>0.047233</td>\n","      <td>0.829700</td>\n","      <td>0.800382</td>\n","      <td>0.863220</td>\n","      <td>0.934024</td>\n","      <td>0.934024</td>\n","      <td>0.934024</td>\n","      <td>0.784443</td>\n","      <td>0.742031</td>\n","      <td>0.832390</td>\n","      <td>0.766556</td>\n","      <td>0.727018</td>\n","      <td>0.810642</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8298657135158324, 'eval_macro_precision': 0.8011256525124756, 'eval_macro_recall': 0.862663423291731, 'eval_micro_f1': 0.9340235128969995, 'eval_micro_precision': 0.9340235128969995, 'eval_micro_recall': 0.9340235128969995, 'eval_macro_f1_no_o': 0.7846844645863665, 'eval_macro_precision_no_o': 0.7431131621656553, 'eval_macro_recall_no_o': 0.8316015714723939, 'eval_micro_f1_no_o': 0.7666666666666667, 'eval_micro_precision_no_o': 0.7278481012658228, 'eval_micro_recall_no_o': 0.8098591549295775, 'eval_loss': 0.04719858983977853, 'eval_runtime': 7.6205, 'eval_samples_per_second': 7.874, 'eval_steps_per_second': 1.05, 'epoch': 19.0}\n","Accuracy for fold  2 :  0.7666666666666667  --  0.9340235128969995\n","--------------------------------\n","Testing process has finished.\n","Train run #3\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 17:34, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.008100</td>\n","      <td>0.052399</td>\n","      <td>0.819803</td>\n","      <td>0.813798</td>\n","      <td>0.826441</td>\n","      <td>0.934433</td>\n","      <td>0.934433</td>\n","      <td>0.934433</td>\n","      <td>0.771165</td>\n","      <td>0.762826</td>\n","      <td>0.780348</td>\n","      <td>0.757682</td>\n","      <td>0.753287</td>\n","      <td>0.762128</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.004100</td>\n","      <td>0.045080</td>\n","      <td>0.820240</td>\n","      <td>0.779155</td>\n","      <td>0.871443</td>\n","      <td>0.928818</td>\n","      <td>0.928818</td>\n","      <td>0.928818</td>\n","      <td>0.773015</td>\n","      <td>0.712680</td>\n","      <td>0.846655</td>\n","      <td>0.759779</td>\n","      <td>0.698981</td>\n","      <td>0.832160</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.002400</td>\n","      <td>0.044109</td>\n","      <td>0.821676</td>\n","      <td>0.792037</td>\n","      <td>0.855661</td>\n","      <td>0.932093</td>\n","      <td>0.932093</td>\n","      <td>0.932093</td>\n","      <td>0.774034</td>\n","      <td>0.731164</td>\n","      <td>0.822632</td>\n","      <td>0.759105</td>\n","      <td>0.719593</td>\n","      <td>0.803208</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001700</td>\n","      <td>0.043014</td>\n","      <td>0.826925</td>\n","      <td>0.794482</td>\n","      <td>0.864981</td>\n","      <td>0.931333</td>\n","      <td>0.931333</td>\n","      <td>0.931333</td>\n","      <td>0.781404</td>\n","      <td>0.733838</td>\n","      <td>0.836342</td>\n","      <td>0.763292</td>\n","      <td>0.713896</td>\n","      <td>0.820031</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001500</td>\n","      <td>0.046986</td>\n","      <td>0.835288</td>\n","      <td>0.822447</td>\n","      <td>0.849061</td>\n","      <td>0.938644</td>\n","      <td>0.938644</td>\n","      <td>0.938644</td>\n","      <td>0.790989</td>\n","      <td>0.772705</td>\n","      <td>0.810508</td>\n","      <td>0.774540</td>\n","      <td>0.759398</td>\n","      <td>0.790297</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001100</td>\n","      <td>0.045620</td>\n","      <td>0.833570</td>\n","      <td>0.812156</td>\n","      <td>0.857042</td>\n","      <td>0.935895</td>\n","      <td>0.935895</td>\n","      <td>0.935895</td>\n","      <td>0.789256</td>\n","      <td>0.758378</td>\n","      <td>0.822845</td>\n","      <td>0.769694</td>\n","      <td>0.740861</td>\n","      <td>0.800861</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000900</td>\n","      <td>0.044895</td>\n","      <td>0.832933</td>\n","      <td>0.809200</td>\n","      <td>0.859283</td>\n","      <td>0.936129</td>\n","      <td>0.936129</td>\n","      <td>0.936129</td>\n","      <td>0.788393</td>\n","      <td>0.754148</td>\n","      <td>0.826086</td>\n","      <td>0.771851</td>\n","      <td>0.739864</td>\n","      <td>0.806729</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.001000</td>\n","      <td>0.045273</td>\n","      <td>0.836390</td>\n","      <td>0.816560</td>\n","      <td>0.858003</td>\n","      <td>0.937006</td>\n","      <td>0.937006</td>\n","      <td>0.937006</td>\n","      <td>0.792831</td>\n","      <td>0.764086</td>\n","      <td>0.823921</td>\n","      <td>0.773745</td>\n","      <td>0.745020</td>\n","      <td>0.804773</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.001100</td>\n","      <td>0.045657</td>\n","      <td>0.833683</td>\n","      <td>0.812522</td>\n","      <td>0.856850</td>\n","      <td>0.936422</td>\n","      <td>0.936422</td>\n","      <td>0.936422</td>\n","      <td>0.789268</td>\n","      <td>0.758727</td>\n","      <td>0.822452</td>\n","      <td>0.770822</td>\n","      <td>0.741947</td>\n","      <td>0.802034</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8317805707473789, 'eval_macro_precision': 0.8111338645548277, 'eval_macro_recall': 0.8543342380152715, 'eval_micro_f1': 0.9361291454641165, 'eval_micro_precision': 0.9361291454641165, 'eval_micro_recall': 0.9361291454641165, 'eval_macro_f1_no_o': 0.78676467604729, 'eval_macro_precision_no_o': 0.7569657097288677, 'eval_macro_recall_no_o': 0.8190747225069401, 'eval_micro_f1_no_o': 0.769288671433948, 'eval_micro_precision_no_o': 0.7411167512690355, 'eval_micro_recall_no_o': 0.7996870109546166, 'eval_loss': 0.045696636646183224, 'eval_runtime': 7.6748, 'eval_samples_per_second': 7.818, 'eval_steps_per_second': 1.042, 'epoch': 19.0}\n","Accuracy for fold  3 :  0.769288671433948  --  0.9361291454641165\n","--------------------------------\n","Testing process has finished.\n","Train run #4\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 26:09, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.007600</td>\n","      <td>0.050434</td>\n","      <td>0.814203</td>\n","      <td>0.785701</td>\n","      <td>0.851728</td>\n","      <td>0.928935</td>\n","      <td>0.928935</td>\n","      <td>0.928935</td>\n","      <td>0.764799</td>\n","      <td>0.722259</td>\n","      <td>0.819245</td>\n","      <td>0.754717</td>\n","      <td>0.703654</td>\n","      <td>0.813772</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003700</td>\n","      <td>0.043203</td>\n","      <td>0.818145</td>\n","      <td>0.774386</td>\n","      <td>0.871804</td>\n","      <td>0.928642</td>\n","      <td>0.928642</td>\n","      <td>0.928642</td>\n","      <td>0.770117</td>\n","      <td>0.705860</td>\n","      <td>0.847365</td>\n","      <td>0.758216</td>\n","      <td>0.694435</td>\n","      <td>0.834898</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.002100</td>\n","      <td>0.043910</td>\n","      <td>0.830521</td>\n","      <td>0.805428</td>\n","      <td>0.859092</td>\n","      <td>0.937767</td>\n","      <td>0.937767</td>\n","      <td>0.937767</td>\n","      <td>0.784783</td>\n","      <td>0.749122</td>\n","      <td>0.825051</td>\n","      <td>0.774680</td>\n","      <td>0.747093</td>\n","      <td>0.804382</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001400</td>\n","      <td>0.043588</td>\n","      <td>0.835646</td>\n","      <td>0.813600</td>\n","      <td>0.859816</td>\n","      <td>0.937299</td>\n","      <td>0.937299</td>\n","      <td>0.937299</td>\n","      <td>0.791891</td>\n","      <td>0.760009</td>\n","      <td>0.826568</td>\n","      <td>0.777049</td>\n","      <td>0.746129</td>\n","      <td>0.810642</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001300</td>\n","      <td>0.046903</td>\n","      <td>0.836699</td>\n","      <td>0.820736</td>\n","      <td>0.853793</td>\n","      <td>0.938059</td>\n","      <td>0.938059</td>\n","      <td>0.938059</td>\n","      <td>0.792990</td>\n","      <td>0.770081</td>\n","      <td>0.817390</td>\n","      <td>0.774206</td>\n","      <td>0.753425</td>\n","      <td>0.796166</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001000</td>\n","      <td>0.048226</td>\n","      <td>0.830877</td>\n","      <td>0.812059</td>\n","      <td>0.851371</td>\n","      <td>0.936012</td>\n","      <td>0.936012</td>\n","      <td>0.936012</td>\n","      <td>0.785532</td>\n","      <td>0.758330</td>\n","      <td>0.814940</td>\n","      <td>0.767547</td>\n","      <td>0.741254</td>\n","      <td>0.795775</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000800</td>\n","      <td>0.046507</td>\n","      <td>0.828579</td>\n","      <td>0.808604</td>\n","      <td>0.850590</td>\n","      <td>0.936480</td>\n","      <td>0.936480</td>\n","      <td>0.936480</td>\n","      <td>0.782222</td>\n","      <td>0.753557</td>\n","      <td>0.813578</td>\n","      <td>0.766295</td>\n","      <td>0.740957</td>\n","      <td>0.793427</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000900</td>\n","      <td>0.044598</td>\n","      <td>0.833570</td>\n","      <td>0.810896</td>\n","      <td>0.858594</td>\n","      <td>0.936948</td>\n","      <td>0.936948</td>\n","      <td>0.936948</td>\n","      <td>0.788944</td>\n","      <td>0.756063</td>\n","      <td>0.824915</td>\n","      <td>0.772396</td>\n","      <td>0.739878</td>\n","      <td>0.807903</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.001000</td>\n","      <td>0.044516</td>\n","      <td>0.834042</td>\n","      <td>0.810788</td>\n","      <td>0.859778</td>\n","      <td>0.937123</td>\n","      <td>0.937123</td>\n","      <td>0.937123</td>\n","      <td>0.789565</td>\n","      <td>0.755830</td>\n","      <td>0.826563</td>\n","      <td>0.773627</td>\n","      <td>0.740172</td>\n","      <td>0.810250</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8340423777616239, 'eval_macro_precision': 0.8107881364967311, 'eval_macro_recall': 0.859778301695964, 'eval_micro_f1': 0.937123471954144, 'eval_micro_precision': 0.937123471954144, 'eval_micro_recall': 0.937123471954144, 'eval_macro_f1_no_o': 0.789564594297052, 'eval_macro_precision_no_o': 0.7558299707454377, 'eval_macro_recall_no_o': 0.826562710951655, 'eval_micro_f1_no_o': 0.7736271946208442, 'eval_micro_precision_no_o': 0.7401715511079342, 'eval_micro_recall_no_o': 0.8102503912363067, 'eval_loss': 0.044531269707840694, 'eval_runtime': 24.1604, 'eval_samples_per_second': 2.483, 'eval_steps_per_second': 0.331, 'epoch': 19.0}\n","Accuracy for fold  4 :  0.7736271946208442  --  0.937123471954144\n","--------------------------------\n","Testing process has finished.\n","Train run #5\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 32:50, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.007500</td>\n","      <td>0.050776</td>\n","      <td>0.806269</td>\n","      <td>0.758493</td>\n","      <td>0.867113</td>\n","      <td>0.921390</td>\n","      <td>0.921390</td>\n","      <td>0.921390</td>\n","      <td>0.755767</td>\n","      <td>0.684728</td>\n","      <td>0.843908</td>\n","      <td>0.741436</td>\n","      <td>0.667293</td>\n","      <td>0.834116</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003500</td>\n","      <td>0.047816</td>\n","      <td>0.818938</td>\n","      <td>0.789916</td>\n","      <td>0.853753</td>\n","      <td>0.932561</td>\n","      <td>0.932561</td>\n","      <td>0.932561</td>\n","      <td>0.770258</td>\n","      <td>0.728839</td>\n","      <td>0.819354</td>\n","      <td>0.757938</td>\n","      <td>0.725161</td>\n","      <td>0.793818</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.002100</td>\n","      <td>0.046435</td>\n","      <td>0.830676</td>\n","      <td>0.808240</td>\n","      <td>0.855568</td>\n","      <td>0.935603</td>\n","      <td>0.935603</td>\n","      <td>0.935603</td>\n","      <td>0.785391</td>\n","      <td>0.753355</td>\n","      <td>0.820674</td>\n","      <td>0.767025</td>\n","      <td>0.740619</td>\n","      <td>0.795383</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001500</td>\n","      <td>0.047051</td>\n","      <td>0.835174</td>\n","      <td>0.818884</td>\n","      <td>0.853050</td>\n","      <td>0.937006</td>\n","      <td>0.937006</td>\n","      <td>0.937006</td>\n","      <td>0.791366</td>\n","      <td>0.767810</td>\n","      <td>0.817019</td>\n","      <td>0.774829</td>\n","      <td>0.751471</td>\n","      <td>0.799687</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001300</td>\n","      <td>0.047989</td>\n","      <td>0.837796</td>\n","      <td>0.818299</td>\n","      <td>0.858962</td>\n","      <td>0.937474</td>\n","      <td>0.937474</td>\n","      <td>0.937474</td>\n","      <td>0.794745</td>\n","      <td>0.766671</td>\n","      <td>0.825015</td>\n","      <td>0.776666</td>\n","      <td>0.750456</td>\n","      <td>0.804773</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001000</td>\n","      <td>0.046223</td>\n","      <td>0.836896</td>\n","      <td>0.814931</td>\n","      <td>0.861552</td>\n","      <td>0.936480</td>\n","      <td>0.936480</td>\n","      <td>0.936480</td>\n","      <td>0.793802</td>\n","      <td>0.761641</td>\n","      <td>0.829501</td>\n","      <td>0.776970</td>\n","      <td>0.741729</td>\n","      <td>0.815728</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000800</td>\n","      <td>0.046655</td>\n","      <td>0.836970</td>\n","      <td>0.819256</td>\n","      <td>0.856545</td>\n","      <td>0.937767</td>\n","      <td>0.937767</td>\n","      <td>0.937767</td>\n","      <td>0.793530</td>\n","      <td>0.767765</td>\n","      <td>0.821748</td>\n","      <td>0.776919</td>\n","      <td>0.749909</td>\n","      <td>0.805947</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000900</td>\n","      <td>0.046376</td>\n","      <td>0.838794</td>\n","      <td>0.817159</td>\n","      <td>0.862930</td>\n","      <td>0.937533</td>\n","      <td>0.937533</td>\n","      <td>0.937533</td>\n","      <td>0.796085</td>\n","      <td>0.764511</td>\n","      <td>0.830949</td>\n","      <td>0.779230</td>\n","      <td>0.745533</td>\n","      <td>0.816119</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.001000</td>\n","      <td>0.046392</td>\n","      <td>0.837402</td>\n","      <td>0.812766</td>\n","      <td>0.865036</td>\n","      <td>0.936539</td>\n","      <td>0.936539</td>\n","      <td>0.936539</td>\n","      <td>0.794416</td>\n","      <td>0.758418</td>\n","      <td>0.834352</td>\n","      <td>0.777221</td>\n","      <td>0.738977</td>\n","      <td>0.819640</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8377862920676975, 'eval_macro_precision': 0.8136785179950838, 'eval_macro_recall': 0.8647282006153303, 'eval_micro_f1': 0.9366555536058958, 'eval_micro_precision': 0.9366555536058958, 'eval_micro_recall': 0.9366555536058958, 'eval_macro_f1_no_o': 0.7949036892720236, 'eval_macro_precision_no_o': 0.7596780763250028, 'eval_macro_recall_no_o': 0.8338502867888516, 'eval_micro_f1_no_o': 0.7773444753946147, 'eval_micro_precision_no_o': 0.7398373983739838, 'eval_micro_recall_no_o': 0.8188575899843505, 'eval_loss': 0.046407052009271865, 'eval_runtime': 7.5987, 'eval_samples_per_second': 7.896, 'eval_steps_per_second': 1.053, 'epoch': 19.0}\n","Accuracy for fold  5 :  0.7773444753946147  --  0.9366555536058958\n","--------------------------------\n","Testing process has finished.\n","Train run #6\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 17:33, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.007300</td>\n","      <td>0.054223</td>\n","      <td>0.815368</td>\n","      <td>0.793027</td>\n","      <td>0.843382</td>\n","      <td>0.928233</td>\n","      <td>0.928233</td>\n","      <td>0.928233</td>\n","      <td>0.766500</td>\n","      <td>0.733173</td>\n","      <td>0.807314</td>\n","      <td>0.749355</td>\n","      <td>0.708362</td>\n","      <td>0.795383</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003700</td>\n","      <td>0.048421</td>\n","      <td>0.829397</td>\n","      <td>0.806527</td>\n","      <td>0.854650</td>\n","      <td>0.935252</td>\n","      <td>0.935252</td>\n","      <td>0.935252</td>\n","      <td>0.783880</td>\n","      <td>0.750993</td>\n","      <td>0.819908</td>\n","      <td>0.768826</td>\n","      <td>0.739256</td>\n","      <td>0.800861</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.002000</td>\n","      <td>0.048572</td>\n","      <td>0.824876</td>\n","      <td>0.800641</td>\n","      <td>0.852798</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.777756</td>\n","      <td>0.743447</td>\n","      <td>0.816958</td>\n","      <td>0.762571</td>\n","      <td>0.737747</td>\n","      <td>0.789124</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001400</td>\n","      <td>0.049401</td>\n","      <td>0.830656</td>\n","      <td>0.808083</td>\n","      <td>0.855558</td>\n","      <td>0.935661</td>\n","      <td>0.935661</td>\n","      <td>0.935661</td>\n","      <td>0.785519</td>\n","      <td>0.752845</td>\n","      <td>0.821256</td>\n","      <td>0.771391</td>\n","      <td>0.739677</td>\n","      <td>0.805947</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001200</td>\n","      <td>0.052002</td>\n","      <td>0.830216</td>\n","      <td>0.807744</td>\n","      <td>0.855469</td>\n","      <td>0.936597</td>\n","      <td>0.936597</td>\n","      <td>0.936597</td>\n","      <td>0.784584</td>\n","      <td>0.752442</td>\n","      <td>0.820404</td>\n","      <td>0.770449</td>\n","      <td>0.743273</td>\n","      <td>0.799687</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001000</td>\n","      <td>0.052066</td>\n","      <td>0.831253</td>\n","      <td>0.806776</td>\n","      <td>0.858881</td>\n","      <td>0.934842</td>\n","      <td>0.934842</td>\n","      <td>0.934842</td>\n","      <td>0.786388</td>\n","      <td>0.750707</td>\n","      <td>0.826215</td>\n","      <td>0.768859</td>\n","      <td>0.732130</td>\n","      <td>0.809468</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000800</td>\n","      <td>0.050831</td>\n","      <td>0.833082</td>\n","      <td>0.813314</td>\n","      <td>0.855240</td>\n","      <td>0.935603</td>\n","      <td>0.935603</td>\n","      <td>0.935603</td>\n","      <td>0.788686</td>\n","      <td>0.759683</td>\n","      <td>0.820832</td>\n","      <td>0.770151</td>\n","      <td>0.737728</td>\n","      <td>0.805556</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000800</td>\n","      <td>0.050245</td>\n","      <td>0.836432</td>\n","      <td>0.816259</td>\n","      <td>0.858782</td>\n","      <td>0.935778</td>\n","      <td>0.935778</td>\n","      <td>0.935778</td>\n","      <td>0.793215</td>\n","      <td>0.763546</td>\n","      <td>0.825739</td>\n","      <td>0.772676</td>\n","      <td>0.738758</td>\n","      <td>0.809859</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000900</td>\n","      <td>0.050148</td>\n","      <td>0.836059</td>\n","      <td>0.816904</td>\n","      <td>0.857243</td>\n","      <td>0.935837</td>\n","      <td>0.935837</td>\n","      <td>0.935837</td>\n","      <td>0.792702</td>\n","      <td>0.764472</td>\n","      <td>0.823595</td>\n","      <td>0.772566</td>\n","      <td>0.739535</td>\n","      <td>0.808685</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8360585242743643, 'eval_macro_precision': 0.8169035826344473, 'eval_macro_recall': 0.8572432886774605, 'eval_micro_f1': 0.9358366964964614, 'eval_micro_precision': 0.9358366964964614, 'eval_micro_recall': 0.9358366964964614, 'eval_macro_f1_no_o': 0.7927023779609375, 'eval_macro_precision_no_o': 0.7644716392894108, 'eval_macro_recall_no_o': 0.8235953199604752, 'eval_micro_f1_no_o': 0.7725658755372826, 'eval_micro_precision_no_o': 0.7395348837209302, 'eval_micro_recall_no_o': 0.8086854460093896, 'eval_loss': 0.05016937883683568, 'eval_runtime': 7.6886, 'eval_samples_per_second': 7.804, 'eval_steps_per_second': 1.04, 'epoch': 19.0}\n","Accuracy for fold  6 :  0.7725658755372826  --  0.9358366964964614\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #7\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 17:39, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006900</td>\n","      <td>0.051575</td>\n","      <td>0.825796</td>\n","      <td>0.790738</td>\n","      <td>0.867190</td>\n","      <td>0.931684</td>\n","      <td>0.931684</td>\n","      <td>0.931684</td>\n","      <td>0.779935</td>\n","      <td>0.728848</td>\n","      <td>0.839356</td>\n","      <td>0.766151</td>\n","      <td>0.716230</td>\n","      <td>0.823552</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003400</td>\n","      <td>0.053242</td>\n","      <td>0.830721</td>\n","      <td>0.828991</td>\n","      <td>0.833164</td>\n","      <td>0.938995</td>\n","      <td>0.938995</td>\n","      <td>0.938995</td>\n","      <td>0.784947</td>\n","      <td>0.782862</td>\n","      <td>0.787982</td>\n","      <td>0.772977</td>\n","      <td>0.776025</td>\n","      <td>0.769953</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.002000</td>\n","      <td>0.048842</td>\n","      <td>0.828576</td>\n","      <td>0.803057</td>\n","      <td>0.857311</td>\n","      <td>0.934901</td>\n","      <td>0.934901</td>\n","      <td>0.934901</td>\n","      <td>0.782779</td>\n","      <td>0.746189</td>\n","      <td>0.823617</td>\n","      <td>0.767041</td>\n","      <td>0.735632</td>\n","      <td>0.801252</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001500</td>\n","      <td>0.051902</td>\n","      <td>0.815466</td>\n","      <td>0.784826</td>\n","      <td>0.850631</td>\n","      <td>0.927590</td>\n","      <td>0.927590</td>\n","      <td>0.927590</td>\n","      <td>0.766726</td>\n","      <td>0.722289</td>\n","      <td>0.817118</td>\n","      <td>0.746961</td>\n","      <td>0.705637</td>\n","      <td>0.793427</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001300</td>\n","      <td>0.049775</td>\n","      <td>0.825777</td>\n","      <td>0.804362</td>\n","      <td>0.849827</td>\n","      <td>0.934082</td>\n","      <td>0.934082</td>\n","      <td>0.934082</td>\n","      <td>0.779122</td>\n","      <td>0.748528</td>\n","      <td>0.813203</td>\n","      <td>0.761239</td>\n","      <td>0.735939</td>\n","      <td>0.788341</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001000</td>\n","      <td>0.046926</td>\n","      <td>0.824897</td>\n","      <td>0.797500</td>\n","      <td>0.855978</td>\n","      <td>0.933263</td>\n","      <td>0.933263</td>\n","      <td>0.933263</td>\n","      <td>0.778016</td>\n","      <td>0.738498</td>\n","      <td>0.822390</td>\n","      <td>0.760275</td>\n","      <td>0.724566</td>\n","      <td>0.799687</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000800</td>\n","      <td>0.046089</td>\n","      <td>0.829899</td>\n","      <td>0.805844</td>\n","      <td>0.856613</td>\n","      <td>0.934257</td>\n","      <td>0.934257</td>\n","      <td>0.934257</td>\n","      <td>0.784598</td>\n","      <td>0.749709</td>\n","      <td>0.822984</td>\n","      <td>0.764640</td>\n","      <td>0.730577</td>\n","      <td>0.802034</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000800</td>\n","      <td>0.046065</td>\n","      <td>0.826745</td>\n","      <td>0.800796</td>\n","      <td>0.855912</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.780334</td>\n","      <td>0.742931</td>\n","      <td>0.821981</td>\n","      <td>0.763290</td>\n","      <td>0.729412</td>\n","      <td>0.800469</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000900</td>\n","      <td>0.046059</td>\n","      <td>0.828108</td>\n","      <td>0.802717</td>\n","      <td>0.856578</td>\n","      <td>0.934140</td>\n","      <td>0.934140</td>\n","      <td>0.934140</td>\n","      <td>0.782178</td>\n","      <td>0.745427</td>\n","      <td>0.822984</td>\n","      <td>0.763643</td>\n","      <td>0.728759</td>\n","      <td>0.802034</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.827797235000801, 'eval_macro_precision': 0.8021714385414571, 'eval_macro_recall': 0.8565613013340247, 'eval_micro_f1': 0.9340820026905305, 'eval_micro_precision': 0.9340820026905305, 'eval_micro_recall': 0.9340820026905305, 'eval_macro_f1_no_o': 0.7817757193849149, 'eval_macro_precision_no_o': 0.7447001943277679, 'eval_macro_recall_no_o': 0.8229840114341567, 'eval_micro_f1_no_o': 0.7635009310986965, 'eval_micro_precision_no_o': 0.728500355366027, 'eval_micro_recall_no_o': 0.8020344287949922, 'eval_loss': 0.04608009240861672, 'eval_runtime': 7.6367, 'eval_samples_per_second': 7.857, 'eval_steps_per_second': 1.048, 'epoch': 19.0}\n","Accuracy for fold  7 :  0.7635009310986965  --  0.9340820026905305\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #8\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 17:34, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006700</td>\n","      <td>0.049638</td>\n","      <td>0.812801</td>\n","      <td>0.805755</td>\n","      <td>0.824771</td>\n","      <td>0.933146</td>\n","      <td>0.933146</td>\n","      <td>0.933146</td>\n","      <td>0.761882</td>\n","      <td>0.751653</td>\n","      <td>0.778672</td>\n","      <td>0.751880</td>\n","      <td>0.741163</td>\n","      <td>0.762911</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003500</td>\n","      <td>0.052162</td>\n","      <td>0.803251</td>\n","      <td>0.762903</td>\n","      <td>0.860187</td>\n","      <td>0.917763</td>\n","      <td>0.917763</td>\n","      <td>0.917763</td>\n","      <td>0.752416</td>\n","      <td>0.691189</td>\n","      <td>0.835429</td>\n","      <td>0.730208</td>\n","      <td>0.656367</td>\n","      <td>0.822770</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001900</td>\n","      <td>0.053801</td>\n","      <td>0.830095</td>\n","      <td>0.825972</td>\n","      <td>0.836125</td>\n","      <td>0.939054</td>\n","      <td>0.939054</td>\n","      <td>0.939054</td>\n","      <td>0.783928</td>\n","      <td>0.779018</td>\n","      <td>0.791380</td>\n","      <td>0.768927</td>\n","      <td>0.777068</td>\n","      <td>0.760955</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001300</td>\n","      <td>0.048829</td>\n","      <td>0.820884</td>\n","      <td>0.781508</td>\n","      <td>0.868128</td>\n","      <td>0.927473</td>\n","      <td>0.927473</td>\n","      <td>0.927473</td>\n","      <td>0.774088</td>\n","      <td>0.716190</td>\n","      <td>0.842303</td>\n","      <td>0.754386</td>\n","      <td>0.695380</td>\n","      <td>0.824335</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001200</td>\n","      <td>0.045583</td>\n","      <td>0.820676</td>\n","      <td>0.789172</td>\n","      <td>0.857156</td>\n","      <td>0.931391</td>\n","      <td>0.931391</td>\n","      <td>0.931391</td>\n","      <td>0.772822</td>\n","      <td>0.727327</td>\n","      <td>0.824877</td>\n","      <td>0.757056</td>\n","      <td>0.716230</td>\n","      <td>0.802817</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000900</td>\n","      <td>0.047382</td>\n","      <td>0.820720</td>\n","      <td>0.786555</td>\n","      <td>0.861288</td>\n","      <td>0.929812</td>\n","      <td>0.929812</td>\n","      <td>0.929812</td>\n","      <td>0.773432</td>\n","      <td>0.723619</td>\n","      <td>0.831670</td>\n","      <td>0.758382</td>\n","      <td>0.709754</td>\n","      <td>0.814163</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.049493</td>\n","      <td>0.824026</td>\n","      <td>0.792615</td>\n","      <td>0.861033</td>\n","      <td>0.930105</td>\n","      <td>0.930105</td>\n","      <td>0.930105</td>\n","      <td>0.777826</td>\n","      <td>0.731744</td>\n","      <td>0.831261</td>\n","      <td>0.759803</td>\n","      <td>0.711650</td>\n","      <td>0.814945</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000800</td>\n","      <td>0.049925</td>\n","      <td>0.823811</td>\n","      <td>0.793772</td>\n","      <td>0.858401</td>\n","      <td>0.932327</td>\n","      <td>0.932327</td>\n","      <td>0.932327</td>\n","      <td>0.777035</td>\n","      <td>0.733529</td>\n","      <td>0.826537</td>\n","      <td>0.763381</td>\n","      <td>0.722572</td>\n","      <td>0.809077</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000900</td>\n","      <td>0.050041</td>\n","      <td>0.824538</td>\n","      <td>0.795110</td>\n","      <td>0.858316</td>\n","      <td>0.932620</td>\n","      <td>0.932620</td>\n","      <td>0.932620</td>\n","      <td>0.777933</td>\n","      <td>0.735310</td>\n","      <td>0.826286</td>\n","      <td>0.763858</td>\n","      <td>0.723739</td>\n","      <td>0.808685</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8250450281783434, 'eval_macro_precision': 0.7959715015474157, 'eval_macro_recall': 0.8583329717005701, 'eval_micro_f1': 0.9326782476457858, 'eval_micro_precision': 0.9326782476457858, 'eval_micro_recall': 0.9326782476457858, 'eval_macro_f1_no_o': 0.7786083051271743, 'eval_macro_precision_no_o': 0.7364808066221856, 'eval_macro_recall_no_o': 0.8262861097584292, 'eval_micro_f1_no_o': 0.7641404805914972, 'eval_micro_precision_no_o': 0.7242466713384723, 'eval_micro_recall_no_o': 0.8086854460093896, 'eval_loss': 0.05004023092048252, 'eval_runtime': 7.5908, 'eval_samples_per_second': 7.904, 'eval_steps_per_second': 1.054, 'epoch': 19.0}\n","Accuracy for fold  8 :  0.7641404805914972  --  0.9326782476457858\n","--------------------------------\n","Testing process has finished.\n","Train run #9\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 28:09, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006600</td>\n","      <td>0.050966</td>\n","      <td>0.814814</td>\n","      <td>0.787652</td>\n","      <td>0.845616</td>\n","      <td>0.930748</td>\n","      <td>0.930748</td>\n","      <td>0.930748</td>\n","      <td>0.765087</td>\n","      <td>0.726015</td>\n","      <td>0.808963</td>\n","      <td>0.752143</td>\n","      <td>0.718149</td>\n","      <td>0.789515</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003200</td>\n","      <td>0.053560</td>\n","      <td>0.818497</td>\n","      <td>0.798634</td>\n","      <td>0.841857</td>\n","      <td>0.932444</td>\n","      <td>0.932444</td>\n","      <td>0.932444</td>\n","      <td>0.769817</td>\n","      <td>0.741657</td>\n","      <td>0.802621</td>\n","      <td>0.755986</td>\n","      <td>0.735033</td>\n","      <td>0.778169</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.002000</td>\n","      <td>0.048120</td>\n","      <td>0.828059</td>\n","      <td>0.800011</td>\n","      <td>0.860066</td>\n","      <td>0.932035</td>\n","      <td>0.932035</td>\n","      <td>0.932035</td>\n","      <td>0.782649</td>\n","      <td>0.741600</td>\n","      <td>0.828895</td>\n","      <td>0.761362</td>\n","      <td>0.718652</td>\n","      <td>0.809468</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001300</td>\n","      <td>0.047136</td>\n","      <td>0.823766</td>\n","      <td>0.795150</td>\n","      <td>0.856696</td>\n","      <td>0.932152</td>\n","      <td>0.932152</td>\n","      <td>0.932152</td>\n","      <td>0.776864</td>\n","      <td>0.735451</td>\n","      <td>0.823967</td>\n","      <td>0.759859</td>\n","      <td>0.721265</td>\n","      <td>0.802817</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001300</td>\n","      <td>0.046737</td>\n","      <td>0.828966</td>\n","      <td>0.801764</td>\n","      <td>0.860036</td>\n","      <td>0.932620</td>\n","      <td>0.932620</td>\n","      <td>0.932620</td>\n","      <td>0.783778</td>\n","      <td>0.744135</td>\n","      <td>0.828511</td>\n","      <td>0.762890</td>\n","      <td>0.722942</td>\n","      <td>0.807512</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000900</td>\n","      <td>0.044323</td>\n","      <td>0.827772</td>\n","      <td>0.799179</td>\n","      <td>0.860338</td>\n","      <td>0.935135</td>\n","      <td>0.935135</td>\n","      <td>0.935135</td>\n","      <td>0.781586</td>\n","      <td>0.740392</td>\n","      <td>0.828020</td>\n","      <td>0.768202</td>\n","      <td>0.731259</td>\n","      <td>0.809077</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.046114</td>\n","      <td>0.817402</td>\n","      <td>0.779788</td>\n","      <td>0.862552</td>\n","      <td>0.929578</td>\n","      <td>0.929578</td>\n","      <td>0.929578</td>\n","      <td>0.768824</td>\n","      <td>0.714049</td>\n","      <td>0.833516</td>\n","      <td>0.755209</td>\n","      <td>0.703341</td>\n","      <td>0.815336</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000800</td>\n","      <td>0.045736</td>\n","      <td>0.822530</td>\n","      <td>0.793436</td>\n","      <td>0.855826</td>\n","      <td>0.932503</td>\n","      <td>0.932503</td>\n","      <td>0.932503</td>\n","      <td>0.775103</td>\n","      <td>0.732981</td>\n","      <td>0.822760</td>\n","      <td>0.760496</td>\n","      <td>0.721150</td>\n","      <td>0.804382</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000900</td>\n","      <td>0.045821</td>\n","      <td>0.823400</td>\n","      <td>0.795499</td>\n","      <td>0.855167</td>\n","      <td>0.932737</td>\n","      <td>0.932737</td>\n","      <td>0.932737</td>\n","      <td>0.776235</td>\n","      <td>0.735842</td>\n","      <td>0.821721</td>\n","      <td>0.761075</td>\n","      <td>0.723142</td>\n","      <td>0.803208</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8233569476172338, 'eval_macro_precision': 0.7951964273801523, 'eval_macro_recall': 0.8554281795025382, 'eval_micro_f1': 0.9326782476457858, 'eval_micro_precision': 0.9326782476457858, 'eval_micro_recall': 0.9326782476457858, 'eval_macro_f1_no_o': 0.7761905201456621, 'eval_macro_precision_no_o': 0.7354168195771376, 'eval_macro_recall_no_o': 0.8221150455627911, 'eval_micro_f1_no_o': 0.7610226009633198, 'eval_micro_precision_no_o': 0.7227304714989444, 'eval_micro_recall_no_o': 0.8035993740219093, 'eval_loss': 0.04583173378299155, 'eval_runtime': 24.2361, 'eval_samples_per_second': 2.476, 'eval_steps_per_second': 0.33, 'epoch': 19.0}\n","Accuracy for fold  9 :  0.7610226009633198  --  0.9326782476457858\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #10\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 32:41, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006400</td>\n","      <td>0.056077</td>\n","      <td>0.810535</td>\n","      <td>0.784736</td>\n","      <td>0.839501</td>\n","      <td>0.927063</td>\n","      <td>0.927063</td>\n","      <td>0.927063</td>\n","      <td>0.760257</td>\n","      <td>0.722816</td>\n","      <td>0.801865</td>\n","      <td>0.743452</td>\n","      <td>0.707817</td>\n","      <td>0.782864</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003400</td>\n","      <td>0.056849</td>\n","      <td>0.811881</td>\n","      <td>0.809812</td>\n","      <td>0.817484</td>\n","      <td>0.934784</td>\n","      <td>0.934784</td>\n","      <td>0.934784</td>\n","      <td>0.760199</td>\n","      <td>0.758487</td>\n","      <td>0.766617</td>\n","      <td>0.747857</td>\n","      <td>0.762292</td>\n","      <td>0.733959</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001800</td>\n","      <td>0.046872</td>\n","      <td>0.822640</td>\n","      <td>0.790655</td>\n","      <td>0.859465</td>\n","      <td>0.931099</td>\n","      <td>0.931099</td>\n","      <td>0.931099</td>\n","      <td>0.775545</td>\n","      <td>0.729109</td>\n","      <td>0.828346</td>\n","      <td>0.757848</td>\n","      <td>0.713940</td>\n","      <td>0.807512</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001300</td>\n","      <td>0.049932</td>\n","      <td>0.834802</td>\n","      <td>0.811931</td>\n","      <td>0.860363</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.934725</td>\n","      <td>0.791171</td>\n","      <td>0.757493</td>\n","      <td>0.828374</td>\n","      <td>0.769373</td>\n","      <td>0.731149</td>\n","      <td>0.811815</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001300</td>\n","      <td>0.050955</td>\n","      <td>0.824161</td>\n","      <td>0.790126</td>\n","      <td>0.864703</td>\n","      <td>0.929695</td>\n","      <td>0.929695</td>\n","      <td>0.929695</td>\n","      <td>0.777738</td>\n","      <td>0.727513</td>\n","      <td>0.836499</td>\n","      <td>0.755146</td>\n","      <td>0.701207</td>\n","      <td>0.818075</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000900</td>\n","      <td>0.053755</td>\n","      <td>0.836935</td>\n","      <td>0.824803</td>\n","      <td>0.849812</td>\n","      <td>0.939054</td>\n","      <td>0.939054</td>\n","      <td>0.939054</td>\n","      <td>0.792962</td>\n","      <td>0.775791</td>\n","      <td>0.811120</td>\n","      <td>0.772928</td>\n","      <td>0.759924</td>\n","      <td>0.786385</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.048963</td>\n","      <td>0.836274</td>\n","      <td>0.810512</td>\n","      <td>0.865049</td>\n","      <td>0.936071</td>\n","      <td>0.936071</td>\n","      <td>0.936071</td>\n","      <td>0.792970</td>\n","      <td>0.755460</td>\n","      <td>0.834438</td>\n","      <td>0.775223</td>\n","      <td>0.736953</td>\n","      <td>0.817684</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000800</td>\n","      <td>0.049130</td>\n","      <td>0.833696</td>\n","      <td>0.806303</td>\n","      <td>0.864592</td>\n","      <td>0.935018</td>\n","      <td>0.935018</td>\n","      <td>0.935018</td>\n","      <td>0.789611</td>\n","      <td>0.749673</td>\n","      <td>0.834151</td>\n","      <td>0.770595</td>\n","      <td>0.729881</td>\n","      <td>0.816119</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.049319</td>\n","      <td>0.833603</td>\n","      <td>0.806040</td>\n","      <td>0.864688</td>\n","      <td>0.935193</td>\n","      <td>0.935193</td>\n","      <td>0.935193</td>\n","      <td>0.789451</td>\n","      <td>0.749344</td>\n","      <td>0.834186</td>\n","      <td>0.770937</td>\n","      <td>0.730810</td>\n","      <td>0.815728</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8348200370418951, 'eval_macro_precision': 0.8079596441188632, 'eval_macro_recall': 0.8650932111524476, 'eval_micro_f1': 0.9351933087676201, 'eval_micro_precision': 0.9351933087676201, 'eval_micro_recall': 0.9351933087676201, 'eval_macro_f1_no_o': 0.7911097355356276, 'eval_macro_precision_no_o': 0.7519044347912581, 'eval_macro_recall_no_o': 0.8347954412459245, 'eval_micro_f1_no_o': 0.771618625277162, 'eval_micro_precision_no_o': 0.7310924369747899, 'eval_micro_recall_no_o': 0.8169014084507042, 'eval_loss': 0.04931900623396359, 'eval_runtime': 24.6059, 'eval_samples_per_second': 2.438, 'eval_steps_per_second': 0.325, 'epoch': 19.0}\n","Accuracy for fold  10 :  0.771618625277162  --  0.9351933087676201\n","--------------------------------\n","Testing process has finished.\n","Train run #11\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:14, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006400</td>\n","      <td>0.049282</td>\n","      <td>0.813763</td>\n","      <td>0.774052</td>\n","      <td>0.863133</td>\n","      <td>0.930397</td>\n","      <td>0.930397</td>\n","      <td>0.930397</td>\n","      <td>0.763719</td>\n","      <td>0.706367</td>\n","      <td>0.833833</td>\n","      <td>0.755636</td>\n","      <td>0.705842</td>\n","      <td>0.812989</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003200</td>\n","      <td>0.048612</td>\n","      <td>0.808100</td>\n","      <td>0.778054</td>\n","      <td>0.847303</td>\n","      <td>0.929227</td>\n","      <td>0.929227</td>\n","      <td>0.929227</td>\n","      <td>0.756152</td>\n","      <td>0.712797</td>\n","      <td>0.811648</td>\n","      <td>0.744264</td>\n","      <td>0.706110</td>\n","      <td>0.786776</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001900</td>\n","      <td>0.043414</td>\n","      <td>0.816210</td>\n","      <td>0.791548</td>\n","      <td>0.855841</td>\n","      <td>0.928058</td>\n","      <td>0.928058</td>\n","      <td>0.928058</td>\n","      <td>0.767463</td>\n","      <td>0.729446</td>\n","      <td>0.825279</td>\n","      <td>0.751034</td>\n","      <td>0.694712</td>\n","      <td>0.817293</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001300</td>\n","      <td>0.043379</td>\n","      <td>0.813574</td>\n","      <td>0.766836</td>\n","      <td>0.873507</td>\n","      <td>0.924841</td>\n","      <td>0.924841</td>\n","      <td>0.924841</td>\n","      <td>0.764472</td>\n","      <td>0.694890</td>\n","      <td>0.851332</td>\n","      <td>0.746300</td>\n","      <td>0.672419</td>\n","      <td>0.838419</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001100</td>\n","      <td>0.046803</td>\n","      <td>0.833768</td>\n","      <td>0.820582</td>\n","      <td>0.848731</td>\n","      <td>0.939288</td>\n","      <td>0.939288</td>\n","      <td>0.939288</td>\n","      <td>0.788760</td>\n","      <td>0.769881</td>\n","      <td>0.809999</td>\n","      <td>0.775822</td>\n","      <td>0.758982</td>\n","      <td>0.793427</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000900</td>\n","      <td>0.046619</td>\n","      <td>0.832116</td>\n","      <td>0.809145</td>\n","      <td>0.857646</td>\n","      <td>0.936890</td>\n","      <td>0.936890</td>\n","      <td>0.936890</td>\n","      <td>0.787020</td>\n","      <td>0.754016</td>\n","      <td>0.823399</td>\n","      <td>0.771370</td>\n","      <td>0.741959</td>\n","      <td>0.803208</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.047580</td>\n","      <td>0.835190</td>\n","      <td>0.809475</td>\n","      <td>0.863870</td>\n","      <td>0.936071</td>\n","      <td>0.936071</td>\n","      <td>0.936071</td>\n","      <td>0.791348</td>\n","      <td>0.754025</td>\n","      <td>0.832569</td>\n","      <td>0.771975</td>\n","      <td>0.735221</td>\n","      <td>0.812598</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000800</td>\n","      <td>0.046976</td>\n","      <td>0.830445</td>\n","      <td>0.808322</td>\n","      <td>0.855023</td>\n","      <td>0.935369</td>\n","      <td>0.935369</td>\n","      <td>0.935369</td>\n","      <td>0.785010</td>\n","      <td>0.752797</td>\n","      <td>0.820452</td>\n","      <td>0.766299</td>\n","      <td>0.733286</td>\n","      <td>0.802426</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000900</td>\n","      <td>0.047331</td>\n","      <td>0.830399</td>\n","      <td>0.808716</td>\n","      <td>0.854407</td>\n","      <td>0.935661</td>\n","      <td>0.935661</td>\n","      <td>0.935661</td>\n","      <td>0.784888</td>\n","      <td>0.753364</td>\n","      <td>0.819470</td>\n","      <td>0.766841</td>\n","      <td>0.734935</td>\n","      <td>0.801643</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8308813504396488, 'eval_macro_precision': 0.8096111442600908, 'eval_macro_recall': 0.8543897146370892, 'eval_micro_f1': 0.9356027373223372, 'eval_micro_precision': 0.9356027373223372, 'eval_micro_recall': 0.9356027373223372, 'eval_macro_f1_no_o': 0.785542934074361, 'eval_macro_precision_no_o': 0.7545584057472166, 'eval_macro_recall_no_o': 0.819469622954672, 'eval_micro_f1_no_o': 0.7666978484565014, 'eval_micro_precision_no_o': 0.734671925421298, 'eval_micro_recall_no_o': 0.8016431924882629, 'eval_loss': 0.047293329240771224, 'eval_runtime': 25.8018, 'eval_samples_per_second': 2.325, 'eval_steps_per_second': 0.31, 'epoch': 19.0}\n","Accuracy for fold  11 :  0.7666978484565014  --  0.9356027373223372\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #12\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:13, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006200</td>\n","      <td>0.047959</td>\n","      <td>0.816046</td>\n","      <td>0.781353</td>\n","      <td>0.856860</td>\n","      <td>0.928467</td>\n","      <td>0.928467</td>\n","      <td>0.928467</td>\n","      <td>0.767582</td>\n","      <td>0.716769</td>\n","      <td>0.826432</td>\n","      <td>0.756982</td>\n","      <td>0.705544</td>\n","      <td>0.816510</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003000</td>\n","      <td>0.047459</td>\n","      <td>0.818239</td>\n","      <td>0.808781</td>\n","      <td>0.829330</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.768863</td>\n","      <td>0.754890</td>\n","      <td>0.785003</td>\n","      <td>0.756209</td>\n","      <td>0.738984</td>\n","      <td>0.774257</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001700</td>\n","      <td>0.043843</td>\n","      <td>0.829615</td>\n","      <td>0.812049</td>\n","      <td>0.849334</td>\n","      <td>0.936188</td>\n","      <td>0.936188</td>\n","      <td>0.936188</td>\n","      <td>0.783795</td>\n","      <td>0.757784</td>\n","      <td>0.812637</td>\n","      <td>0.769375</td>\n","      <td>0.737617</td>\n","      <td>0.803991</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001200</td>\n","      <td>0.045225</td>\n","      <td>0.829981</td>\n","      <td>0.822078</td>\n","      <td>0.840317</td>\n","      <td>0.937299</td>\n","      <td>0.937299</td>\n","      <td>0.937299</td>\n","      <td>0.784078</td>\n","      <td>0.771939</td>\n","      <td>0.799445</td>\n","      <td>0.769933</td>\n","      <td>0.749537</td>\n","      <td>0.791471</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001100</td>\n","      <td>0.046760</td>\n","      <td>0.827146</td>\n","      <td>0.809345</td>\n","      <td>0.846822</td>\n","      <td>0.934024</td>\n","      <td>0.934024</td>\n","      <td>0.934024</td>\n","      <td>0.780899</td>\n","      <td>0.754486</td>\n","      <td>0.809770</td>\n","      <td>0.762243</td>\n","      <td>0.729778</td>\n","      <td>0.797731</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.041632</td>\n","      <td>0.824615</td>\n","      <td>0.795707</td>\n","      <td>0.857951</td>\n","      <td>0.931567</td>\n","      <td>0.931567</td>\n","      <td>0.931567</td>\n","      <td>0.778018</td>\n","      <td>0.735300</td>\n","      <td>0.826534</td>\n","      <td>0.759489</td>\n","      <td>0.711696</td>\n","      <td>0.814163</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.047341</td>\n","      <td>0.832257</td>\n","      <td>0.807415</td>\n","      <td>0.860572</td>\n","      <td>0.933965</td>\n","      <td>0.933965</td>\n","      <td>0.933965</td>\n","      <td>0.787957</td>\n","      <td>0.751146</td>\n","      <td>0.829318</td>\n","      <td>0.769033</td>\n","      <td>0.725538</td>\n","      <td>0.818075</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000800</td>\n","      <td>0.047565</td>\n","      <td>0.829905</td>\n","      <td>0.801526</td>\n","      <td>0.862418</td>\n","      <td>0.933205</td>\n","      <td>0.933205</td>\n","      <td>0.933205</td>\n","      <td>0.784971</td>\n","      <td>0.743144</td>\n","      <td>0.832214</td>\n","      <td>0.767570</td>\n","      <td>0.721114</td>\n","      <td>0.820423</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.047431</td>\n","      <td>0.831491</td>\n","      <td>0.805336</td>\n","      <td>0.861233</td>\n","      <td>0.933907</td>\n","      <td>0.933907</td>\n","      <td>0.933907</td>\n","      <td>0.786949</td>\n","      <td>0.748352</td>\n","      <td>0.830244</td>\n","      <td>0.768976</td>\n","      <td>0.725130</td>\n","      <td>0.818466</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8319623428555722, 'eval_macro_precision': 0.8061869372705883, 'eval_macro_recall': 0.861266999638754, 'eval_micro_f1': 0.9340235128969995, 'eval_micro_precision': 0.9340235128969995, 'eval_micro_recall': 0.9340235128969995, 'eval_macro_f1_no_o': 0.7875535695270148, 'eval_macro_precision_no_o': 0.749485589453041, 'eval_macro_recall_no_o': 0.8302439943834327, 'eval_micro_f1_no_o': 0.7692590549733406, 'eval_micro_precision_no_o': 0.7256330211585155, 'eval_micro_recall_no_o': 0.8184663536776213, 'eval_loss': 0.047421609295755236, 'eval_runtime': 24.9085, 'eval_samples_per_second': 2.409, 'eval_steps_per_second': 0.321, 'epoch': 19.0}\n","Accuracy for fold  12 :  0.7692590549733406  --  0.9340235128969995\n","--------------------------------\n","Testing process has finished.\n","Train run #13\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:05, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006300</td>\n","      <td>0.053713</td>\n","      <td>0.822650</td>\n","      <td>0.811011</td>\n","      <td>0.836206</td>\n","      <td>0.937182</td>\n","      <td>0.937182</td>\n","      <td>0.937182</td>\n","      <td>0.774336</td>\n","      <td>0.758317</td>\n","      <td>0.792910</td>\n","      <td>0.765949</td>\n","      <td>0.759323</td>\n","      <td>0.772692</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003100</td>\n","      <td>0.051988</td>\n","      <td>0.828625</td>\n","      <td>0.809341</td>\n","      <td>0.850427</td>\n","      <td>0.936129</td>\n","      <td>0.936129</td>\n","      <td>0.936129</td>\n","      <td>0.782637</td>\n","      <td>0.755019</td>\n","      <td>0.813590</td>\n","      <td>0.769406</td>\n","      <td>0.745415</td>\n","      <td>0.794992</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001800</td>\n","      <td>0.046440</td>\n","      <td>0.831026</td>\n","      <td>0.806030</td>\n","      <td>0.858974</td>\n","      <td>0.935135</td>\n","      <td>0.935135</td>\n","      <td>0.935135</td>\n","      <td>0.786129</td>\n","      <td>0.749848</td>\n","      <td>0.826292</td>\n","      <td>0.771120</td>\n","      <td>0.735273</td>\n","      <td>0.810642</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001300</td>\n","      <td>0.047160</td>\n","      <td>0.830774</td>\n","      <td>0.814648</td>\n","      <td>0.848137</td>\n","      <td>0.936714</td>\n","      <td>0.936714</td>\n","      <td>0.936714</td>\n","      <td>0.785258</td>\n","      <td>0.761907</td>\n","      <td>0.810239</td>\n","      <td>0.768998</td>\n","      <td>0.745682</td>\n","      <td>0.793818</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001200</td>\n","      <td>0.045834</td>\n","      <td>0.818484</td>\n","      <td>0.791618</td>\n","      <td>0.849578</td>\n","      <td>0.931625</td>\n","      <td>0.931625</td>\n","      <td>0.931625</td>\n","      <td>0.769751</td>\n","      <td>0.730692</td>\n","      <td>0.814384</td>\n","      <td>0.755325</td>\n","      <td>0.717200</td>\n","      <td>0.797731</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.046396</td>\n","      <td>0.825517</td>\n","      <td>0.804153</td>\n","      <td>0.849329</td>\n","      <td>0.932678</td>\n","      <td>0.932678</td>\n","      <td>0.932678</td>\n","      <td>0.779115</td>\n","      <td>0.747782</td>\n","      <td>0.813662</td>\n","      <td>0.760485</td>\n","      <td>0.726237</td>\n","      <td>0.798122</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.044928</td>\n","      <td>0.826200</td>\n","      <td>0.809875</td>\n","      <td>0.843886</td>\n","      <td>0.935427</td>\n","      <td>0.935427</td>\n","      <td>0.935427</td>\n","      <td>0.779470</td>\n","      <td>0.755888</td>\n","      <td>0.804845</td>\n","      <td>0.765643</td>\n","      <td>0.742826</td>\n","      <td>0.789906</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.044873</td>\n","      <td>0.828971</td>\n","      <td>0.813583</td>\n","      <td>0.845446</td>\n","      <td>0.935486</td>\n","      <td>0.935486</td>\n","      <td>0.935486</td>\n","      <td>0.783123</td>\n","      <td>0.760677</td>\n","      <td>0.806994</td>\n","      <td>0.765708</td>\n","      <td>0.741569</td>\n","      <td>0.791471</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.045037</td>\n","      <td>0.825535</td>\n","      <td>0.807478</td>\n","      <td>0.845034</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.778662</td>\n","      <td>0.752477</td>\n","      <td>0.806743</td>\n","      <td>0.763019</td>\n","      <td>0.736880</td>\n","      <td>0.791080</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8255348038212139, 'eval_macro_precision': 0.8074775357845079, 'eval_macro_recall': 0.8450340559370555, 'eval_micro_f1': 0.9346669006258408, 'eval_micro_precision': 0.9346669006258408, 'eval_micro_recall': 0.9346669006258408, 'eval_macro_f1_no_o': 0.7786623576571907, 'eval_macro_precision_no_o': 0.752476840717624, 'eval_macro_recall_no_o': 0.806743250797123, 'eval_micro_f1_no_o': 0.7630188679245282, 'eval_micro_precision_no_o': 0.7368804664723032, 'eval_micro_recall_no_o': 0.7910798122065728, 'eval_loss': 0.045041736842479925, 'eval_runtime': 25.5309, 'eval_samples_per_second': 2.35, 'eval_steps_per_second': 0.313, 'epoch': 19.0}\n","Accuracy for fold  13 :  0.7630188679245282  --  0.9346669006258408\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #14\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:20, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006100</td>\n","      <td>0.046905</td>\n","      <td>0.822540</td>\n","      <td>0.798394</td>\n","      <td>0.850532</td>\n","      <td>0.936129</td>\n","      <td>0.936129</td>\n","      <td>0.936129</td>\n","      <td>0.774267</td>\n","      <td>0.739927</td>\n","      <td>0.813707</td>\n","      <td>0.765982</td>\n","      <td>0.739352</td>\n","      <td>0.794601</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003000</td>\n","      <td>0.051907</td>\n","      <td>0.833396</td>\n","      <td>0.825000</td>\n","      <td>0.842351</td>\n","      <td>0.939112</td>\n","      <td>0.939112</td>\n","      <td>0.939112</td>\n","      <td>0.788455</td>\n","      <td>0.776804</td>\n","      <td>0.800851</td>\n","      <td>0.775082</td>\n","      <td>0.768964</td>\n","      <td>0.781299</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001800</td>\n","      <td>0.055328</td>\n","      <td>0.816820</td>\n","      <td>0.813527</td>\n","      <td>0.823477</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.767127</td>\n","      <td>0.763124</td>\n","      <td>0.775617</td>\n","      <td>0.755564</td>\n","      <td>0.760809</td>\n","      <td>0.750391</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001200</td>\n","      <td>0.051271</td>\n","      <td>0.831599</td>\n","      <td>0.819908</td>\n","      <td>0.844952</td>\n","      <td>0.936188</td>\n","      <td>0.936188</td>\n","      <td>0.936188</td>\n","      <td>0.786674</td>\n","      <td>0.769769</td>\n","      <td>0.805786</td>\n","      <td>0.769025</td>\n","      <td>0.752057</td>\n","      <td>0.786776</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001100</td>\n","      <td>0.052922</td>\n","      <td>0.827866</td>\n","      <td>0.816305</td>\n","      <td>0.840215</td>\n","      <td>0.935837</td>\n","      <td>0.935837</td>\n","      <td>0.935837</td>\n","      <td>0.781569</td>\n","      <td>0.764892</td>\n","      <td>0.799287</td>\n","      <td>0.764402</td>\n","      <td>0.748220</td>\n","      <td>0.781299</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.055762</td>\n","      <td>0.833306</td>\n","      <td>0.826973</td>\n","      <td>0.840291</td>\n","      <td>0.937533</td>\n","      <td>0.937533</td>\n","      <td>0.937533</td>\n","      <td>0.788761</td>\n","      <td>0.779504</td>\n","      <td>0.798884</td>\n","      <td>0.772999</td>\n","      <td>0.762267</td>\n","      <td>0.784038</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.048795</td>\n","      <td>0.829778</td>\n","      <td>0.808424</td>\n","      <td>0.853909</td>\n","      <td>0.934608</td>\n","      <td>0.934608</td>\n","      <td>0.934608</td>\n","      <td>0.784605</td>\n","      <td>0.753445</td>\n","      <td>0.819425</td>\n","      <td>0.769202</td>\n","      <td>0.736315</td>\n","      <td>0.805164</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.054243</td>\n","      <td>0.830235</td>\n","      <td>0.817462</td>\n","      <td>0.844252</td>\n","      <td>0.935895</td>\n","      <td>0.935895</td>\n","      <td>0.935895</td>\n","      <td>0.784880</td>\n","      <td>0.766182</td>\n","      <td>0.805219</td>\n","      <td>0.768675</td>\n","      <td>0.747505</td>\n","      <td>0.791080</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.053602</td>\n","      <td>0.830203</td>\n","      <td>0.816415</td>\n","      <td>0.845387</td>\n","      <td>0.935837</td>\n","      <td>0.935837</td>\n","      <td>0.935837</td>\n","      <td>0.784863</td>\n","      <td>0.764721</td>\n","      <td>0.806847</td>\n","      <td>0.769026</td>\n","      <td>0.746775</td>\n","      <td>0.792645</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8306192060578103, 'eval_macro_precision': 0.8173218203738574, 'eval_macro_recall': 0.8452674048780929, 'eval_micro_f1': 0.9360121658770545, 'eval_micro_precision': 0.9360121658770545, 'eval_micro_recall': 0.9360121658770545, 'eval_macro_f1_no_o': 0.7853702979499876, 'eval_macro_precision_no_o': 0.7659277501194569, 'eval_macro_recall_no_o': 0.8065959089775898, 'eval_micro_f1_no_o': 0.7692307692307693, 'eval_micro_precision_no_o': 0.7475083056478405, 'eval_micro_recall_no_o': 0.7922535211267606, 'eval_loss': 0.0536465163865311, 'eval_runtime': 24.9552, 'eval_samples_per_second': 2.404, 'eval_steps_per_second': 0.321, 'epoch': 19.0}\n","Accuracy for fold  14 :  0.7692307692307693  --  0.9360121658770545\n","--------------------------------\n","Testing process has finished.\n","Train run #15\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:39, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006200</td>\n","      <td>0.046887</td>\n","      <td>0.818998</td>\n","      <td>0.795094</td>\n","      <td>0.847347</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.770009</td>\n","      <td>0.735880</td>\n","      <td>0.810033</td>\n","      <td>0.761528</td>\n","      <td>0.733769</td>\n","      <td>0.791471</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003000</td>\n","      <td>0.043721</td>\n","      <td>0.815074</td>\n","      <td>0.779409</td>\n","      <td>0.857253</td>\n","      <td>0.929988</td>\n","      <td>0.929988</td>\n","      <td>0.929988</td>\n","      <td>0.765619</td>\n","      <td>0.713629</td>\n","      <td>0.826176</td>\n","      <td>0.755588</td>\n","      <td>0.705463</td>\n","      <td>0.813380</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001800</td>\n","      <td>0.049236</td>\n","      <td>0.810025</td>\n","      <td>0.775620</td>\n","      <td>0.850549</td>\n","      <td>0.926010</td>\n","      <td>0.926010</td>\n","      <td>0.926010</td>\n","      <td>0.759938</td>\n","      <td>0.709816</td>\n","      <td>0.818108</td>\n","      <td>0.746720</td>\n","      <td>0.698840</td>\n","      <td>0.801643</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001200</td>\n","      <td>0.045614</td>\n","      <td>0.824465</td>\n","      <td>0.800999</td>\n","      <td>0.852140</td>\n","      <td>0.932152</td>\n","      <td>0.932152</td>\n","      <td>0.932152</td>\n","      <td>0.777901</td>\n","      <td>0.743365</td>\n","      <td>0.817983</td>\n","      <td>0.761481</td>\n","      <td>0.722925</td>\n","      <td>0.804382</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001100</td>\n","      <td>0.051377</td>\n","      <td>0.819124</td>\n","      <td>0.791443</td>\n","      <td>0.851063</td>\n","      <td>0.931801</td>\n","      <td>0.931801</td>\n","      <td>0.931801</td>\n","      <td>0.770730</td>\n","      <td>0.730975</td>\n","      <td>0.816111</td>\n","      <td>0.757130</td>\n","      <td>0.723033</td>\n","      <td>0.794601</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.053133</td>\n","      <td>0.824339</td>\n","      <td>0.809888</td>\n","      <td>0.841773</td>\n","      <td>0.935252</td>\n","      <td>0.935252</td>\n","      <td>0.935252</td>\n","      <td>0.777101</td>\n","      <td>0.756459</td>\n","      <td>0.801708</td>\n","      <td>0.764852</td>\n","      <td>0.747294</td>\n","      <td>0.783255</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.050377</td>\n","      <td>0.824764</td>\n","      <td>0.806284</td>\n","      <td>0.845923</td>\n","      <td>0.933439</td>\n","      <td>0.933439</td>\n","      <td>0.933439</td>\n","      <td>0.778088</td>\n","      <td>0.751240</td>\n","      <td>0.808479</td>\n","      <td>0.762856</td>\n","      <td>0.735561</td>\n","      <td>0.792254</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.046813</td>\n","      <td>0.824489</td>\n","      <td>0.801288</td>\n","      <td>0.851063</td>\n","      <td>0.932327</td>\n","      <td>0.932327</td>\n","      <td>0.932327</td>\n","      <td>0.777921</td>\n","      <td>0.744061</td>\n","      <td>0.816227</td>\n","      <td>0.761355</td>\n","      <td>0.726207</td>\n","      <td>0.800078</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.047007</td>\n","      <td>0.827015</td>\n","      <td>0.805624</td>\n","      <td>0.851274</td>\n","      <td>0.932678</td>\n","      <td>0.932678</td>\n","      <td>0.932678</td>\n","      <td>0.781262</td>\n","      <td>0.749929</td>\n","      <td>0.816370</td>\n","      <td>0.762775</td>\n","      <td>0.728795</td>\n","      <td>0.800078</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8269228905077031, 'eval_macro_precision': 0.8059806136961307, 'eval_macro_recall': 0.8506349420558583, 'eval_micro_f1': 0.9326782476457858, 'eval_micro_precision': 0.9326782476457858, 'eval_micro_recall': 0.9326782476457858, 'eval_macro_f1_no_o': 0.7811377598159167, 'eval_macro_precision_no_o': 0.7504261677332771, 'eval_macro_recall_no_o': 0.815494825430093, 'eval_micro_f1_no_o': 0.7626865671641793, 'eval_micro_precision_no_o': 0.7289586305278174, 'eval_micro_recall_no_o': 0.7996870109546166, 'eval_loss': 0.04701633599712902, 'eval_runtime': 25.5272, 'eval_samples_per_second': 2.35, 'eval_steps_per_second': 0.313, 'epoch': 19.0}\n","Accuracy for fold  15 :  0.7626865671641793  --  0.9326782476457858\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #16\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:42, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006100</td>\n","      <td>0.046739</td>\n","      <td>0.813564</td>\n","      <td>0.797087</td>\n","      <td>0.836429</td>\n","      <td>0.933848</td>\n","      <td>0.933848</td>\n","      <td>0.933848</td>\n","      <td>0.762521</td>\n","      <td>0.738635</td>\n","      <td>0.794904</td>\n","      <td>0.754117</td>\n","      <td>0.730473</td>\n","      <td>0.779343</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002800</td>\n","      <td>0.046601</td>\n","      <td>0.821898</td>\n","      <td>0.811280</td>\n","      <td>0.833048</td>\n","      <td>0.935369</td>\n","      <td>0.935369</td>\n","      <td>0.935369</td>\n","      <td>0.773509</td>\n","      <td>0.758404</td>\n","      <td>0.789318</td>\n","      <td>0.758515</td>\n","      <td>0.746308</td>\n","      <td>0.771127</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001700</td>\n","      <td>0.047040</td>\n","      <td>0.815196</td>\n","      <td>0.814065</td>\n","      <td>0.819513</td>\n","      <td>0.934550</td>\n","      <td>0.934550</td>\n","      <td>0.934550</td>\n","      <td>0.764751</td>\n","      <td>0.763142</td>\n","      <td>0.770606</td>\n","      <td>0.752978</td>\n","      <td>0.751657</td>\n","      <td>0.754304</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001200</td>\n","      <td>0.050044</td>\n","      <td>0.817894</td>\n","      <td>0.807439</td>\n","      <td>0.831099</td>\n","      <td>0.934316</td>\n","      <td>0.934316</td>\n","      <td>0.934316</td>\n","      <td>0.768404</td>\n","      <td>0.753405</td>\n","      <td>0.787063</td>\n","      <td>0.755906</td>\n","      <td>0.742361</td>\n","      <td>0.769953</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001100</td>\n","      <td>0.047154</td>\n","      <td>0.818606</td>\n","      <td>0.796499</td>\n","      <td>0.844358</td>\n","      <td>0.932737</td>\n","      <td>0.932737</td>\n","      <td>0.932737</td>\n","      <td>0.769627</td>\n","      <td>0.737496</td>\n","      <td>0.806576</td>\n","      <td>0.755797</td>\n","      <td>0.723854</td>\n","      <td>0.790689</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.049310</td>\n","      <td>0.814453</td>\n","      <td>0.794543</td>\n","      <td>0.837103</td>\n","      <td>0.930807</td>\n","      <td>0.930807</td>\n","      <td>0.930807</td>\n","      <td>0.764397</td>\n","      <td>0.735038</td>\n","      <td>0.797361</td>\n","      <td>0.748974</td>\n","      <td>0.715609</td>\n","      <td>0.785603</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.049922</td>\n","      <td>0.819302</td>\n","      <td>0.796893</td>\n","      <td>0.844364</td>\n","      <td>0.931158</td>\n","      <td>0.931158</td>\n","      <td>0.931158</td>\n","      <td>0.770894</td>\n","      <td>0.737951</td>\n","      <td>0.807317</td>\n","      <td>0.752600</td>\n","      <td>0.716407</td>\n","      <td>0.792645</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.048770</td>\n","      <td>0.821436</td>\n","      <td>0.799096</td>\n","      <td>0.846989</td>\n","      <td>0.930573</td>\n","      <td>0.930573</td>\n","      <td>0.930573</td>\n","      <td>0.773914</td>\n","      <td>0.740698</td>\n","      <td>0.811345</td>\n","      <td>0.752954</td>\n","      <td>0.712937</td>\n","      <td>0.797731</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000900</td>\n","      <td>0.048910</td>\n","      <td>0.820959</td>\n","      <td>0.798895</td>\n","      <td>0.845987</td>\n","      <td>0.930865</td>\n","      <td>0.930865</td>\n","      <td>0.930865</td>\n","      <td>0.773205</td>\n","      <td>0.740470</td>\n","      <td>0.809825</td>\n","      <td>0.753237</td>\n","      <td>0.714386</td>\n","      <td>0.796557</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8209589773839474, 'eval_macro_precision': 0.7988951534487332, 'eval_macro_recall': 0.8459871958096417, 'eval_micro_f1': 0.930865064046324, 'eval_micro_precision': 0.930865064046324, 'eval_micro_recall': 0.930865064046324, 'eval_macro_f1_no_o': 0.7732049020855484, 'eval_macro_precision_no_o': 0.7404702043643435, 'eval_macro_recall_no_o': 0.8098250752371913, 'eval_micro_f1_no_o': 0.7532371439141694, 'eval_micro_precision_no_o': 0.7143859649122807, 'eval_micro_recall_no_o': 0.7965571205007824, 'eval_loss': 0.04891428932478448, 'eval_runtime': 26.0677, 'eval_samples_per_second': 2.302, 'eval_steps_per_second': 0.307, 'epoch': 19.0}\n","Accuracy for fold  16 :  0.7532371439141694  --  0.930865064046324\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #17\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:45, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006000</td>\n","      <td>0.046709</td>\n","      <td>0.816328</td>\n","      <td>0.802260</td>\n","      <td>0.831598</td>\n","      <td>0.934024</td>\n","      <td>0.934024</td>\n","      <td>0.934024</td>\n","      <td>0.766433</td>\n","      <td>0.746245</td>\n","      <td>0.788210</td>\n","      <td>0.757252</td>\n","      <td>0.739195</td>\n","      <td>0.776213</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002900</td>\n","      <td>0.043730</td>\n","      <td>0.829278</td>\n","      <td>0.810126</td>\n","      <td>0.850076</td>\n","      <td>0.936597</td>\n","      <td>0.936597</td>\n","      <td>0.936597</td>\n","      <td>0.783193</td>\n","      <td>0.755500</td>\n","      <td>0.813053</td>\n","      <td>0.768100</td>\n","      <td>0.741266</td>\n","      <td>0.796948</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001700</td>\n","      <td>0.046274</td>\n","      <td>0.827765</td>\n","      <td>0.810611</td>\n","      <td>0.846367</td>\n","      <td>0.934784</td>\n","      <td>0.934784</td>\n","      <td>0.934784</td>\n","      <td>0.781554</td>\n","      <td>0.756606</td>\n","      <td>0.808406</td>\n","      <td>0.762318</td>\n","      <td>0.736592</td>\n","      <td>0.789906</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001200</td>\n","      <td>0.047739</td>\n","      <td>0.829418</td>\n","      <td>0.813201</td>\n","      <td>0.846950</td>\n","      <td>0.935603</td>\n","      <td>0.935603</td>\n","      <td>0.935603</td>\n","      <td>0.783671</td>\n","      <td>0.760165</td>\n","      <td>0.808908</td>\n","      <td>0.765530</td>\n","      <td>0.741924</td>\n","      <td>0.790689</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001100</td>\n","      <td>0.047742</td>\n","      <td>0.828589</td>\n","      <td>0.803085</td>\n","      <td>0.857179</td>\n","      <td>0.933088</td>\n","      <td>0.933088</td>\n","      <td>0.933088</td>\n","      <td>0.783040</td>\n","      <td>0.745795</td>\n","      <td>0.824335</td>\n","      <td>0.761623</td>\n","      <td>0.723180</td>\n","      <td>0.804382</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000900</td>\n","      <td>0.049570</td>\n","      <td>0.825053</td>\n","      <td>0.806539</td>\n","      <td>0.845953</td>\n","      <td>0.934608</td>\n","      <td>0.934608</td>\n","      <td>0.934608</td>\n","      <td>0.777937</td>\n","      <td>0.751550</td>\n","      <td>0.807487</td>\n","      <td>0.759878</td>\n","      <td>0.738552</td>\n","      <td>0.782473</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.048092</td>\n","      <td>0.830849</td>\n","      <td>0.813279</td>\n","      <td>0.850068</td>\n","      <td>0.934608</td>\n","      <td>0.934608</td>\n","      <td>0.934608</td>\n","      <td>0.785744</td>\n","      <td>0.759992</td>\n","      <td>0.813661</td>\n","      <td>0.763301</td>\n","      <td>0.734709</td>\n","      <td>0.794210</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.047926</td>\n","      <td>0.830330</td>\n","      <td>0.811144</td>\n","      <td>0.851527</td>\n","      <td>0.934491</td>\n","      <td>0.934491</td>\n","      <td>0.934491</td>\n","      <td>0.785080</td>\n","      <td>0.757014</td>\n","      <td>0.815790</td>\n","      <td>0.763548</td>\n","      <td>0.733165</td>\n","      <td>0.796557</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000900</td>\n","      <td>0.047857</td>\n","      <td>0.830092</td>\n","      <td>0.810254</td>\n","      <td>0.852061</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.784836</td>\n","      <td>0.755787</td>\n","      <td>0.816686</td>\n","      <td>0.763242</td>\n","      <td>0.731611</td>\n","      <td>0.797731</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8301347496148387, 'eval_macro_precision': 0.810250367337018, 'eval_macro_recall': 0.852180458771282, 'eval_micro_f1': 0.9340235128969995, 'eval_micro_precision': 0.9340235128969995, 'eval_micro_recall': 0.9340235128969995, 'eval_macro_f1_no_o': 0.7849411087063589, 'eval_macro_precision_no_o': 0.7557846828951377, 'eval_macro_recall_no_o': 0.8169365748337539, 'eval_micro_f1_no_o': 0.7630446979614737, 'eval_micro_precision_no_o': 0.7309208169115012, 'eval_micro_recall_no_o': 0.7981220657276995, 'eval_loss': 0.04780777345761938, 'eval_runtime': 25.2254, 'eval_samples_per_second': 2.379, 'eval_steps_per_second': 0.317, 'epoch': 19.0}\n","Accuracy for fold  17 :  0.7630446979614737  --  0.9340235128969995\n","--------------------------------\n","Testing process has finished.\n","Train run #18\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:23, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.006000</td>\n","      <td>0.049081</td>\n","      <td>0.815874</td>\n","      <td>0.783733</td>\n","      <td>0.856533</td>\n","      <td>0.933088</td>\n","      <td>0.933088</td>\n","      <td>0.933088</td>\n","      <td>0.766022</td>\n","      <td>0.720122</td>\n","      <td>0.823222</td>\n","      <td>0.759941</td>\n","      <td>0.723638</td>\n","      <td>0.800078</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002800</td>\n","      <td>0.046114</td>\n","      <td>0.827115</td>\n","      <td>0.808576</td>\n","      <td>0.847224</td>\n","      <td>0.935076</td>\n","      <td>0.935076</td>\n","      <td>0.935076</td>\n","      <td>0.780947</td>\n","      <td>0.753973</td>\n","      <td>0.809984</td>\n","      <td>0.769057</td>\n","      <td>0.741023</td>\n","      <td>0.799296</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.045837</td>\n","      <td>0.832129</td>\n","      <td>0.813881</td>\n","      <td>0.851853</td>\n","      <td>0.936422</td>\n","      <td>0.936422</td>\n","      <td>0.936422</td>\n","      <td>0.787351</td>\n","      <td>0.760877</td>\n","      <td>0.815766</td>\n","      <td>0.772770</td>\n","      <td>0.745905</td>\n","      <td>0.801643</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001100</td>\n","      <td>0.045490</td>\n","      <td>0.829032</td>\n","      <td>0.808233</td>\n","      <td>0.851817</td>\n","      <td>0.935252</td>\n","      <td>0.935252</td>\n","      <td>0.935252</td>\n","      <td>0.783373</td>\n","      <td>0.753156</td>\n","      <td>0.816200</td>\n","      <td>0.768798</td>\n","      <td>0.738207</td>\n","      <td>0.802034</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001100</td>\n","      <td>0.045630</td>\n","      <td>0.829463</td>\n","      <td>0.812467</td>\n","      <td>0.848793</td>\n","      <td>0.934842</td>\n","      <td>0.934842</td>\n","      <td>0.934842</td>\n","      <td>0.784079</td>\n","      <td>0.758786</td>\n","      <td>0.812443</td>\n","      <td>0.768799</td>\n","      <td>0.736559</td>\n","      <td>0.803991</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.046334</td>\n","      <td>0.831032</td>\n","      <td>0.822709</td>\n","      <td>0.839983</td>\n","      <td>0.937533</td>\n","      <td>0.937533</td>\n","      <td>0.937533</td>\n","      <td>0.785605</td>\n","      <td>0.773246</td>\n","      <td>0.798794</td>\n","      <td>0.772440</td>\n","      <td>0.756088</td>\n","      <td>0.789515</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.046894</td>\n","      <td>0.832506</td>\n","      <td>0.819549</td>\n","      <td>0.846368</td>\n","      <td>0.937474</td>\n","      <td>0.937474</td>\n","      <td>0.937474</td>\n","      <td>0.787595</td>\n","      <td>0.768684</td>\n","      <td>0.807697</td>\n","      <td>0.773678</td>\n","      <td>0.752776</td>\n","      <td>0.795775</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000800</td>\n","      <td>0.046640</td>\n","      <td>0.831277</td>\n","      <td>0.817266</td>\n","      <td>0.846592</td>\n","      <td>0.935778</td>\n","      <td>0.935778</td>\n","      <td>0.935778</td>\n","      <td>0.786272</td>\n","      <td>0.765504</td>\n","      <td>0.808752</td>\n","      <td>0.769347</td>\n","      <td>0.743253</td>\n","      <td>0.797340</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000900</td>\n","      <td>0.046522</td>\n","      <td>0.830918</td>\n","      <td>0.814095</td>\n","      <td>0.849306</td>\n","      <td>0.935544</td>\n","      <td>0.935544</td>\n","      <td>0.935544</td>\n","      <td>0.785827</td>\n","      <td>0.761014</td>\n","      <td>0.812692</td>\n","      <td>0.769346</td>\n","      <td>0.739884</td>\n","      <td>0.801252</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8309184499166689, 'eval_macro_precision': 0.8140953532062503, 'eval_macro_recall': 0.8493062432797884, 'eval_micro_f1': 0.9355442475288064, 'eval_micro_precision': 0.9355442475288063, 'eval_micro_recall': 0.9355442475288063, 'eval_macro_f1_no_o': 0.7858268398149972, 'eval_macro_precision_no_o': 0.7610135451268859, 'eval_macro_recall_no_o': 0.8126916611449376, 'eval_micro_f1_no_o': 0.7693463561232157, 'eval_micro_precision_no_o': 0.7398843930635838, 'eval_micro_recall_no_o': 0.8012519561815337, 'eval_loss': 0.04653154863469051, 'eval_runtime': 25.4012, 'eval_samples_per_second': 2.362, 'eval_steps_per_second': 0.315, 'epoch': 19.0}\n","Accuracy for fold  18 :  0.7693463561232157  --  0.9355442475288064\n","--------------------------------\n","Testing process has finished.\n","Train run #19\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:50, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005900</td>\n","      <td>0.047799</td>\n","      <td>0.811953</td>\n","      <td>0.783958</td>\n","      <td>0.844758</td>\n","      <td>0.932444</td>\n","      <td>0.932444</td>\n","      <td>0.932444</td>\n","      <td>0.760941</td>\n","      <td>0.720651</td>\n","      <td>0.807590</td>\n","      <td>0.757953</td>\n","      <td>0.722597</td>\n","      <td>0.796948</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002800</td>\n","      <td>0.048256</td>\n","      <td>0.818034</td>\n","      <td>0.793587</td>\n","      <td>0.845249</td>\n","      <td>0.931742</td>\n","      <td>0.931742</td>\n","      <td>0.931742</td>\n","      <td>0.769251</td>\n","      <td>0.733877</td>\n","      <td>0.808268</td>\n","      <td>0.756111</td>\n","      <td>0.722797</td>\n","      <td>0.792645</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001700</td>\n","      <td>0.053041</td>\n","      <td>0.818403</td>\n","      <td>0.797657</td>\n","      <td>0.841314</td>\n","      <td>0.933439</td>\n","      <td>0.933439</td>\n","      <td>0.933439</td>\n","      <td>0.769384</td>\n","      <td>0.739594</td>\n","      <td>0.802036</td>\n","      <td>0.758959</td>\n","      <td>0.732702</td>\n","      <td>0.787167</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001200</td>\n","      <td>0.050421</td>\n","      <td>0.817942</td>\n","      <td>0.788798</td>\n","      <td>0.851461</td>\n","      <td>0.929578</td>\n","      <td>0.929578</td>\n","      <td>0.929578</td>\n","      <td>0.769627</td>\n","      <td>0.726717</td>\n","      <td>0.818269</td>\n","      <td>0.754662</td>\n","      <td>0.708305</td>\n","      <td>0.807512</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001100</td>\n","      <td>0.052282</td>\n","      <td>0.815189</td>\n","      <td>0.781454</td>\n","      <td>0.854511</td>\n","      <td>0.928876</td>\n","      <td>0.928876</td>\n","      <td>0.928876</td>\n","      <td>0.766098</td>\n","      <td>0.716977</td>\n","      <td>0.822566</td>\n","      <td>0.752829</td>\n","      <td>0.705681</td>\n","      <td>0.806729</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.056266</td>\n","      <td>0.816891</td>\n","      <td>0.793684</td>\n","      <td>0.842748</td>\n","      <td>0.931508</td>\n","      <td>0.931508</td>\n","      <td>0.931508</td>\n","      <td>0.767738</td>\n","      <td>0.734360</td>\n","      <td>0.804613</td>\n","      <td>0.753612</td>\n","      <td>0.724125</td>\n","      <td>0.785603</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000700</td>\n","      <td>0.056420</td>\n","      <td>0.816442</td>\n","      <td>0.791498</td>\n","      <td>0.844305</td>\n","      <td>0.931567</td>\n","      <td>0.931567</td>\n","      <td>0.931567</td>\n","      <td>0.767115</td>\n","      <td>0.731113</td>\n","      <td>0.806963</td>\n","      <td>0.754668</td>\n","      <td>0.721786</td>\n","      <td>0.790689</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.054571</td>\n","      <td>0.818858</td>\n","      <td>0.791453</td>\n","      <td>0.849811</td>\n","      <td>0.931158</td>\n","      <td>0.931158</td>\n","      <td>0.931158</td>\n","      <td>0.770506</td>\n","      <td>0.730686</td>\n","      <td>0.814993</td>\n","      <td>0.756617</td>\n","      <td>0.717949</td>\n","      <td>0.799687</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.054559</td>\n","      <td>0.819982</td>\n","      <td>0.793291</td>\n","      <td>0.850034</td>\n","      <td>0.931333</td>\n","      <td>0.931333</td>\n","      <td>0.931333</td>\n","      <td>0.772004</td>\n","      <td>0.733180</td>\n","      <td>0.815243</td>\n","      <td>0.757548</td>\n","      <td>0.719311</td>\n","      <td>0.800078</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.819982393693616, 'eval_macro_precision': 0.7932905199966445, 'eval_macro_recall': 0.8500337473636714, 'eval_micro_f1': 0.9313329823945722, 'eval_micro_precision': 0.9313329823945722, 'eval_micro_recall': 0.9313329823945722, 'eval_macro_f1_no_o': 0.7720040760042911, 'eval_macro_precision_no_o': 0.7331802134635584, 'eval_macro_recall_no_o': 0.8152434009962769, 'eval_micro_f1_no_o': 0.7575476940174106, 'eval_micro_precision_no_o': 0.719310587407668, 'eval_micro_recall_no_o': 0.8000782472613458, 'eval_loss': 0.05455071241685801, 'eval_runtime': 25.2203, 'eval_samples_per_second': 2.379, 'eval_steps_per_second': 0.317, 'epoch': 19.0}\n","Accuracy for fold  19 :  0.7575476940174106  --  0.9313329823945722\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #20\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:52, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005700</td>\n","      <td>0.054543</td>\n","      <td>0.812128</td>\n","      <td>0.792898</td>\n","      <td>0.833975</td>\n","      <td>0.932971</td>\n","      <td>0.932971</td>\n","      <td>0.932971</td>\n","      <td>0.760986</td>\n","      <td>0.733815</td>\n","      <td>0.791631</td>\n","      <td>0.753286</td>\n","      <td>0.734126</td>\n","      <td>0.773474</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.003100</td>\n","      <td>0.057239</td>\n","      <td>0.814181</td>\n","      <td>0.802333</td>\n","      <td>0.827725</td>\n","      <td>0.932561</td>\n","      <td>0.932561</td>\n","      <td>0.932561</td>\n","      <td>0.763975</td>\n","      <td>0.747322</td>\n","      <td>0.782886</td>\n","      <td>0.752361</td>\n","      <td>0.741360</td>\n","      <td>0.763693</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.053148</td>\n","      <td>0.820424</td>\n","      <td>0.800993</td>\n","      <td>0.842349</td>\n","      <td>0.930339</td>\n","      <td>0.930339</td>\n","      <td>0.930339</td>\n","      <td>0.772869</td>\n","      <td>0.744084</td>\n","      <td>0.804929</td>\n","      <td>0.754471</td>\n","      <td>0.720128</td>\n","      <td>0.792254</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001200</td>\n","      <td>0.059966</td>\n","      <td>0.817326</td>\n","      <td>0.805659</td>\n","      <td>0.830421</td>\n","      <td>0.930982</td>\n","      <td>0.930982</td>\n","      <td>0.930982</td>\n","      <td>0.768540</td>\n","      <td>0.751389</td>\n","      <td>0.787580</td>\n","      <td>0.750904</td>\n","      <td>0.731011</td>\n","      <td>0.771909</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001100</td>\n","      <td>0.043422</td>\n","      <td>0.819750</td>\n","      <td>0.777625</td>\n","      <td>0.871753</td>\n","      <td>0.928642</td>\n","      <td>0.928642</td>\n","      <td>0.928642</td>\n","      <td>0.772109</td>\n","      <td>0.709945</td>\n","      <td>0.847228</td>\n","      <td>0.756210</td>\n","      <td>0.691883</td>\n","      <td>0.833725</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.055975</td>\n","      <td>0.808846</td>\n","      <td>0.784042</td>\n","      <td>0.836740</td>\n","      <td>0.928525</td>\n","      <td>0.928525</td>\n","      <td>0.928525</td>\n","      <td>0.757491</td>\n","      <td>0.721176</td>\n","      <td>0.797862</td>\n","      <td>0.745185</td>\n","      <td>0.707454</td>\n","      <td>0.787167</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.057611</td>\n","      <td>0.810212</td>\n","      <td>0.788367</td>\n","      <td>0.834524</td>\n","      <td>0.928993</td>\n","      <td>0.928993</td>\n","      <td>0.928993</td>\n","      <td>0.759264</td>\n","      <td>0.727272</td>\n","      <td>0.794495</td>\n","      <td>0.746041</td>\n","      <td>0.712202</td>\n","      <td>0.783255</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.054341</td>\n","      <td>0.808812</td>\n","      <td>0.779942</td>\n","      <td>0.841904</td>\n","      <td>0.928058</td>\n","      <td>0.928058</td>\n","      <td>0.928058</td>\n","      <td>0.757517</td>\n","      <td>0.715136</td>\n","      <td>0.805436</td>\n","      <td>0.745601</td>\n","      <td>0.701379</td>\n","      <td>0.795775</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.054074</td>\n","      <td>0.811096</td>\n","      <td>0.779090</td>\n","      <td>0.848338</td>\n","      <td>0.927882</td>\n","      <td>0.927882</td>\n","      <td>0.927882</td>\n","      <td>0.760648</td>\n","      <td>0.713645</td>\n","      <td>0.814519</td>\n","      <td>0.747361</td>\n","      <td>0.698775</td>\n","      <td>0.803208</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8107215688052859, 'eval_macro_precision': 0.7789347387303819, 'eval_macro_recall': 0.8476820036809647, 'eval_micro_f1': 0.9278235947827104, 'eval_micro_precision': 0.9278235947827104, 'eval_micro_recall': 0.9278235947827104, 'eval_macro_f1_no_o': 0.7601602613744242, 'eval_macro_precision_no_o': 0.7134619491922983, 'eval_macro_recall_no_o': 0.813643629784738, 'eval_micro_f1_no_o': 0.7471327143637356, 'eval_micro_precision_no_o': 0.6986721144024515, 'eval_micro_recall_no_o': 0.8028169014084507, 'eval_loss': 0.05408593140012575, 'eval_runtime': 26.1395, 'eval_samples_per_second': 2.295, 'eval_steps_per_second': 0.306, 'epoch': 19.0}\n","Accuracy for fold  20 :  0.7471327143637356  --  0.9278235947827104\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #21\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:57, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005800</td>\n","      <td>0.056641</td>\n","      <td>0.810070</td>\n","      <td>0.794004</td>\n","      <td>0.827848</td>\n","      <td>0.931801</td>\n","      <td>0.931801</td>\n","      <td>0.931801</td>\n","      <td>0.758599</td>\n","      <td>0.735941</td>\n","      <td>0.783531</td>\n","      <td>0.750527</td>\n","      <td>0.734908</td>\n","      <td>0.766823</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002900</td>\n","      <td>0.051669</td>\n","      <td>0.817414</td>\n","      <td>0.803466</td>\n","      <td>0.832941</td>\n","      <td>0.932561</td>\n","      <td>0.932561</td>\n","      <td>0.932561</td>\n","      <td>0.768300</td>\n","      <td>0.748118</td>\n","      <td>0.790574</td>\n","      <td>0.755234</td>\n","      <td>0.735360</td>\n","      <td>0.776213</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.057642</td>\n","      <td>0.816861</td>\n","      <td>0.815207</td>\n","      <td>0.819958</td>\n","      <td>0.935018</td>\n","      <td>0.935018</td>\n","      <td>0.935018</td>\n","      <td>0.767069</td>\n","      <td>0.765207</td>\n","      <td>0.770855</td>\n","      <td>0.756150</td>\n","      <td>0.760792</td>\n","      <td>0.751565</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001200</td>\n","      <td>0.046730</td>\n","      <td>0.815549</td>\n","      <td>0.787061</td>\n","      <td>0.849167</td>\n","      <td>0.928408</td>\n","      <td>0.928408</td>\n","      <td>0.928408</td>\n","      <td>0.766588</td>\n","      <td>0.724520</td>\n","      <td>0.815394</td>\n","      <td>0.749863</td>\n","      <td>0.703462</td>\n","      <td>0.802817</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001200</td>\n","      <td>0.046103</td>\n","      <td>0.809488</td>\n","      <td>0.766391</td>\n","      <td>0.862400</td>\n","      <td>0.925484</td>\n","      <td>0.925484</td>\n","      <td>0.925484</td>\n","      <td>0.758836</td>\n","      <td>0.695371</td>\n","      <td>0.835170</td>\n","      <td>0.743303</td>\n","      <td>0.679974</td>\n","      <td>0.819640</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.052407</td>\n","      <td>0.811601</td>\n","      <td>0.778563</td>\n","      <td>0.850019</td>\n","      <td>0.929286</td>\n","      <td>0.929286</td>\n","      <td>0.929286</td>\n","      <td>0.760840</td>\n","      <td>0.712942</td>\n","      <td>0.815820</td>\n","      <td>0.746882</td>\n","      <td>0.703039</td>\n","      <td>0.796557</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.055285</td>\n","      <td>0.810388</td>\n","      <td>0.783725</td>\n","      <td>0.840602</td>\n","      <td>0.929812</td>\n","      <td>0.929812</td>\n","      <td>0.929812</td>\n","      <td>0.759245</td>\n","      <td>0.720826</td>\n","      <td>0.802347</td>\n","      <td>0.747159</td>\n","      <td>0.713269</td>\n","      <td>0.784429</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.053107</td>\n","      <td>0.810246</td>\n","      <td>0.786017</td>\n","      <td>0.838284</td>\n","      <td>0.930397</td>\n","      <td>0.930397</td>\n","      <td>0.930397</td>\n","      <td>0.758889</td>\n","      <td>0.723829</td>\n","      <td>0.798981</td>\n","      <td>0.747807</td>\n","      <td>0.715102</td>\n","      <td>0.783646</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.053386</td>\n","      <td>0.809824</td>\n","      <td>0.786831</td>\n","      <td>0.836279</td>\n","      <td>0.930397</td>\n","      <td>0.930397</td>\n","      <td>0.930397</td>\n","      <td>0.758334</td>\n","      <td>0.725047</td>\n","      <td>0.796193</td>\n","      <td>0.747475</td>\n","      <td>0.716129</td>\n","      <td>0.781690</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8096455573325285, 'eval_macro_precision': 0.7865239886656451, 'eval_macro_recall': 0.8362787055259725, 'eval_micro_f1': 0.9303971456980756, 'eval_micro_precision': 0.9303971456980756, 'eval_micro_recall': 0.9303971456980756, 'eval_macro_f1_no_o': 0.7580955369721197, 'eval_macro_precision_no_o': 0.7246381683408479, 'eval_macro_recall_no_o': 0.7961927109142576, 'eval_micro_f1_no_o': 0.7474747474747475, 'eval_micro_precision_no_o': 0.7161290322580646, 'eval_micro_recall_no_o': 0.7816901408450704, 'eval_loss': 0.05339955043982627, 'eval_runtime': 26.1387, 'eval_samples_per_second': 2.295, 'eval_steps_per_second': 0.306, 'epoch': 19.0}\n","Accuracy for fold  21 :  0.7474747474747475  --  0.9303971456980756\n","--------------------------------\n","Testing process has finished.\n","Train run #22\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:57, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005600</td>\n","      <td>0.049867</td>\n","      <td>0.812006</td>\n","      <td>0.788918</td>\n","      <td>0.839543</td>\n","      <td>0.929461</td>\n","      <td>0.929461</td>\n","      <td>0.929461</td>\n","      <td>0.761573</td>\n","      <td>0.727671</td>\n","      <td>0.801347</td>\n","      <td>0.748562</td>\n","      <td>0.711966</td>\n","      <td>0.789124</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002800</td>\n","      <td>0.052524</td>\n","      <td>0.807622</td>\n","      <td>0.799814</td>\n","      <td>0.817718</td>\n","      <td>0.932035</td>\n","      <td>0.932035</td>\n","      <td>0.932035</td>\n","      <td>0.755178</td>\n","      <td>0.743743</td>\n","      <td>0.769657</td>\n","      <td>0.748655</td>\n","      <td>0.735650</td>\n","      <td>0.762128</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.050711</td>\n","      <td>0.811862</td>\n","      <td>0.783972</td>\n","      <td>0.843618</td>\n","      <td>0.929578</td>\n","      <td>0.929578</td>\n","      <td>0.929578</td>\n","      <td>0.761329</td>\n","      <td>0.720875</td>\n","      <td>0.806872</td>\n","      <td>0.748982</td>\n","      <td>0.710822</td>\n","      <td>0.791471</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001100</td>\n","      <td>0.052585</td>\n","      <td>0.797923</td>\n","      <td>0.752082</td>\n","      <td>0.855311</td>\n","      <td>0.922209</td>\n","      <td>0.922209</td>\n","      <td>0.922209</td>\n","      <td>0.744026</td>\n","      <td>0.676843</td>\n","      <td>0.826383</td>\n","      <td>0.732943</td>\n","      <td>0.669906</td>\n","      <td>0.809077</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001200</td>\n","      <td>0.052789</td>\n","      <td>0.799840</td>\n","      <td>0.756102</td>\n","      <td>0.854005</td>\n","      <td>0.924197</td>\n","      <td>0.924197</td>\n","      <td>0.924197</td>\n","      <td>0.746149</td>\n","      <td>0.682365</td>\n","      <td>0.823656</td>\n","      <td>0.736409</td>\n","      <td>0.678195</td>\n","      <td>0.805556</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.054084</td>\n","      <td>0.809262</td>\n","      <td>0.776213</td>\n","      <td>0.847712</td>\n","      <td>0.928876</td>\n","      <td>0.928876</td>\n","      <td>0.928876</td>\n","      <td>0.757701</td>\n","      <td>0.709696</td>\n","      <td>0.812812</td>\n","      <td>0.744322</td>\n","      <td>0.699725</td>\n","      <td>0.794992</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.055472</td>\n","      <td>0.810519</td>\n","      <td>0.788783</td>\n","      <td>0.836896</td>\n","      <td>0.931391</td>\n","      <td>0.931391</td>\n","      <td>0.931391</td>\n","      <td>0.759025</td>\n","      <td>0.727549</td>\n","      <td>0.796649</td>\n","      <td>0.749531</td>\n","      <td>0.719582</td>\n","      <td>0.782081</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.055762</td>\n","      <td>0.809698</td>\n","      <td>0.786817</td>\n","      <td>0.837349</td>\n","      <td>0.931041</td>\n","      <td>0.931041</td>\n","      <td>0.931041</td>\n","      <td>0.757992</td>\n","      <td>0.724865</td>\n","      <td>0.797437</td>\n","      <td>0.748737</td>\n","      <td>0.717461</td>\n","      <td>0.782864</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000800</td>\n","      <td>0.055856</td>\n","      <td>0.809103</td>\n","      <td>0.785838</td>\n","      <td>0.837332</td>\n","      <td>0.930982</td>\n","      <td>0.930982</td>\n","      <td>0.930982</td>\n","      <td>0.757210</td>\n","      <td>0.723560</td>\n","      <td>0.797437</td>\n","      <td>0.748597</td>\n","      <td>0.717204</td>\n","      <td>0.782864</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8091028852909076, 'eval_macro_precision': 0.7858378202379123, 'eval_macro_recall': 0.8373320586647713, 'eval_micro_f1': 0.930982043633386, 'eval_micro_precision': 0.930982043633386, 'eval_micro_recall': 0.930982043633386, 'eval_macro_f1_no_o': 0.7572102068252958, 'eval_macro_precision_no_o': 0.7235601867751275, 'eval_macro_recall_no_o': 0.7974367159566688, 'eval_micro_f1_no_o': 0.7485970819304153, 'eval_micro_precision_no_o': 0.7172043010752688, 'eval_micro_recall_no_o': 0.7828638497652582, 'eval_loss': 0.055856228283664676, 'eval_runtime': 25.4921, 'eval_samples_per_second': 2.354, 'eval_steps_per_second': 0.314, 'epoch': 19.0}\n","Accuracy for fold  22 :  0.7485970819304153  --  0.930982043633386\n","--------------------------------\n","Testing process has finished.\n","Train run #23\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:28, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005600</td>\n","      <td>0.053101</td>\n","      <td>0.808034</td>\n","      <td>0.789004</td>\n","      <td>0.832157</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.934199</td>\n","      <td>0.755173</td>\n","      <td>0.728820</td>\n","      <td>0.788313</td>\n","      <td>0.753462</td>\n","      <td>0.740923</td>\n","      <td>0.766432</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002900</td>\n","      <td>0.046779</td>\n","      <td>0.803735</td>\n","      <td>0.776986</td>\n","      <td>0.835936</td>\n","      <td>0.928058</td>\n","      <td>0.928058</td>\n","      <td>0.928058</td>\n","      <td>0.750831</td>\n","      <td>0.711533</td>\n","      <td>0.797317</td>\n","      <td>0.746043</td>\n","      <td>0.704309</td>\n","      <td>0.793036</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.050962</td>\n","      <td>0.811247</td>\n","      <td>0.806158</td>\n","      <td>0.816471</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.759573</td>\n","      <td>0.752131</td>\n","      <td>0.767192</td>\n","      <td>0.757300</td>\n","      <td>0.748757</td>\n","      <td>0.766041</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001100</td>\n","      <td>0.049126</td>\n","      <td>0.798389</td>\n","      <td>0.767265</td>\n","      <td>0.841908</td>\n","      <td>0.927941</td>\n","      <td>0.927941</td>\n","      <td>0.927941</td>\n","      <td>0.743287</td>\n","      <td>0.697792</td>\n","      <td>0.805211</td>\n","      <td>0.739982</td>\n","      <td>0.695084</td>\n","      <td>0.791080</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001000</td>\n","      <td>0.051220</td>\n","      <td>0.797419</td>\n","      <td>0.769840</td>\n","      <td>0.833113</td>\n","      <td>0.929695</td>\n","      <td>0.929695</td>\n","      <td>0.929695</td>\n","      <td>0.741661</td>\n","      <td>0.702076</td>\n","      <td>0.792018</td>\n","      <td>0.741514</td>\n","      <td>0.708482</td>\n","      <td>0.777778</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000800</td>\n","      <td>0.048975</td>\n","      <td>0.804599</td>\n","      <td>0.771616</td>\n","      <td>0.844792</td>\n","      <td>0.930690</td>\n","      <td>0.930690</td>\n","      <td>0.930690</td>\n","      <td>0.751190</td>\n","      <td>0.703709</td>\n","      <td>0.808208</td>\n","      <td>0.749539</td>\n","      <td>0.709002</td>\n","      <td>0.794992</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.050393</td>\n","      <td>0.801299</td>\n","      <td>0.777194</td>\n","      <td>0.833329</td>\n","      <td>0.930514</td>\n","      <td>0.930514</td>\n","      <td>0.930514</td>\n","      <td>0.746636</td>\n","      <td>0.711739</td>\n","      <td>0.792054</td>\n","      <td>0.743326</td>\n","      <td>0.710818</td>\n","      <td>0.778951</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.050602</td>\n","      <td>0.802330</td>\n","      <td>0.777530</td>\n","      <td>0.834567</td>\n","      <td>0.930573</td>\n","      <td>0.930573</td>\n","      <td>0.930573</td>\n","      <td>0.748035</td>\n","      <td>0.712166</td>\n","      <td>0.793773</td>\n","      <td>0.744264</td>\n","      <td>0.711230</td>\n","      <td>0.780516</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.050597</td>\n","      <td>0.802606</td>\n","      <td>0.777839</td>\n","      <td>0.834755</td>\n","      <td>0.930631</td>\n","      <td>0.930631</td>\n","      <td>0.930631</td>\n","      <td>0.748392</td>\n","      <td>0.712555</td>\n","      <td>0.794024</td>\n","      <td>0.744498</td>\n","      <td>0.711333</td>\n","      <td>0.780908</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8025945842309431, 'eval_macro_precision': 0.7778619989397713, 'eval_macro_recall': 0.8347553454140157, 'eval_micro_f1': 0.9306311048721998, 'eval_micro_precision': 0.9306311048721998, 'eval_micro_recall': 0.9306311048721998, 'eval_macro_f1_no_o': 0.7483659698686514, 'eval_macro_precision_no_o': 0.7125633743736254, 'eval_macro_recall_no_o': 0.7940240219760404, 'eval_micro_f1_no_o': 0.7443595002796942, 'eval_micro_precision_no_o': 0.7110794442465266, 'eval_micro_recall_no_o': 0.7809076682316118, 'eval_loss': 0.05059805737558539, 'eval_runtime': 25.0581, 'eval_samples_per_second': 2.394, 'eval_steps_per_second': 0.319, 'epoch': 19.0}\n","Accuracy for fold  23 :  0.7443595002796942  --  0.9306311048721998\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #24\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:28, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005400</td>\n","      <td>0.050576</td>\n","      <td>0.811130</td>\n","      <td>0.780843</td>\n","      <td>0.848497</td>\n","      <td>0.932444</td>\n","      <td>0.932444</td>\n","      <td>0.932444</td>\n","      <td>0.759800</td>\n","      <td>0.716385</td>\n","      <td>0.812599</td>\n","      <td>0.757480</td>\n","      <td>0.721416</td>\n","      <td>0.797340</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002800</td>\n","      <td>0.047588</td>\n","      <td>0.813630</td>\n","      <td>0.784478</td>\n","      <td>0.848899</td>\n","      <td>0.934082</td>\n","      <td>0.934082</td>\n","      <td>0.934082</td>\n","      <td>0.762864</td>\n","      <td>0.721374</td>\n","      <td>0.812470</td>\n","      <td>0.762208</td>\n","      <td>0.730369</td>\n","      <td>0.796948</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.050761</td>\n","      <td>0.817111</td>\n","      <td>0.794384</td>\n","      <td>0.843174</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.767415</td>\n","      <td>0.735106</td>\n","      <td>0.804149</td>\n","      <td>0.762427</td>\n","      <td>0.737477</td>\n","      <td>0.789124</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001100</td>\n","      <td>0.053367</td>\n","      <td>0.810619</td>\n","      <td>0.790475</td>\n","      <td>0.836257</td>\n","      <td>0.933205</td>\n","      <td>0.933205</td>\n","      <td>0.933205</td>\n","      <td>0.758692</td>\n","      <td>0.730088</td>\n","      <td>0.794605</td>\n","      <td>0.750949</td>\n","      <td>0.729351</td>\n","      <td>0.773865</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001000</td>\n","      <td>0.052424</td>\n","      <td>0.817228</td>\n","      <td>0.789118</td>\n","      <td>0.849914</td>\n","      <td>0.933848</td>\n","      <td>0.933848</td>\n","      <td>0.933848</td>\n","      <td>0.767595</td>\n","      <td>0.727402</td>\n","      <td>0.813846</td>\n","      <td>0.759948</td>\n","      <td>0.727208</td>\n","      <td>0.795775</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000700</td>\n","      <td>0.053477</td>\n","      <td>0.814197</td>\n","      <td>0.787809</td>\n","      <td>0.844864</td>\n","      <td>0.933263</td>\n","      <td>0.933263</td>\n","      <td>0.933263</td>\n","      <td>0.763512</td>\n","      <td>0.725832</td>\n","      <td>0.806860</td>\n","      <td>0.754781</td>\n","      <td>0.724622</td>\n","      <td>0.787559</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.053734</td>\n","      <td>0.815676</td>\n","      <td>0.790816</td>\n","      <td>0.844278</td>\n","      <td>0.933790</td>\n","      <td>0.933790</td>\n","      <td>0.933790</td>\n","      <td>0.765476</td>\n","      <td>0.730083</td>\n","      <td>0.805827</td>\n","      <td>0.757154</td>\n","      <td>0.729681</td>\n","      <td>0.786776</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.054161</td>\n","      <td>0.816476</td>\n","      <td>0.792606</td>\n","      <td>0.843955</td>\n","      <td>0.934024</td>\n","      <td>0.934024</td>\n","      <td>0.934024</td>\n","      <td>0.766450</td>\n","      <td>0.732377</td>\n","      <td>0.805305</td>\n","      <td>0.757154</td>\n","      <td>0.729681</td>\n","      <td>0.786776</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.054332</td>\n","      <td>0.817131</td>\n","      <td>0.793316</td>\n","      <td>0.844451</td>\n","      <td>0.933965</td>\n","      <td>0.933965</td>\n","      <td>0.933965</td>\n","      <td>0.767338</td>\n","      <td>0.733258</td>\n","      <td>0.806058</td>\n","      <td>0.757285</td>\n","      <td>0.728918</td>\n","      <td>0.787950</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8169373264294297, 'eval_macro_precision': 0.7932171283439761, 'eval_macro_recall': 0.8441554878908009, 'eval_micro_f1': 0.9339065333099374, 'eval_micro_precision': 0.9339065333099374, 'eval_micro_recall': 0.9339065333099374, 'eval_macro_f1_no_o': 0.7670906514280009, 'eval_macro_precision_no_o': 0.7331489394080593, 'eval_macro_recall_no_o': 0.8056635214836335, 'eval_micro_f1_no_o': 0.7570515231289958, 'eval_micro_precision_no_o': 0.7288196958725561, 'eval_micro_recall_no_o': 0.7875586854460094, 'eval_loss': 0.054338972405336486, 'eval_runtime': 26.4098, 'eval_samples_per_second': 2.272, 'eval_steps_per_second': 0.303, 'epoch': 19.0}\n","Accuracy for fold  24 :  0.7570515231289958  --  0.9339065333099374\n","--------------------------------\n","Testing process has finished.\n","Train run #25\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 34:20, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005500</td>\n","      <td>0.052175</td>\n","      <td>0.802552</td>\n","      <td>0.787180</td>\n","      <td>0.825525</td>\n","      <td>0.933029</td>\n","      <td>0.933029</td>\n","      <td>0.933029</td>\n","      <td>0.748142</td>\n","      <td>0.727291</td>\n","      <td>0.779126</td>\n","      <td>0.748056</td>\n","      <td>0.743431</td>\n","      <td>0.752739</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002800</td>\n","      <td>0.049255</td>\n","      <td>0.808746</td>\n","      <td>0.770278</td>\n","      <td>0.856671</td>\n","      <td>0.928175</td>\n","      <td>0.928175</td>\n","      <td>0.928175</td>\n","      <td>0.757490</td>\n","      <td>0.701288</td>\n","      <td>0.826156</td>\n","      <td>0.750722</td>\n","      <td>0.696452</td>\n","      <td>0.814163</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.052020</td>\n","      <td>0.801795</td>\n","      <td>0.771319</td>\n","      <td>0.843357</td>\n","      <td>0.929110</td>\n","      <td>0.929110</td>\n","      <td>0.929110</td>\n","      <td>0.747792</td>\n","      <td>0.703509</td>\n","      <td>0.806776</td>\n","      <td>0.745538</td>\n","      <td>0.703717</td>\n","      <td>0.792645</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001200</td>\n","      <td>0.051729</td>\n","      <td>0.817148</td>\n","      <td>0.787335</td>\n","      <td>0.851778</td>\n","      <td>0.932620</td>\n","      <td>0.932620</td>\n","      <td>0.932620</td>\n","      <td>0.767995</td>\n","      <td>0.725294</td>\n","      <td>0.817064</td>\n","      <td>0.761072</td>\n","      <td>0.725692</td>\n","      <td>0.800078</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001000</td>\n","      <td>0.052255</td>\n","      <td>0.814071</td>\n","      <td>0.789003</td>\n","      <td>0.842509</td>\n","      <td>0.932152</td>\n","      <td>0.932152</td>\n","      <td>0.932152</td>\n","      <td>0.763974</td>\n","      <td>0.727920</td>\n","      <td>0.804477</td>\n","      <td>0.758324</td>\n","      <td>0.726523</td>\n","      <td>0.793036</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000700</td>\n","      <td>0.051604</td>\n","      <td>0.817797</td>\n","      <td>0.801291</td>\n","      <td>0.836059</td>\n","      <td>0.935837</td>\n","      <td>0.935837</td>\n","      <td>0.935837</td>\n","      <td>0.768136</td>\n","      <td>0.744743</td>\n","      <td>0.793860</td>\n","      <td>0.764706</td>\n","      <td>0.747015</td>\n","      <td>0.783255</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.052964</td>\n","      <td>0.820611</td>\n","      <td>0.805444</td>\n","      <td>0.837122</td>\n","      <td>0.936246</td>\n","      <td>0.936246</td>\n","      <td>0.936246</td>\n","      <td>0.771885</td>\n","      <td>0.750366</td>\n","      <td>0.795185</td>\n","      <td>0.767024</td>\n","      <td>0.750374</td>\n","      <td>0.784429</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000700</td>\n","      <td>0.051852</td>\n","      <td>0.819290</td>\n","      <td>0.799350</td>\n","      <td>0.841296</td>\n","      <td>0.935252</td>\n","      <td>0.935252</td>\n","      <td>0.935252</td>\n","      <td>0.770327</td>\n","      <td>0.741904</td>\n","      <td>0.801484</td>\n","      <td>0.765732</td>\n","      <td>0.742647</td>\n","      <td>0.790297</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.051828</td>\n","      <td>0.817253</td>\n","      <td>0.794640</td>\n","      <td>0.842535</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.767690</td>\n","      <td>0.735386</td>\n","      <td>0.803526</td>\n","      <td>0.764329</td>\n","      <td>0.737627</td>\n","      <td>0.793036</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8171828876652152, 'eval_macro_precision': 0.7947466441241448, 'eval_macro_recall': 0.8422569991560735, 'eval_micro_f1': 0.9346669006258408, 'eval_micro_precision': 0.9346669006258408, 'eval_micro_recall': 0.9346669006258408, 'eval_macro_f1_no_o': 0.767596473783903, 'eval_macro_precision_no_o': 0.7355500513158759, 'eval_macro_recall_no_o': 0.8031322031706637, 'eval_micro_f1_no_o': 0.7642399094681253, 'eval_micro_precision_no_o': 0.7378004369992717, 'eval_micro_recall_no_o': 0.7926447574334898, 'eval_loss': 0.05182424474041909, 'eval_runtime': 26.4319, 'eval_samples_per_second': 2.27, 'eval_steps_per_second': 0.303, 'epoch': 19.0}\n","Accuracy for fold  25 :  0.7642399094681253  --  0.9346669006258408\n","--------------------------------\n","Testing process has finished.\n","Train run #26\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 34:03, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005300</td>\n","      <td>0.052211</td>\n","      <td>0.805363</td>\n","      <td>0.765215</td>\n","      <td>0.854658</td>\n","      <td>0.924665</td>\n","      <td>0.924665</td>\n","      <td>0.924665</td>\n","      <td>0.753937</td>\n","      <td>0.694852</td>\n","      <td>0.825031</td>\n","      <td>0.746071</td>\n","      <td>0.686268</td>\n","      <td>0.817293</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002700</td>\n","      <td>0.050593</td>\n","      <td>0.815141</td>\n","      <td>0.785598</td>\n","      <td>0.849610</td>\n","      <td>0.928525</td>\n","      <td>0.928525</td>\n","      <td>0.928525</td>\n","      <td>0.766214</td>\n","      <td>0.722846</td>\n","      <td>0.816054</td>\n","      <td>0.752928</td>\n","      <td>0.707359</td>\n","      <td>0.804773</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001500</td>\n","      <td>0.042829</td>\n","      <td>0.824026</td>\n","      <td>0.810954</td>\n","      <td>0.838304</td>\n","      <td>0.936597</td>\n","      <td>0.936597</td>\n","      <td>0.936597</td>\n","      <td>0.776444</td>\n","      <td>0.757560</td>\n","      <td>0.796922</td>\n","      <td>0.769935</td>\n","      <td>0.751303</td>\n","      <td>0.789515</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001100</td>\n","      <td>0.047207</td>\n","      <td>0.824096</td>\n","      <td>0.810834</td>\n","      <td>0.838324</td>\n","      <td>0.936422</td>\n","      <td>0.936422</td>\n","      <td>0.936422</td>\n","      <td>0.776674</td>\n","      <td>0.757910</td>\n","      <td>0.796720</td>\n","      <td>0.769821</td>\n","      <td>0.755748</td>\n","      <td>0.784429</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.000900</td>\n","      <td>0.044309</td>\n","      <td>0.832242</td>\n","      <td>0.812819</td>\n","      <td>0.853803</td>\n","      <td>0.936305</td>\n","      <td>0.936305</td>\n","      <td>0.936305</td>\n","      <td>0.787544</td>\n","      <td>0.758933</td>\n","      <td>0.818962</td>\n","      <td>0.774519</td>\n","      <td>0.741151</td>\n","      <td>0.811033</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000700</td>\n","      <td>0.044521</td>\n","      <td>0.819920</td>\n","      <td>0.790088</td>\n","      <td>0.854619</td>\n","      <td>0.933146</td>\n","      <td>0.933146</td>\n","      <td>0.933146</td>\n","      <td>0.771560</td>\n","      <td>0.728292</td>\n","      <td>0.821243</td>\n","      <td>0.764066</td>\n","      <td>0.722862</td>\n","      <td>0.810250</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.051227</td>\n","      <td>0.825470</td>\n","      <td>0.814009</td>\n","      <td>0.837915</td>\n","      <td>0.937065</td>\n","      <td>0.937065</td>\n","      <td>0.937065</td>\n","      <td>0.778195</td>\n","      <td>0.761864</td>\n","      <td>0.795830</td>\n","      <td>0.768728</td>\n","      <td>0.755094</td>\n","      <td>0.782864</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.050875</td>\n","      <td>0.823029</td>\n","      <td>0.810083</td>\n","      <td>0.837481</td>\n","      <td>0.936188</td>\n","      <td>0.936188</td>\n","      <td>0.936188</td>\n","      <td>0.775032</td>\n","      <td>0.756373</td>\n","      <td>0.795687</td>\n","      <td>0.765706</td>\n","      <td>0.747855</td>\n","      <td>0.784429</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.050321</td>\n","      <td>0.821986</td>\n","      <td>0.805494</td>\n","      <td>0.840799</td>\n","      <td>0.935720</td>\n","      <td>0.935720</td>\n","      <td>0.935720</td>\n","      <td>0.773712</td>\n","      <td>0.749930</td>\n","      <td>0.800569</td>\n","      <td>0.765175</td>\n","      <td>0.742636</td>\n","      <td>0.789124</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'eval_macro_f1': 0.8219859572840833, 'eval_macro_precision': 0.8054937747108321, 'eval_macro_recall': 0.8407989687427255, 'eval_micro_f1': 0.9357197169093993, 'eval_micro_precision': 0.9357197169093993, 'eval_micro_recall': 0.9357197169093993, 'eval_macro_f1_no_o': 0.7737121847991267, 'eval_macro_precision_no_o': 0.7499298587596112, 'eval_macro_recall_no_o': 0.8005692230692958, 'eval_micro_f1_no_o': 0.7651745068285282, 'eval_micro_precision_no_o': 0.7426362297496318, 'eval_micro_recall_no_o': 0.7891236306729265, 'eval_loss': 0.05033026659554404, 'eval_runtime': 26.1538, 'eval_samples_per_second': 2.294, 'eval_steps_per_second': 0.306, 'epoch': 19.0}\n","Accuracy for fold  26 :  0.7651745068285282  --  0.9357197169093993\n","--------------------------------\n","Testing process has finished.\n","Train run #27\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 34:01, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005300</td>\n","      <td>0.051033</td>\n","      <td>0.819110</td>\n","      <td>0.801926</td>\n","      <td>0.838078</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.934667</td>\n","      <td>0.770225</td>\n","      <td>0.745477</td>\n","      <td>0.797331</td>\n","      <td>0.764215</td>\n","      <td>0.741176</td>\n","      <td>0.788732</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002600</td>\n","      <td>0.067219</td>\n","      <td>0.802288</td>\n","      <td>0.822671</td>\n","      <td>0.786388</td>\n","      <td>0.932093</td>\n","      <td>0.932093</td>\n","      <td>0.932093</td>\n","      <td>0.748063</td>\n","      <td>0.778373</td>\n","      <td>0.723666</td>\n","      <td>0.731910</td>\n","      <td>0.778562</td>\n","      <td>0.690532</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.044570</td>\n","      <td>0.818286</td>\n","      <td>0.795923</td>\n","      <td>0.844142</td>\n","      <td>0.934140</td>\n","      <td>0.934140</td>\n","      <td>0.934140</td>\n","      <td>0.769275</td>\n","      <td>0.736998</td>\n","      <td>0.806173</td>\n","      <td>0.765335</td>\n","      <td>0.735135</td>\n","      <td>0.798122</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001100</td>\n","      <td>0.044012</td>\n","      <td>0.801213</td>\n","      <td>0.746071</td>\n","      <td>0.878081</td>\n","      <td>0.926127</td>\n","      <td>0.926127</td>\n","      <td>0.926127</td>\n","      <td>0.747823</td>\n","      <td>0.667774</td>\n","      <td>0.856583</td>\n","      <td>0.749164</td>\n","      <td>0.680960</td>\n","      <td>0.832551</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.001400</td>\n","      <td>0.043353</td>\n","      <td>0.810195</td>\n","      <td>0.782317</td>\n","      <td>0.846813</td>\n","      <td>0.929344</td>\n","      <td>0.929344</td>\n","      <td>0.929344</td>\n","      <td>0.758904</td>\n","      <td>0.717339</td>\n","      <td>0.812004</td>\n","      <td>0.748136</td>\n","      <td>0.698947</td>\n","      <td>0.804773</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000700</td>\n","      <td>0.050059</td>\n","      <td>0.808963</td>\n","      <td>0.794653</td>\n","      <td>0.827860</td>\n","      <td>0.932503</td>\n","      <td>0.932503</td>\n","      <td>0.932503</td>\n","      <td>0.756686</td>\n","      <td>0.735735</td>\n","      <td>0.783730</td>\n","      <td>0.750142</td>\n","      <td>0.727139</td>\n","      <td>0.774648</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.048950</td>\n","      <td>0.815269</td>\n","      <td>0.800079</td>\n","      <td>0.834301</td>\n","      <td>0.933965</td>\n","      <td>0.933965</td>\n","      <td>0.933965</td>\n","      <td>0.764872</td>\n","      <td>0.742474</td>\n","      <td>0.792363</td>\n","      <td>0.756930</td>\n","      <td>0.730615</td>\n","      <td>0.785211</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.049160</td>\n","      <td>0.818769</td>\n","      <td>0.801662</td>\n","      <td>0.839610</td>\n","      <td>0.933497</td>\n","      <td>0.933497</td>\n","      <td>0.933497</td>\n","      <td>0.769659</td>\n","      <td>0.744216</td>\n","      <td>0.800039</td>\n","      <td>0.757576</td>\n","      <td>0.725806</td>\n","      <td>0.792254</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.049279</td>\n","      <td>0.816438</td>\n","      <td>0.798750</td>\n","      <td>0.837772</td>\n","      <td>0.933614</td>\n","      <td>0.933614</td>\n","      <td>0.933614</td>\n","      <td>0.766557</td>\n","      <td>0.740489</td>\n","      <td>0.797450</td>\n","      <td>0.757922</td>\n","      <td>0.727764</td>\n","      <td>0.790689</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8164377600080452, 'eval_macro_precision': 0.7987500836950816, 'eval_macro_recall': 0.8377716421075363, 'eval_micro_f1': 0.9336140843422823, 'eval_micro_precision': 0.9336140843422823, 'eval_micro_recall': 0.9336140843422823, 'eval_macro_f1_no_o': 0.7665574161945043, 'eval_macro_precision_no_o': 0.7404889384090847, 'eval_macro_recall_no_o': 0.7974497350375428, 'eval_micro_f1_no_o': 0.7579223701481342, 'eval_micro_precision_no_o': 0.7277637738566799, 'eval_micro_recall_no_o': 0.7906885758998435, 'eval_loss': 0.04928689435576719, 'eval_runtime': 25.2097, 'eval_samples_per_second': 2.38, 'eval_steps_per_second': 0.317, 'epoch': 19.0}\n","Accuracy for fold  27 :  0.7579223701481342  --  0.9336140843422823\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #28\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 34:13, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005200</td>\n","      <td>0.047571</td>\n","      <td>0.804467</td>\n","      <td>0.759312</td>\n","      <td>0.861803</td>\n","      <td>0.925075</td>\n","      <td>0.925075</td>\n","      <td>0.925075</td>\n","      <td>0.752627</td>\n","      <td>0.686226</td>\n","      <td>0.835040</td>\n","      <td>0.748850</td>\n","      <td>0.683344</td>\n","      <td>0.828247</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002500</td>\n","      <td>0.046327</td>\n","      <td>0.818148</td>\n","      <td>0.797712</td>\n","      <td>0.840466</td>\n","      <td>0.933907</td>\n","      <td>0.933907</td>\n","      <td>0.933907</td>\n","      <td>0.769225</td>\n","      <td>0.739654</td>\n","      <td>0.801272</td>\n","      <td>0.765557</td>\n","      <td>0.736880</td>\n","      <td>0.796557</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001500</td>\n","      <td>0.048878</td>\n","      <td>0.823695</td>\n","      <td>0.809041</td>\n","      <td>0.841324</td>\n","      <td>0.932035</td>\n","      <td>0.932035</td>\n","      <td>0.932035</td>\n","      <td>0.777065</td>\n","      <td>0.754430</td>\n","      <td>0.803608</td>\n","      <td>0.763319</td>\n","      <td>0.726245</td>\n","      <td>0.804382</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001100</td>\n","      <td>0.047642</td>\n","      <td>0.813489</td>\n","      <td>0.784890</td>\n","      <td>0.846732</td>\n","      <td>0.928467</td>\n","      <td>0.928467</td>\n","      <td>0.928467</td>\n","      <td>0.764142</td>\n","      <td>0.721954</td>\n","      <td>0.812422</td>\n","      <td>0.754889</td>\n","      <td>0.708405</td>\n","      <td>0.807903</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.000900</td>\n","      <td>0.048348</td>\n","      <td>0.819295</td>\n","      <td>0.802125</td>\n","      <td>0.838095</td>\n","      <td>0.932269</td>\n","      <td>0.932269</td>\n","      <td>0.932269</td>\n","      <td>0.771185</td>\n","      <td>0.745869</td>\n","      <td>0.798637</td>\n","      <td>0.762387</td>\n","      <td>0.732684</td>\n","      <td>0.794601</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000700</td>\n","      <td>0.047088</td>\n","      <td>0.819371</td>\n","      <td>0.796833</td>\n","      <td>0.847267</td>\n","      <td>0.929344</td>\n","      <td>0.929344</td>\n","      <td>0.929344</td>\n","      <td>0.771771</td>\n","      <td>0.737173</td>\n","      <td>0.813389</td>\n","      <td>0.758571</td>\n","      <td>0.707136</td>\n","      <td>0.818075</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.047778</td>\n","      <td>0.821044</td>\n","      <td>0.794693</td>\n","      <td>0.852881</td>\n","      <td>0.929286</td>\n","      <td>0.929286</td>\n","      <td>0.929286</td>\n","      <td>0.774161</td>\n","      <td>0.734282</td>\n","      <td>0.821218</td>\n","      <td>0.761302</td>\n","      <td>0.707801</td>\n","      <td>0.823552</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.001200</td>\n","      <td>0.046957</td>\n","      <td>0.816832</td>\n","      <td>0.785295</td>\n","      <td>0.856652</td>\n","      <td>0.926010</td>\n","      <td>0.926010</td>\n","      <td>0.926010</td>\n","      <td>0.769027</td>\n","      <td>0.720935</td>\n","      <td>0.827942</td>\n","      <td>0.752703</td>\n","      <td>0.688169</td>\n","      <td>0.830595</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000700</td>\n","      <td>0.047356</td>\n","      <td>0.817002</td>\n","      <td>0.786171</td>\n","      <td>0.855738</td>\n","      <td>0.926303</td>\n","      <td>0.926303</td>\n","      <td>0.926303</td>\n","      <td>0.769176</td>\n","      <td>0.722191</td>\n","      <td>0.826494</td>\n","      <td>0.752799</td>\n","      <td>0.689678</td>\n","      <td>0.828638</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8169025037859187, 'eval_macro_precision': 0.7859165705219189, 'eval_macro_recall': 0.8557549312013581, 'eval_micro_f1': 0.9263613499444346, 'eval_micro_precision': 0.9263613499444346, 'eval_micro_recall': 0.9263613499444346, 'eval_macro_f1_no_o': 0.7690315844374184, 'eval_macro_precision_no_o': 0.7218508813679291, 'eval_macro_recall_no_o': 0.8264935886664327, 'eval_micro_f1_no_o': 0.7529328119445432, 'eval_micro_precision_no_o': 0.6899022801302932, 'eval_micro_recall_no_o': 0.8286384976525821, 'eval_loss': 0.04736547365179528, 'eval_runtime': 25.3691, 'eval_samples_per_second': 2.365, 'eval_steps_per_second': 0.315, 'epoch': 19.0}\n","Accuracy for fold  28 :  0.7529328119445432  --  0.9263613499444346\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1522\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3610\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Testing process has finished.\n","Train run #29\n","--------------------------------\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2999' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2999/3610 28:42 < 05:51, 1.74 it/s, Epoch 15.78/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005200</td>\n","      <td>0.051569</td>\n","      <td>0.812816</td>\n","      <td>0.787795</td>\n","      <td>0.840943</td>\n","      <td>0.930631</td>\n","      <td>0.930631</td>\n","      <td>0.930631</td>\n","      <td>0.762639</td>\n","      <td>0.726549</td>\n","      <td>0.802824</td>\n","      <td>0.754435</td>\n","      <td>0.721686</td>\n","      <td>0.790297</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002600</td>\n","      <td>0.054950</td>\n","      <td>0.810347</td>\n","      <td>0.790806</td>\n","      <td>0.831736</td>\n","      <td>0.929754</td>\n","      <td>0.929754</td>\n","      <td>0.929754</td>\n","      <td>0.759469</td>\n","      <td>0.731425</td>\n","      <td>0.789953</td>\n","      <td>0.748204</td>\n","      <td>0.723848</td>\n","      <td>0.774257</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.065089</td>\n","      <td>0.810711</td>\n","      <td>0.799399</td>\n","      <td>0.824153</td>\n","      <td>0.931274</td>\n","      <td>0.931274</td>\n","      <td>0.931274</td>\n","      <td>0.759707</td>\n","      <td>0.744315</td>\n","      <td>0.777939</td>\n","      <td>0.747860</td>\n","      <td>0.743808</td>\n","      <td>0.751956</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001100</td>\n","      <td>0.055892</td>\n","      <td>0.815915</td>\n","      <td>0.787809</td>\n","      <td>0.849218</td>\n","      <td>0.927356</td>\n","      <td>0.927356</td>\n","      <td>0.927356</td>\n","      <td>0.767693</td>\n","      <td>0.726085</td>\n","      <td>0.816127</td>\n","      <td>0.753195</td>\n","      <td>0.706023</td>\n","      <td>0.807121</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.000900</td>\n","      <td>0.060644</td>\n","      <td>0.815265</td>\n","      <td>0.793792</td>\n","      <td>0.839065</td>\n","      <td>0.928175</td>\n","      <td>0.928175</td>\n","      <td>0.928175</td>\n","      <td>0.766443</td>\n","      <td>0.734884</td>\n","      <td>0.801053</td>\n","      <td>0.748185</td>\n","      <td>0.713525</td>\n","      <td>0.786385</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000700</td>\n","      <td>0.053666</td>\n","      <td>0.821586</td>\n","      <td>0.803380</td>\n","      <td>0.842107</td>\n","      <td>0.931216</td>\n","      <td>0.931216</td>\n","      <td>0.931216</td>\n","      <td>0.774306</td>\n","      <td>0.747063</td>\n","      <td>0.804583</td>\n","      <td>0.758557</td>\n","      <td>0.723050</td>\n","      <td>0.797731</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.059726</td>\n","      <td>0.815736</td>\n","      <td>0.798739</td>\n","      <td>0.834869</td>\n","      <td>0.927999</td>\n","      <td>0.927999</td>\n","      <td>0.927999</td>\n","      <td>0.767171</td>\n","      <td>0.741705</td>\n","      <td>0.795436</td>\n","      <td>0.748228</td>\n","      <td>0.714897</td>\n","      <td>0.784820</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-380\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-760\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1140\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1520\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-1900\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2280\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2660\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3610' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3610/3610 33:08, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>380</td>\n","      <td>0.005200</td>\n","      <td>0.051569</td>\n","      <td>0.812816</td>\n","      <td>0.787795</td>\n","      <td>0.840943</td>\n","      <td>0.930631</td>\n","      <td>0.930631</td>\n","      <td>0.930631</td>\n","      <td>0.762639</td>\n","      <td>0.726549</td>\n","      <td>0.802824</td>\n","      <td>0.754435</td>\n","      <td>0.721686</td>\n","      <td>0.790297</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.002600</td>\n","      <td>0.054950</td>\n","      <td>0.810347</td>\n","      <td>0.790806</td>\n","      <td>0.831736</td>\n","      <td>0.929754</td>\n","      <td>0.929754</td>\n","      <td>0.929754</td>\n","      <td>0.759469</td>\n","      <td>0.731425</td>\n","      <td>0.789953</td>\n","      <td>0.748204</td>\n","      <td>0.723848</td>\n","      <td>0.774257</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.001600</td>\n","      <td>0.065089</td>\n","      <td>0.810711</td>\n","      <td>0.799399</td>\n","      <td>0.824153</td>\n","      <td>0.931274</td>\n","      <td>0.931274</td>\n","      <td>0.931274</td>\n","      <td>0.759707</td>\n","      <td>0.744315</td>\n","      <td>0.777939</td>\n","      <td>0.747860</td>\n","      <td>0.743808</td>\n","      <td>0.751956</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.001100</td>\n","      <td>0.055892</td>\n","      <td>0.815915</td>\n","      <td>0.787809</td>\n","      <td>0.849218</td>\n","      <td>0.927356</td>\n","      <td>0.927356</td>\n","      <td>0.927356</td>\n","      <td>0.767693</td>\n","      <td>0.726085</td>\n","      <td>0.816127</td>\n","      <td>0.753195</td>\n","      <td>0.706023</td>\n","      <td>0.807121</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.000900</td>\n","      <td>0.060644</td>\n","      <td>0.815265</td>\n","      <td>0.793792</td>\n","      <td>0.839065</td>\n","      <td>0.928175</td>\n","      <td>0.928175</td>\n","      <td>0.928175</td>\n","      <td>0.766443</td>\n","      <td>0.734884</td>\n","      <td>0.801053</td>\n","      <td>0.748185</td>\n","      <td>0.713525</td>\n","      <td>0.786385</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.000700</td>\n","      <td>0.053666</td>\n","      <td>0.821586</td>\n","      <td>0.803380</td>\n","      <td>0.842107</td>\n","      <td>0.931216</td>\n","      <td>0.931216</td>\n","      <td>0.931216</td>\n","      <td>0.774306</td>\n","      <td>0.747063</td>\n","      <td>0.804583</td>\n","      <td>0.758557</td>\n","      <td>0.723050</td>\n","      <td>0.797731</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.000600</td>\n","      <td>0.059726</td>\n","      <td>0.815736</td>\n","      <td>0.798739</td>\n","      <td>0.834869</td>\n","      <td>0.927999</td>\n","      <td>0.927999</td>\n","      <td>0.927999</td>\n","      <td>0.767171</td>\n","      <td>0.741705</td>\n","      <td>0.795436</td>\n","      <td>0.748228</td>\n","      <td>0.714897</td>\n","      <td>0.784820</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.000600</td>\n","      <td>0.058787</td>\n","      <td>0.822409</td>\n","      <td>0.794988</td>\n","      <td>0.854458</td>\n","      <td>0.929227</td>\n","      <td>0.929227</td>\n","      <td>0.929227</td>\n","      <td>0.775858</td>\n","      <td>0.735028</td>\n","      <td>0.822747</td>\n","      <td>0.757515</td>\n","      <td>0.708831</td>\n","      <td>0.813380</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000600</td>\n","      <td>0.059144</td>\n","      <td>0.822423</td>\n","      <td>0.797018</td>\n","      <td>0.851967</td>\n","      <td>0.929403</td>\n","      <td>0.929403</td>\n","      <td>0.929403</td>\n","      <td>0.775832</td>\n","      <td>0.737935</td>\n","      <td>0.819150</td>\n","      <td>0.757132</td>\n","      <td>0.710852</td>\n","      <td>0.809859</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-3040\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3420\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.8224626438905608, 'eval_macro_precision': 0.7972780260901976, 'eval_macro_recall': 0.851705388734413, 'eval_micro_f1': 0.9294613090015792, 'eval_micro_precision': 0.9294613090015792, 'eval_micro_recall': 0.9294613090015792, 'eval_macro_f1_no_o': 0.7758724307068081, 'eval_macro_precision_no_o': 0.7383028506876913, 'eval_macro_recall_no_o': 0.8187559826318318, 'eval_micro_f1_no_o': 0.7571820677035681, 'eval_micro_precision_no_o': 0.7112409762805088, 'eval_micro_recall_no_o': 0.8094679186228482, 'eval_loss': 0.05914935278548607, 'eval_runtime': 17.0396, 'eval_samples_per_second': 3.521, 'eval_steps_per_second': 0.469, 'epoch': 19.0}\n","Accuracy for fold  29 :  0.7571820677035681  --  0.9294613090015792\n","--------------------------------\n","Testing process has finished.\n"]}]},{"cell_type":"code","source":["loop_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUj8qXz7yfUr","executionInfo":{"status":"ok","timestamp":1647038132901,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"33cd5fe5-d048-48d6-9aea-10adae0a83bd"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.74794573, 0.76577071, 0.76666667, 0.76928867, 0.77362719,\n","       0.77734448, 0.77256588, 0.76350093, 0.76414048, 0.7610226 ,\n","       0.77161863, 0.76669785, 0.76925905, 0.76301887, 0.76923077,\n","       0.76268657, 0.75323714, 0.7630447 , 0.76934636, 0.75754769,\n","       0.74713271, 0.74747475, 0.74859708, 0.7443595 , 0.75705152,\n","       0.76423991, 0.76517451, 0.75792237, 0.75293281, 0.75718207])"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["loop_resultss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GrO5h7ryfqY","executionInfo":{"status":"ok","timestamp":1647038133793,"user_tz":360,"elapsed":632,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"23e3d642-d982-4f61-8487-39bedc2ab848"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.93221033, 0.93343861, 0.93402351, 0.93612915, 0.93712347,\n","       0.93665555, 0.9358367 , 0.934082  , 0.93267825, 0.93267825,\n","       0.93519331, 0.93560274, 0.93402351, 0.9346669 , 0.93601217,\n","       0.93267825, 0.93086506, 0.93402351, 0.93554425, 0.93133298,\n","       0.92782359, 0.93039715, 0.93098204, 0.9306311 , 0.93390653,\n","       0.9346669 , 0.93571972, 0.93361408, 0.92636135, 0.92946131])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["sum = 0.0\n","for value in loop_results:\n","  sum += value\n","print(f'Average micro_f1_no_o: {sum/len(loop_results)} %')\n","\n","sum = 0.0\n","for value in loop_resultss:\n","  sum += value\n","print(f'Average micro_f1: {sum/len(loop_results)} %')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hpMAPShygLg","executionInfo":{"status":"ok","timestamp":1647038133794,"user_tz":360,"elapsed":3,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"f4d18310-217a-41cc-86bd-9a061c964a11"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Average micro_f1_no_o: 0.7616542730424077 %\n","Average micro_f1: 0.9332787428593712 %\n"]}]},{"cell_type":"code","source":["ner_model = NerModel(BertEmbModel).to('cuda')  # make sure we move the model to the GPU for training\n","ner_model.load_state_dict(torch.load(save_path))\n","ner_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ap-NfS2QyibI","executionInfo":{"status":"ok","timestamp":1647038134783,"user_tz":360,"elapsed":991,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"0b4c83c3-89fd-42c7-d129-176b43cff1a6"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NerModel(\n","  (sci_embeddings): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (embd_dropout): Dropout(p=0.1, inplace=False)\n","  (ff_dropout): Dropout(p=0.1, inplace=False)\n","  (ff): Linear(in_features=200, out_features=14, bias=True)\n","  (tanh): Tanh()\n","  (lstm): LSTM(768, 100, bidirectional=True)\n","  (lstm_drop): Dropout(p=0.4, inplace=False)\n","  (ff_act): ReLU()\n","  (classifier): Linear(in_features=14, out_features=4, bias=True)\n",")"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"dcdDIwq0kXy2"},"source":["# Get Values of Thruth"]},{"cell_type":"code","metadata":{"id":"uCnsc2rplFzW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647038134784,"user_tz":360,"elapsed":4,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"5c1ee92c-b731-48e2-f68a-bd5c87c836d6"},"source":["# Pytorch thing (if we aren't training, do this)\n","ner_model.eval()\n","ner_model.to('cuda')"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NerModel(\n","  (sci_embeddings): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (embd_dropout): Dropout(p=0.1, inplace=False)\n","  (ff_dropout): Dropout(p=0.1, inplace=False)\n","  (ff): Linear(in_features=200, out_features=14, bias=True)\n","  (tanh): Tanh()\n","  (lstm): LSTM(768, 100, bidirectional=True)\n","  (lstm_drop): Dropout(p=0.4, inplace=False)\n","  (ff_act): ReLU()\n","  (classifier): Linear(in_features=14, out_features=4, bias=True)\n",")"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"FVfjoHmcI0pL","executionInfo":{"status":"ok","timestamp":1647038174763,"user_tz":360,"elapsed":39981,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["output_preds = []\n","output_real = []\n","for x in range(len(test)):\n","  inputs1 = test[x]['input_ids'][0].clone().detach().to(torch.long).unsqueeze(0).to('cuda')\n","  inputs2 = test[x]['attention_mask'][0].clone().detach().to(torch.long).unsqueeze(0).to('cuda')\n","  inputs = {'input_ids': inputs1,  'attention_mask': inputs2}\n","  #print(inputs)\n","\n","  temp_test = test[x]\n","  temp_out = temp_test.pop(\"labels\")\n","  output_real.append(np.array(temp_out[temp_out != -100])) \n","\n","  gen_preds = ner_model(**inputs)\n","  label_preds = np.argmax(gen_preds.cpu().detach().numpy(), axis=-1)[0]\n","  output_preds.append(label_preds[temp_out != -100])\n"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXBvXjpP-VGh"},"source":["for x in range(len(output_real)):\n","  output_real[x] = [ID2Entity(y) for y in output_real[x]]\n","  output_preds[x] = [ID2Entity(z) for z in output_preds[x]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pd1Xl1giQ7z7"},"source":["#output_real[28]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rslGT68vvC8"},"source":["from sklearn.metrics import mean_squared_error, multilabel_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams['figure.figsize'] = [10, 7]\n","plt.rcParams[\"figure.autolayout\"] = True\n","plt.rcParams.update({'font.size': 13})\n","\n","labels = [\"None\", 'TASK', 'DATASET', 'METRIC']\n","cm = confusion_matrix(output_real[0], output_preds[0], labels=labels)\n","for x in range(len(output_real)-1):\n","  cm += confusion_matrix(output_real[x+1], output_preds[x+1], labels=labels)\n","print(cm)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","disp.plot(cmap=plt.cm.Blues)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m10aBi5x5Hdb"},"source":["del cm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgGd1XfheERK"},"source":["len(output_real)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHlg1QV3eZCa","executionInfo":{"status":"ok","timestamp":1647038175567,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":[""],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0w9Lc0d3iOT"},"source":["# Test over Text"]},{"cell_type":"code","metadata":{"id":"mgNM3p1q3hrf"},"source":["def prepare_input(txt):\n","  inputs = BertTokenizer(txt, return_tensors='pt', padding='max_length', truncation=True, max_length=150).to('cuda')\n","  return inputs\n","\n","input_text = [\"English is shown to be trans-context-free on the basis of coordinations of the respectively type that involve strictly syntactic cross-serial agreement.\"]\n","##input_text = [\"The transient analysis of gyro-elastic structured media, composed of periodically placed masses interconnected by elastic rods and attached to gyroscopic spinners, is presented.\"] \n","##input_text = [\"The results indicated that thermal curing promoted the early strength of mortars, while decreased the late strength of mortars.\"] \n","##input_text = [\"A wide variety of processes are attested in the literature, and we find different forms of clippings in our data, including mixtures of different clippings, homophone respellings, phonetic respellings in-cluding informal oral forms, initialisms (but no acronyms), and mixtures of clipping together with homo-phone and phonetic respellings.\"] \n","\n","#input_text = [\"The goal is to accurately predict the running time of applications for task scheduling and job migration.\"] \n","#input_text = [\"This paper reports on the development of a cross-domain framework for describing complex design practices.\"] \n","#input_text = [\"Studies of inequality in China typically ignore cost of liv-ing differences between areas.\"] \n","#input_text = [\"The present study was designed to explore the long-term differences be-tween three mouse models for depression.\"]\n","#input_text = [\"Finally, regarding professional competencies, teachers appeared to be largely unprepared to conduct language assessments consistent with the LAR demands.\"] \n","\n","##input_text = [\"propose a fast and reliable restoration method of virtual resources on OpenStack when physical servers or virtual machines are down.\"] \n","##input_text = [\"The results from our simulations reveal that the network assisted adaptation clearly outperforms the purely client-based DASH heuristics in some of the metrics, not all of them, particularly, in situations when the achievable throughput is moderately high or the link quality of the mobile clients does not differ from each other substantially.\"] \n","##input_text = [\"For hard rock drilling in coal mine, the drilling efficiency and service life of polycrystalline diamond compact bit are very low.\"] \n","##input_text = [\"Capturing changes in foreign reserves and exchange rates through the exchange market pressure, this article investigates whether economic policy uncertainty plays any role in exchange market pressure movements while controlling for the effects of domestic and external factors.\"] \n","##input_text = [\"This paper presents design of an self contained actuators unit in wide area damping control of power system in stabilizing system response for both nominal system condition and during actuator faults.\"] \n","\n","##input_text = [\"Ultrasound-based brain stimulation techniques may become a powerful new technique to modulate the human brain in a focal and targeted manner.\"] \n","#input_text = [\"Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization.\"]\n","\n","# Tokenize + pad\n","inputs = prepare_input(input_text)\n","\n","#inputs\n","#print(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaG1vUq58BFI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638831900718,"user_tz":360,"elapsed":3,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"045c5828-479d-4273-f77c-9cb0de273455"},"source":["# Pytorch thing (if we aren't training, do this)\n","ner_model.eval()\n","\n","# Get predictions\n","preds = ner_model(**inputs).cpu().detach().numpy()\n","preds = np.argmax(preds, axis=-1)[0]\n","pred_labels = [ID2Entity(x) for x in preds]\n","\n","# Convert token ids to text\n","tokens = BertTokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","\n","# Display result\n","for token, label in zip(tokens, pred_labels):\n","  if token == '[SEP]':\n","    break\n","  if token == '[CLS]':\n","    continue\n","  print('{} -> {}'.format(token, label))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a -> None\n","wide -> None\n","variety -> None\n","of -> None\n","processes -> None\n","are -> None\n","att -> None\n","##ested -> None\n","in -> None\n","the -> None\n","literature -> None\n",", -> None\n","and -> None\n","we -> None\n","find -> None\n","different -> None\n","forms -> None\n","of -> None\n","clip -> None\n","##ping -> None\n","##s -> None\n","in -> None\n","our -> None\n","data -> None\n",", -> None\n","including -> None\n","mixtures -> None\n","of -> None\n","different -> None\n","clip -> None\n","##ping -> None\n","##s -> None\n",", -> None\n","homo -> None\n","##phone -> None\n","resp -> None\n","##elling -> None\n","##s -> None\n",", -> None\n","phonetic -> None\n","resp -> None\n","##elling -> None\n","##s -> None\n","in -> None\n","- -> None\n","cl -> None\n","##uding -> None\n","informal -> None\n","oral -> None\n","forms -> None\n",", -> None\n","initial -> None\n","##isms -> None\n","( -> None\n","but -> None\n","no -> None\n","acr -> None\n","##onyms -> None\n",") -> None\n",", -> None\n","and -> None\n","mixtures -> None\n","of -> None\n","clip -> None\n","##ping -> None\n","together -> None\n","with -> None\n","homo -> None\n","- -> None\n","phone -> None\n","and -> None\n","phonetic -> None\n","resp -> None\n","##elling -> None\n","##s -> None\n",". -> None\n"]}]},{"cell_type":"markdown","metadata":{"id":"hC35X0v72kXB"},"source":["# Model Save and Load"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AnK4H5vP2kJU","executionInfo":{"status":"ok","timestamp":1638821351076,"user_tz":360,"elapsed":10,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"e55e8ead-0b25-4eb8-bdc5-f7376e13eaaf"},"source":["print(\"Our model: \\n\\n\", ner_model, '\\n')\n","print(\"The state dict keys: \\n\\n\", ner_model.state_dict().keys())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our model: \n","\n"," NerModel(\n","  (sci_embeddings): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (embd_dropout): Dropout(p=0.1, inplace=False)\n","  (ff_dropout): Dropout(p=0.1, inplace=False)\n","  (ff): Linear(in_features=200, out_features=14, bias=True)\n","  (tanh): Tanh()\n","  (lstm): LSTM(768, 100, bidirectional=True)\n","  (lstm_drop): Dropout(p=0.4, inplace=False)\n","  (ff_act): ReLU()\n","  (classifier): Linear(in_features=14, out_features=4, bias=True)\n",") \n","\n","The state dict keys: \n","\n"," odict_keys(['sci_embeddings.embeddings.position_ids', 'sci_embeddings.embeddings.word_embeddings.weight', 'sci_embeddings.embeddings.position_embeddings.weight', 'sci_embeddings.embeddings.token_type_embeddings.weight', 'sci_embeddings.embeddings.LayerNorm.weight', 'sci_embeddings.embeddings.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.attention.self.query.weight', 'sci_embeddings.encoder.layer.0.attention.self.query.bias', 'sci_embeddings.encoder.layer.0.attention.self.key.weight', 'sci_embeddings.encoder.layer.0.attention.self.key.bias', 'sci_embeddings.encoder.layer.0.attention.self.value.weight', 'sci_embeddings.encoder.layer.0.attention.self.value.bias', 'sci_embeddings.encoder.layer.0.attention.output.dense.weight', 'sci_embeddings.encoder.layer.0.attention.output.dense.bias', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.intermediate.dense.weight', 'sci_embeddings.encoder.layer.0.intermediate.dense.bias', 'sci_embeddings.encoder.layer.0.output.dense.weight', 'sci_embeddings.encoder.layer.0.output.dense.bias', 'sci_embeddings.encoder.layer.0.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.attention.self.query.weight', 'sci_embeddings.encoder.layer.1.attention.self.query.bias', 'sci_embeddings.encoder.layer.1.attention.self.key.weight', 'sci_embeddings.encoder.layer.1.attention.self.key.bias', 'sci_embeddings.encoder.layer.1.attention.self.value.weight', 'sci_embeddings.encoder.layer.1.attention.self.value.bias', 'sci_embeddings.encoder.layer.1.attention.output.dense.weight', 'sci_embeddings.encoder.layer.1.attention.output.dense.bias', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.intermediate.dense.weight', 'sci_embeddings.encoder.layer.1.intermediate.dense.bias', 'sci_embeddings.encoder.layer.1.output.dense.weight', 'sci_embeddings.encoder.layer.1.output.dense.bias', 'sci_embeddings.encoder.layer.1.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.attention.self.query.weight', 'sci_embeddings.encoder.layer.2.attention.self.query.bias', 'sci_embeddings.encoder.layer.2.attention.self.key.weight', 'sci_embeddings.encoder.layer.2.attention.self.key.bias', 'sci_embeddings.encoder.layer.2.attention.self.value.weight', 'sci_embeddings.encoder.layer.2.attention.self.value.bias', 'sci_embeddings.encoder.layer.2.attention.output.dense.weight', 'sci_embeddings.encoder.layer.2.attention.output.dense.bias', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.intermediate.dense.weight', 'sci_embeddings.encoder.layer.2.intermediate.dense.bias', 'sci_embeddings.encoder.layer.2.output.dense.weight', 'sci_embeddings.encoder.layer.2.output.dense.bias', 'sci_embeddings.encoder.layer.2.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.attention.self.query.weight', 'sci_embeddings.encoder.layer.3.attention.self.query.bias', 'sci_embeddings.encoder.layer.3.attention.self.key.weight', 'sci_embeddings.encoder.layer.3.attention.self.key.bias', 'sci_embeddings.encoder.layer.3.attention.self.value.weight', 'sci_embeddings.encoder.layer.3.attention.self.value.bias', 'sci_embeddings.encoder.layer.3.attention.output.dense.weight', 'sci_embeddings.encoder.layer.3.attention.output.dense.bias', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.intermediate.dense.weight', 'sci_embeddings.encoder.layer.3.intermediate.dense.bias', 'sci_embeddings.encoder.layer.3.output.dense.weight', 'sci_embeddings.encoder.layer.3.output.dense.bias', 'sci_embeddings.encoder.layer.3.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.attention.self.query.weight', 'sci_embeddings.encoder.layer.4.attention.self.query.bias', 'sci_embeddings.encoder.layer.4.attention.self.key.weight', 'sci_embeddings.encoder.layer.4.attention.self.key.bias', 'sci_embeddings.encoder.layer.4.attention.self.value.weight', 'sci_embeddings.encoder.layer.4.attention.self.value.bias', 'sci_embeddings.encoder.layer.4.attention.output.dense.weight', 'sci_embeddings.encoder.layer.4.attention.output.dense.bias', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.intermediate.dense.weight', 'sci_embeddings.encoder.layer.4.intermediate.dense.bias', 'sci_embeddings.encoder.layer.4.output.dense.weight', 'sci_embeddings.encoder.layer.4.output.dense.bias', 'sci_embeddings.encoder.layer.4.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.attention.self.query.weight', 'sci_embeddings.encoder.layer.5.attention.self.query.bias', 'sci_embeddings.encoder.layer.5.attention.self.key.weight', 'sci_embeddings.encoder.layer.5.attention.self.key.bias', 'sci_embeddings.encoder.layer.5.attention.self.value.weight', 'sci_embeddings.encoder.layer.5.attention.self.value.bias', 'sci_embeddings.encoder.layer.5.attention.output.dense.weight', 'sci_embeddings.encoder.layer.5.attention.output.dense.bias', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.intermediate.dense.weight', 'sci_embeddings.encoder.layer.5.intermediate.dense.bias', 'sci_embeddings.encoder.layer.5.output.dense.weight', 'sci_embeddings.encoder.layer.5.output.dense.bias', 'sci_embeddings.encoder.layer.5.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.attention.self.query.weight', 'sci_embeddings.encoder.layer.6.attention.self.query.bias', 'sci_embeddings.encoder.layer.6.attention.self.key.weight', 'sci_embeddings.encoder.layer.6.attention.self.key.bias', 'sci_embeddings.encoder.layer.6.attention.self.value.weight', 'sci_embeddings.encoder.layer.6.attention.self.value.bias', 'sci_embeddings.encoder.layer.6.attention.output.dense.weight', 'sci_embeddings.encoder.layer.6.attention.output.dense.bias', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.intermediate.dense.weight', 'sci_embeddings.encoder.layer.6.intermediate.dense.bias', 'sci_embeddings.encoder.layer.6.output.dense.weight', 'sci_embeddings.encoder.layer.6.output.dense.bias', 'sci_embeddings.encoder.layer.6.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.attention.self.query.weight', 'sci_embeddings.encoder.layer.7.attention.self.query.bias', 'sci_embeddings.encoder.layer.7.attention.self.key.weight', 'sci_embeddings.encoder.layer.7.attention.self.key.bias', 'sci_embeddings.encoder.layer.7.attention.self.value.weight', 'sci_embeddings.encoder.layer.7.attention.self.value.bias', 'sci_embeddings.encoder.layer.7.attention.output.dense.weight', 'sci_embeddings.encoder.layer.7.attention.output.dense.bias', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.intermediate.dense.weight', 'sci_embeddings.encoder.layer.7.intermediate.dense.bias', 'sci_embeddings.encoder.layer.7.output.dense.weight', 'sci_embeddings.encoder.layer.7.output.dense.bias', 'sci_embeddings.encoder.layer.7.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.attention.self.query.weight', 'sci_embeddings.encoder.layer.8.attention.self.query.bias', 'sci_embeddings.encoder.layer.8.attention.self.key.weight', 'sci_embeddings.encoder.layer.8.attention.self.key.bias', 'sci_embeddings.encoder.layer.8.attention.self.value.weight', 'sci_embeddings.encoder.layer.8.attention.self.value.bias', 'sci_embeddings.encoder.layer.8.attention.output.dense.weight', 'sci_embeddings.encoder.layer.8.attention.output.dense.bias', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.intermediate.dense.weight', 'sci_embeddings.encoder.layer.8.intermediate.dense.bias', 'sci_embeddings.encoder.layer.8.output.dense.weight', 'sci_embeddings.encoder.layer.8.output.dense.bias', 'sci_embeddings.encoder.layer.8.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.attention.self.query.weight', 'sci_embeddings.encoder.layer.9.attention.self.query.bias', 'sci_embeddings.encoder.layer.9.attention.self.key.weight', 'sci_embeddings.encoder.layer.9.attention.self.key.bias', 'sci_embeddings.encoder.layer.9.attention.self.value.weight', 'sci_embeddings.encoder.layer.9.attention.self.value.bias', 'sci_embeddings.encoder.layer.9.attention.output.dense.weight', 'sci_embeddings.encoder.layer.9.attention.output.dense.bias', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.intermediate.dense.weight', 'sci_embeddings.encoder.layer.9.intermediate.dense.bias', 'sci_embeddings.encoder.layer.9.output.dense.weight', 'sci_embeddings.encoder.layer.9.output.dense.bias', 'sci_embeddings.encoder.layer.9.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.attention.self.query.weight', 'sci_embeddings.encoder.layer.10.attention.self.query.bias', 'sci_embeddings.encoder.layer.10.attention.self.key.weight', 'sci_embeddings.encoder.layer.10.attention.self.key.bias', 'sci_embeddings.encoder.layer.10.attention.self.value.weight', 'sci_embeddings.encoder.layer.10.attention.self.value.bias', 'sci_embeddings.encoder.layer.10.attention.output.dense.weight', 'sci_embeddings.encoder.layer.10.attention.output.dense.bias', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.intermediate.dense.weight', 'sci_embeddings.encoder.layer.10.intermediate.dense.bias', 'sci_embeddings.encoder.layer.10.output.dense.weight', 'sci_embeddings.encoder.layer.10.output.dense.bias', 'sci_embeddings.encoder.layer.10.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.attention.self.query.weight', 'sci_embeddings.encoder.layer.11.attention.self.query.bias', 'sci_embeddings.encoder.layer.11.attention.self.key.weight', 'sci_embeddings.encoder.layer.11.attention.self.key.bias', 'sci_embeddings.encoder.layer.11.attention.self.value.weight', 'sci_embeddings.encoder.layer.11.attention.self.value.bias', 'sci_embeddings.encoder.layer.11.attention.output.dense.weight', 'sci_embeddings.encoder.layer.11.attention.output.dense.bias', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.intermediate.dense.weight', 'sci_embeddings.encoder.layer.11.intermediate.dense.bias', 'sci_embeddings.encoder.layer.11.output.dense.weight', 'sci_embeddings.encoder.layer.11.output.dense.bias', 'sci_embeddings.encoder.layer.11.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.output.LayerNorm.bias', 'sci_embeddings.pooler.dense.weight', 'sci_embeddings.pooler.dense.bias', 'ff.weight', 'ff.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias'])\n"]}]},{"cell_type":"code","metadata":{"id":"KftdO6GD2rWF"},"source":["torch.save(ner_model.state_dict(), 'trained_model_dic.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"kd4NJxK14P14","executionInfo":{"status":"ok","timestamp":1638821352369,"user_tz":360,"elapsed":7,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"335c36f7-29b2-4d7c-d212-441f42d34dc9"},"source":["# download checkpoint file\n","files.download('trained_model_dic.pth')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_96adb4c2-159a-402a-aa47-28def7a32b49\", \"trained_model_dic.pth\", 442559463)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1hvL5Nb3kmo","executionInfo":{"status":"ok","timestamp":1637655465909,"user_tz":360,"elapsed":11,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"a2910628-2592-4edd-b135-c329d0352c5e"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["checkpoint.pth\t  model-fold-1.pth  model-fold-4.pth  trained_model_dic.pth\n","dev.json\t  model-fold-2.pth  sample_data       trained_scibert_ner_model\n","model-fold-0.pth  model-fold-3.pth  test.json\t      train.json\n"]}]},{"cell_type":"markdown","metadata":{"id":"hQL7fqSY2x9e"},"source":["Loading the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ufmrdfeT6Qz","executionInfo":{"status":"ok","timestamp":1638565657653,"user_tz":360,"elapsed":324,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"fd7fd743-41b0-4b32-eefd-db4f427d5da0"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data  test_500_v2.conll\ttrain_1500_v2.conll  trained_model_dic.pth\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4Eus2gmJ-bS","executionInfo":{"status":"ok","timestamp":1638831550021,"user_tz":360,"elapsed":10896,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"1e8838d1-d51c-47db-8ee2-c0bbb2bb3a0b"},"source":["state_dict = torch.load('trained_model_dic.pth')\n","print(state_dict.keys())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["odict_keys(['sci_embeddings.embeddings.position_ids', 'sci_embeddings.embeddings.word_embeddings.weight', 'sci_embeddings.embeddings.position_embeddings.weight', 'sci_embeddings.embeddings.token_type_embeddings.weight', 'sci_embeddings.embeddings.LayerNorm.weight', 'sci_embeddings.embeddings.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.attention.self.query.weight', 'sci_embeddings.encoder.layer.0.attention.self.query.bias', 'sci_embeddings.encoder.layer.0.attention.self.key.weight', 'sci_embeddings.encoder.layer.0.attention.self.key.bias', 'sci_embeddings.encoder.layer.0.attention.self.value.weight', 'sci_embeddings.encoder.layer.0.attention.self.value.bias', 'sci_embeddings.encoder.layer.0.attention.output.dense.weight', 'sci_embeddings.encoder.layer.0.attention.output.dense.bias', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.intermediate.dense.weight', 'sci_embeddings.encoder.layer.0.intermediate.dense.bias', 'sci_embeddings.encoder.layer.0.output.dense.weight', 'sci_embeddings.encoder.layer.0.output.dense.bias', 'sci_embeddings.encoder.layer.0.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.attention.self.query.weight', 'sci_embeddings.encoder.layer.1.attention.self.query.bias', 'sci_embeddings.encoder.layer.1.attention.self.key.weight', 'sci_embeddings.encoder.layer.1.attention.self.key.bias', 'sci_embeddings.encoder.layer.1.attention.self.value.weight', 'sci_embeddings.encoder.layer.1.attention.self.value.bias', 'sci_embeddings.encoder.layer.1.attention.output.dense.weight', 'sci_embeddings.encoder.layer.1.attention.output.dense.bias', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.intermediate.dense.weight', 'sci_embeddings.encoder.layer.1.intermediate.dense.bias', 'sci_embeddings.encoder.layer.1.output.dense.weight', 'sci_embeddings.encoder.layer.1.output.dense.bias', 'sci_embeddings.encoder.layer.1.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.attention.self.query.weight', 'sci_embeddings.encoder.layer.2.attention.self.query.bias', 'sci_embeddings.encoder.layer.2.attention.self.key.weight', 'sci_embeddings.encoder.layer.2.attention.self.key.bias', 'sci_embeddings.encoder.layer.2.attention.self.value.weight', 'sci_embeddings.encoder.layer.2.attention.self.value.bias', 'sci_embeddings.encoder.layer.2.attention.output.dense.weight', 'sci_embeddings.encoder.layer.2.attention.output.dense.bias', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.intermediate.dense.weight', 'sci_embeddings.encoder.layer.2.intermediate.dense.bias', 'sci_embeddings.encoder.layer.2.output.dense.weight', 'sci_embeddings.encoder.layer.2.output.dense.bias', 'sci_embeddings.encoder.layer.2.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.attention.self.query.weight', 'sci_embeddings.encoder.layer.3.attention.self.query.bias', 'sci_embeddings.encoder.layer.3.attention.self.key.weight', 'sci_embeddings.encoder.layer.3.attention.self.key.bias', 'sci_embeddings.encoder.layer.3.attention.self.value.weight', 'sci_embeddings.encoder.layer.3.attention.self.value.bias', 'sci_embeddings.encoder.layer.3.attention.output.dense.weight', 'sci_embeddings.encoder.layer.3.attention.output.dense.bias', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.intermediate.dense.weight', 'sci_embeddings.encoder.layer.3.intermediate.dense.bias', 'sci_embeddings.encoder.layer.3.output.dense.weight', 'sci_embeddings.encoder.layer.3.output.dense.bias', 'sci_embeddings.encoder.layer.3.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.attention.self.query.weight', 'sci_embeddings.encoder.layer.4.attention.self.query.bias', 'sci_embeddings.encoder.layer.4.attention.self.key.weight', 'sci_embeddings.encoder.layer.4.attention.self.key.bias', 'sci_embeddings.encoder.layer.4.attention.self.value.weight', 'sci_embeddings.encoder.layer.4.attention.self.value.bias', 'sci_embeddings.encoder.layer.4.attention.output.dense.weight', 'sci_embeddings.encoder.layer.4.attention.output.dense.bias', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.intermediate.dense.weight', 'sci_embeddings.encoder.layer.4.intermediate.dense.bias', 'sci_embeddings.encoder.layer.4.output.dense.weight', 'sci_embeddings.encoder.layer.4.output.dense.bias', 'sci_embeddings.encoder.layer.4.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.attention.self.query.weight', 'sci_embeddings.encoder.layer.5.attention.self.query.bias', 'sci_embeddings.encoder.layer.5.attention.self.key.weight', 'sci_embeddings.encoder.layer.5.attention.self.key.bias', 'sci_embeddings.encoder.layer.5.attention.self.value.weight', 'sci_embeddings.encoder.layer.5.attention.self.value.bias', 'sci_embeddings.encoder.layer.5.attention.output.dense.weight', 'sci_embeddings.encoder.layer.5.attention.output.dense.bias', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.intermediate.dense.weight', 'sci_embeddings.encoder.layer.5.intermediate.dense.bias', 'sci_embeddings.encoder.layer.5.output.dense.weight', 'sci_embeddings.encoder.layer.5.output.dense.bias', 'sci_embeddings.encoder.layer.5.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.attention.self.query.weight', 'sci_embeddings.encoder.layer.6.attention.self.query.bias', 'sci_embeddings.encoder.layer.6.attention.self.key.weight', 'sci_embeddings.encoder.layer.6.attention.self.key.bias', 'sci_embeddings.encoder.layer.6.attention.self.value.weight', 'sci_embeddings.encoder.layer.6.attention.self.value.bias', 'sci_embeddings.encoder.layer.6.attention.output.dense.weight', 'sci_embeddings.encoder.layer.6.attention.output.dense.bias', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.intermediate.dense.weight', 'sci_embeddings.encoder.layer.6.intermediate.dense.bias', 'sci_embeddings.encoder.layer.6.output.dense.weight', 'sci_embeddings.encoder.layer.6.output.dense.bias', 'sci_embeddings.encoder.layer.6.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.attention.self.query.weight', 'sci_embeddings.encoder.layer.7.attention.self.query.bias', 'sci_embeddings.encoder.layer.7.attention.self.key.weight', 'sci_embeddings.encoder.layer.7.attention.self.key.bias', 'sci_embeddings.encoder.layer.7.attention.self.value.weight', 'sci_embeddings.encoder.layer.7.attention.self.value.bias', 'sci_embeddings.encoder.layer.7.attention.output.dense.weight', 'sci_embeddings.encoder.layer.7.attention.output.dense.bias', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.intermediate.dense.weight', 'sci_embeddings.encoder.layer.7.intermediate.dense.bias', 'sci_embeddings.encoder.layer.7.output.dense.weight', 'sci_embeddings.encoder.layer.7.output.dense.bias', 'sci_embeddings.encoder.layer.7.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.attention.self.query.weight', 'sci_embeddings.encoder.layer.8.attention.self.query.bias', 'sci_embeddings.encoder.layer.8.attention.self.key.weight', 'sci_embeddings.encoder.layer.8.attention.self.key.bias', 'sci_embeddings.encoder.layer.8.attention.self.value.weight', 'sci_embeddings.encoder.layer.8.attention.self.value.bias', 'sci_embeddings.encoder.layer.8.attention.output.dense.weight', 'sci_embeddings.encoder.layer.8.attention.output.dense.bias', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.intermediate.dense.weight', 'sci_embeddings.encoder.layer.8.intermediate.dense.bias', 'sci_embeddings.encoder.layer.8.output.dense.weight', 'sci_embeddings.encoder.layer.8.output.dense.bias', 'sci_embeddings.encoder.layer.8.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.attention.self.query.weight', 'sci_embeddings.encoder.layer.9.attention.self.query.bias', 'sci_embeddings.encoder.layer.9.attention.self.key.weight', 'sci_embeddings.encoder.layer.9.attention.self.key.bias', 'sci_embeddings.encoder.layer.9.attention.self.value.weight', 'sci_embeddings.encoder.layer.9.attention.self.value.bias', 'sci_embeddings.encoder.layer.9.attention.output.dense.weight', 'sci_embeddings.encoder.layer.9.attention.output.dense.bias', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.intermediate.dense.weight', 'sci_embeddings.encoder.layer.9.intermediate.dense.bias', 'sci_embeddings.encoder.layer.9.output.dense.weight', 'sci_embeddings.encoder.layer.9.output.dense.bias', 'sci_embeddings.encoder.layer.9.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.attention.self.query.weight', 'sci_embeddings.encoder.layer.10.attention.self.query.bias', 'sci_embeddings.encoder.layer.10.attention.self.key.weight', 'sci_embeddings.encoder.layer.10.attention.self.key.bias', 'sci_embeddings.encoder.layer.10.attention.self.value.weight', 'sci_embeddings.encoder.layer.10.attention.self.value.bias', 'sci_embeddings.encoder.layer.10.attention.output.dense.weight', 'sci_embeddings.encoder.layer.10.attention.output.dense.bias', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.intermediate.dense.weight', 'sci_embeddings.encoder.layer.10.intermediate.dense.bias', 'sci_embeddings.encoder.layer.10.output.dense.weight', 'sci_embeddings.encoder.layer.10.output.dense.bias', 'sci_embeddings.encoder.layer.10.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.attention.self.query.weight', 'sci_embeddings.encoder.layer.11.attention.self.query.bias', 'sci_embeddings.encoder.layer.11.attention.self.key.weight', 'sci_embeddings.encoder.layer.11.attention.self.key.bias', 'sci_embeddings.encoder.layer.11.attention.self.value.weight', 'sci_embeddings.encoder.layer.11.attention.self.value.bias', 'sci_embeddings.encoder.layer.11.attention.output.dense.weight', 'sci_embeddings.encoder.layer.11.attention.output.dense.bias', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.intermediate.dense.weight', 'sci_embeddings.encoder.layer.11.intermediate.dense.bias', 'sci_embeddings.encoder.layer.11.output.dense.weight', 'sci_embeddings.encoder.layer.11.output.dense.bias', 'sci_embeddings.encoder.layer.11.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.output.LayerNorm.bias', 'sci_embeddings.pooler.dense.weight', 'sci_embeddings.pooler.dense.bias', 'ff.weight', 'ff.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias'])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCFtuyBv3ADq","executionInfo":{"status":"ok","timestamp":1638831550219,"user_tz":360,"elapsed":209,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"40928121-bd3b-422d-e0c7-53907b33e69b"},"source":["ner_model = NerModel(BertEmbModel).to('cuda')\n","ner_model.load_state_dict(state_dict)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"ymiejDO5srYE"},"source":["# Obtain datasets' weights values (Do not run - fixed values)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GsRs55zcsxhk","executionInfo":{"status":"ok","timestamp":1638631909440,"user_tz":360,"elapsed":30431660,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"3e40179a-10be-42e6-bcf8-aa02fc919459"},"source":["zero = 0\n","one=0\n","two=0 \n","three=0\n","four=0\n","five=0\n","six=0\n","local_set = train\n","for i in range(len(local_set)):\n","  for j in range(len(local_set[i]['labels'])):\n","    if local_set[i]['labels'][j] == 0:\n","      zero += 1\n","    elif local_set[i]['labels'][j] == 1:\n","      one += 1\n","    elif local_set[i]['labels'][j] == 2:\n","      two += 1\n","    elif local_set[i]['labels'][j] == 3:\n","      three += 1\n","    elif local_set[i]['labels'][j] == 4:\n","      four += 1\n","    elif local_set[i]['labels'][j] == 5:\n","      five += 1\n","    elif local_set[i]['labels'][j] == 6:\n","      six += 1\n","    \n","print('0: ', zero, '1: ', one, '2: ', two, '3: ', three, '4: ', four, '5: ', five, '6: ', six)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:  48170 1:  4364 2:  1859 3:  1133 4:  0 5:  0 6:  0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzN52yRfsym4","executionInfo":{"status":"ok","timestamp":1638635605524,"user_tz":360,"elapsed":3696093,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"3569920b-8cc2-4634-e9a1-8dc024372263"},"source":["zero = 0\n","one=0\n","two=0 \n","three=0\n","four=0\n","five=0\n","six=0\n","local_set = test\n","for i in range(len(local_set)):\n","  for j in range(len(local_set[i]['labels'])):\n","    if local_set[i]['labels'][j] == 0:\n","      zero += 1\n","    elif local_set[i]['labels'][j] == 1:\n","      one += 1\n","    elif local_set[i]['labels'][j] == 2:\n","      two += 1\n","    elif local_set[i]['labels'][j] == 3:\n","      three += 1\n","    elif local_set[i]['labels'][j] == 4:\n","      four += 1\n","    elif local_set[i]['labels'][j] == 5:\n","      five += 1\n","    elif local_set[i]['labels'][j] == 6:\n","      six += 1\n","    \n","print('0: ', zero, '1: ', one, '2: ', two, '3: ', three, '4: ', four, '5: ', five, '6: ', six)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:  14719 1:  1337 2:  854 3:  386 4:  0 5:  0 6:  0\n"]}]},{"cell_type":"code","metadata":{"id":"eEKHdlDyy1T2"},"source":[""],"execution_count":null,"outputs":[]}]}