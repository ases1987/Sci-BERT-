{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of NCBI+SciModel-Spans.ipynb","provenance":[{"file_id":"1apRBKJ_i2t4gsKy0TfRAM0Gih4gDkI15","timestamp":1634538734401}],"collapsed_sections":["LyMVIljFrioE","52t8l945rf1Y","-DBrafz0KqEq","oGYV8ipcJ5fU","7LthVVOhx_Sz","1Nh8Lcpvb_F4","1DkDHptAhniL","dcdDIwq0kXy2","k0w9Lc0d3iOT","hC35X0v72kXB","ymiejDO5srYE"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2a28a1cb32a2455f85c55133f8d94ee5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e20b83227d7f497c8611272fe6f84b0c","IPY_MODEL_416430c6d9184944a5ddc5a775ee0ffa","IPY_MODEL_15f6189536bd462081fe9561981ab94c"],"layout":"IPY_MODEL_51056f5c2fc34d1a8377372e09a06acd"}},"e20b83227d7f497c8611272fe6f84b0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13893323ab8d462bbddc77fc3b10e4fa","placeholder":"​","style":"IPY_MODEL_12e38cb400214465b6fa4b6d18067c60","value":"Downloading: 100%"}},"416430c6d9184944a5ddc5a775ee0ffa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f77def132fc2413fad8e0d3559bb2a99","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fca3bdde2be4aa69dbe404e8d0e0f5b","value":385}},"15f6189536bd462081fe9561981ab94c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_162100ae9dca4676bd19d8bf0332c1a1","placeholder":"​","style":"IPY_MODEL_80e0c7f388ad4358b8fbc91c121851c0","value":" 385/385 [00:00&lt;00:00, 2.30kB/s]"}},"51056f5c2fc34d1a8377372e09a06acd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13893323ab8d462bbddc77fc3b10e4fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12e38cb400214465b6fa4b6d18067c60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f77def132fc2413fad8e0d3559bb2a99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fca3bdde2be4aa69dbe404e8d0e0f5b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"162100ae9dca4676bd19d8bf0332c1a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80e0c7f388ad4358b8fbc91c121851c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a964d293c4874332bd7ee98f097786e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0fd43643a42d4d4ab5c9a21364eb4172","IPY_MODEL_863adfa245bc44f88db1487cc56d112e","IPY_MODEL_e9b6d93c9d4a44879f330bfb931c9c69"],"layout":"IPY_MODEL_e89e97574ebd43519904b24431ac5810"}},"0fd43643a42d4d4ab5c9a21364eb4172":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7d5a4a2f5654dcf8365453adbe199d4","placeholder":"​","style":"IPY_MODEL_f1b8f2d6d4184ee1a303978456c901be","value":"Downloading: 100%"}},"863adfa245bc44f88db1487cc56d112e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_593489a81c864d51915c410c130a9f2c","max":227845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09f850f21f5a49eb8c4217aac7de49a5","value":227845}},"e9b6d93c9d4a44879f330bfb931c9c69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e20e23b602242b0a5a2e9cda5488bca","placeholder":"​","style":"IPY_MODEL_a8bcdc3fe92f42c294bc37a0263db75a","value":" 223k/223k [00:00&lt;00:00, 553kB/s]"}},"e89e97574ebd43519904b24431ac5810":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7d5a4a2f5654dcf8365453adbe199d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1b8f2d6d4184ee1a303978456c901be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"593489a81c864d51915c410c130a9f2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09f850f21f5a49eb8c4217aac7de49a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e20e23b602242b0a5a2e9cda5488bca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8bcdc3fe92f42c294bc37a0263db75a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f09309b947cd453e805209e7970b13f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4912cc43b9204016bddb2aef37ba381c","IPY_MODEL_1bfef27d85f94c3f9ec07c375026479b","IPY_MODEL_2a87ca7647ad4b18a649a4c865d110d4"],"layout":"IPY_MODEL_5348940775444e969a2bd90ca656a94f"}},"4912cc43b9204016bddb2aef37ba381c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2f5a0da437b4d7fac1e5dbb1657d7fc","placeholder":"​","style":"IPY_MODEL_8abff36902174f0e9ccd0acebd846962","value":"Downloading: 100%"}},"1bfef27d85f94c3f9ec07c375026479b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bc97bfedcf944958ce39e7643a5d9cf","max":442221694,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd1afa9711cc4eb2943db5d378e057f2","value":442221694}},"2a87ca7647ad4b18a649a4c865d110d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_464534dbbaf3452f826f4451f0bae884","placeholder":"​","style":"IPY_MODEL_2dad95aa3a4547509e605902b841a0c2","value":" 422M/422M [00:14&lt;00:00, 31.9MB/s]"}},"5348940775444e969a2bd90ca656a94f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2f5a0da437b4d7fac1e5dbb1657d7fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8abff36902174f0e9ccd0acebd846962":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bc97bfedcf944958ce39e7643a5d9cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd1afa9711cc4eb2943db5d378e057f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"464534dbbaf3452f826f4451f0bae884":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dad95aa3a4547509e605902b841a0c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b14112d8b43e494eb42fb942fcd0fa56":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84ceedb82bfc4f668d26f886aa09476e","IPY_MODEL_2942e439976344fe803cc697974e6c0d","IPY_MODEL_42e56086231a4d01931acd52b24fd1b2"],"layout":"IPY_MODEL_1378501faeb44f2fb9fdd6a746f23eaa"}},"84ceedb82bfc4f668d26f886aa09476e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9b9d705c8df40b2880aa6bb528810a4","placeholder":"​","style":"IPY_MODEL_4f45438cfa4a456694469a29a79b4bc7","value":"Downloading builder script: "}},"2942e439976344fe803cc697974e6c0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b1e2f0f14444c88beacf9a82ff2f4e5","max":2279,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a67587ef3391414a9f08e6a854208bd9","value":2279}},"42e56086231a4d01931acd52b24fd1b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_933063e123f245d89682e1469b0bc362","placeholder":"​","style":"IPY_MODEL_2a0d42e3fb174e8eb26458bab652412e","value":" 5.83k/? [00:00&lt;00:00, 12.3kB/s]"}},"1378501faeb44f2fb9fdd6a746f23eaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9b9d705c8df40b2880aa6bb528810a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f45438cfa4a456694469a29a79b4bc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b1e2f0f14444c88beacf9a82ff2f4e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a67587ef3391414a9f08e6a854208bd9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"933063e123f245d89682e1469b0bc362":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a0d42e3fb174e8eb26458bab652412e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b153ada7a6f48d98314c40a2c1e10a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7cba806d2f842a9b06ca153b8ae7d34","IPY_MODEL_c3be49dd23f243b294c45be6e5def385","IPY_MODEL_98f109d3eb234db2ad6ede1f409a1be7"],"layout":"IPY_MODEL_3062898faa7749e897b5c0cfaf9faee5"}},"f7cba806d2f842a9b06ca153b8ae7d34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48e7394fc06842b89141d9901f668e9b","placeholder":"​","style":"IPY_MODEL_c3526a469559487e8c3a002920fa1039","value":"Downloading metadata: "}},"c3be49dd23f243b294c45be6e5def385":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b23ce2b74e294485ab248a4d4527aae3","max":1549,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce4dc20994174cf384fb07c980510413","value":1549}},"98f109d3eb234db2ad6ede1f409a1be7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e7e718ad0d341ac92b0dfee7e5178e9","placeholder":"​","style":"IPY_MODEL_10b6e25fca26499b872f25d2caecf305","value":" 3.45k/? [00:00&lt;00:00, 89.5kB/s]"}},"3062898faa7749e897b5c0cfaf9faee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48e7394fc06842b89141d9901f668e9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3526a469559487e8c3a002920fa1039":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b23ce2b74e294485ab248a4d4527aae3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce4dc20994174cf384fb07c980510413":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e7e718ad0d341ac92b0dfee7e5178e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10b6e25fca26499b872f25d2caecf305":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b18fe8d640c24d379ecdb9b75a244d50":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_736b03aee07643c0b13ba5a88f603660","IPY_MODEL_3622792b77c84219a0daa23724bebe40","IPY_MODEL_262c5331e4a9489eb781cbdc4ce0bbb9"],"layout":"IPY_MODEL_d82e61e4aebc47edabd14f8870f99fb3"}},"736b03aee07643c0b13ba5a88f603660":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9237167b3b344f84b0c19871b510a497","placeholder":"​","style":"IPY_MODEL_db60177a5469418ebae6366c52f56726","value":"Downloading data files: 100%"}},"3622792b77c84219a0daa23724bebe40":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae8f7b5d3a8d46d1a4dc0a2dc73355ca","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79fd9f04ce6740aeaa00e5603ddd1428","value":3}},"262c5331e4a9489eb781cbdc4ce0bbb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_211e01b459da4ffd817b9424d3d17eeb","placeholder":"​","style":"IPY_MODEL_6739ecb4543d49a4b3a02934825c3b44","value":" 3/3 [00:03&lt;00:00,  1.05it/s]"}},"d82e61e4aebc47edabd14f8870f99fb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9237167b3b344f84b0c19871b510a497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db60177a5469418ebae6366c52f56726":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae8f7b5d3a8d46d1a4dc0a2dc73355ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79fd9f04ce6740aeaa00e5603ddd1428":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"211e01b459da4ffd817b9424d3d17eeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6739ecb4543d49a4b3a02934825c3b44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2822e7cbe17a4331b1adf6152e795f90":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80a65ff8621741a2bb3a8183d1a2a01c","IPY_MODEL_f5d41bcd86b3477e9b023071b5c3776b","IPY_MODEL_3d4be30874b64c638eb4016186fce5f3"],"layout":"IPY_MODEL_8b289ef1e91949f49a7e29a3eee983d5"}},"80a65ff8621741a2bb3a8183d1a2a01c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db172f4cf8b045f9b81320d26748cbc4","placeholder":"​","style":"IPY_MODEL_58fe2b00d3f24a93968a52ec7a92f527","value":"Downloading data: "}},"f5d41bcd86b3477e9b023071b5c3776b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2598b313a7714ca5ba020302357c7d3b","max":283883,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1ff079b1ea8446cb69ae9ec86d76aa6","value":283883}},"3d4be30874b64c638eb4016186fce5f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed8877a88fa04248bd25f44caeb5fe71","placeholder":"​","style":"IPY_MODEL_99948c26e79145ff954a9e86ff9e0c97","value":" 1.14M/? [00:00&lt;00:00, 570kB/s]"}},"8b289ef1e91949f49a7e29a3eee983d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db172f4cf8b045f9b81320d26748cbc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58fe2b00d3f24a93968a52ec7a92f527":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2598b313a7714ca5ba020302357c7d3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1ff079b1ea8446cb69ae9ec86d76aa6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed8877a88fa04248bd25f44caeb5fe71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99948c26e79145ff954a9e86ff9e0c97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72082e68039644f3acf8460c7a29e962":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b066ab5bf7da4cf286474b1486bfc239","IPY_MODEL_c5c78e2c7fe34ccca9ed724409cee264","IPY_MODEL_e2690c8be389450f992b5cbc4914d24a"],"layout":"IPY_MODEL_7321262b3ca843ad801a7c1395b31d7b"}},"b066ab5bf7da4cf286474b1486bfc239":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dd214ee6ee5404a9248846629a385c6","placeholder":"​","style":"IPY_MODEL_5aa05b81a27049e489bb7acef01172bd","value":"Downloading data: "}},"c5c78e2c7fe34ccca9ed724409cee264":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_382ce164ce924bcebba9e7931b6e77e5","max":51200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_efb42bb26bb2498dbedca6d11c3adc05","value":51200}},"e2690c8be389450f992b5cbc4914d24a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_599a0a7fe0ac4be48d66dd5c5c76c373","placeholder":"​","style":"IPY_MODEL_ae56f05e9dff4a91b86c96159b96ebeb","value":" 200k/? [00:00&lt;00:00, 23.1kB/s]"}},"7321262b3ca843ad801a7c1395b31d7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dd214ee6ee5404a9248846629a385c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5aa05b81a27049e489bb7acef01172bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"382ce164ce924bcebba9e7931b6e77e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efb42bb26bb2498dbedca6d11c3adc05":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"599a0a7fe0ac4be48d66dd5c5c76c373":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae56f05e9dff4a91b86c96159b96ebeb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"989ead36537844f49ba8dc0acf70613c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b18d931ac2e4c178bc5fca88a776277","IPY_MODEL_9a5f9ce2d8e54617b255445713c097ce","IPY_MODEL_d518508be712437c852130effd0d8807"],"layout":"IPY_MODEL_901f2d67948a423d91d881ac8275c0f2"}},"9b18d931ac2e4c178bc5fca88a776277":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc2f5b836d864ad6997bb64c8c5f4041","placeholder":"​","style":"IPY_MODEL_d84bb4374697435a8c89efd41c5cf8bf","value":"Downloading data: "}},"9a5f9ce2d8e54617b255445713c097ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ec6fef81a114ca4946654d0c2a89ea7","max":52411,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad1cb29f5563458bbca33eb4014523ee","value":52411}},"d518508be712437c852130effd0d8807":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5ab166961c64b2a9efdda974f1271a4","placeholder":"​","style":"IPY_MODEL_37e7ac5d0eb249d0bfa7a1d9a3bd3f5f","value":" 206k/? [00:00&lt;00:00, 17.6kB/s]"}},"901f2d67948a423d91d881ac8275c0f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc2f5b836d864ad6997bb64c8c5f4041":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d84bb4374697435a8c89efd41c5cf8bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ec6fef81a114ca4946654d0c2a89ea7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad1cb29f5563458bbca33eb4014523ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5ab166961c64b2a9efdda974f1271a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37e7ac5d0eb249d0bfa7a1d9a3bd3f5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbe78ba843e246debd7c71c5a5d4a9a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1236d9ad04394f9887ede9962dd40ede","IPY_MODEL_a2625d6217654452af75bf22a12f79c7","IPY_MODEL_ea7f84b6ea544ec4b8d9d340adc5ae60"],"layout":"IPY_MODEL_47de19d6475a44cfbace74a7646edb97"}},"1236d9ad04394f9887ede9962dd40ede":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f681a2e32224dc693f43ffaf637ee43","placeholder":"​","style":"IPY_MODEL_bc0c47a3258c49498ab0f3d95b368ec0","value":"Extracting data files: 100%"}},"a2625d6217654452af75bf22a12f79c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c653c782a84469aabc1d49a40a1121","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a54ee5edceec489185538b51fa7eb322","value":3}},"ea7f84b6ea544ec4b8d9d340adc5ae60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33011719e1584c28918149b7d90cb21d","placeholder":"​","style":"IPY_MODEL_e82c4914887c4d8bb825b90a278a37e2","value":" 3/3 [00:00&lt;00:00,  6.80it/s]"}},"47de19d6475a44cfbace74a7646edb97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f681a2e32224dc693f43ffaf637ee43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc0c47a3258c49498ab0f3d95b368ec0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37c653c782a84469aabc1d49a40a1121":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a54ee5edceec489185538b51fa7eb322":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33011719e1584c28918149b7d90cb21d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e82c4914887c4d8bb825b90a278a37e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b522e7c2fba5461f98f8691934619075":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7d3e7d40cf94812a646ca92fbb7d631","IPY_MODEL_ce52cf0c792545f88e1456170692e6c7","IPY_MODEL_01a8e59d38644cefb19d8caf6f7e0079"],"layout":"IPY_MODEL_def8586d33364aa799b3242917e4115e"}},"f7d3e7d40cf94812a646ca92fbb7d631":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa8971df0ae14505a5d87c0c8ed413bb","placeholder":"​","style":"IPY_MODEL_93d7fd62dcbe4311bc560d0779372c68","value":"Generating train split:  99%"}},"ce52cf0c792545f88e1456170692e6c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f8870380eb74010bb8bfa51589ba57c","max":5433,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9a625fa4fe6497d99c1747fbc11b65a","value":5433}},"01a8e59d38644cefb19d8caf6f7e0079":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5850f948d43c48cba9828e6282a679ba","placeholder":"​","style":"IPY_MODEL_1e5b4a843aae4f1392f9b8ede906bb37","value":" 5365/5433 [00:03&lt;00:00, 1616.21 examples/s]"}},"def8586d33364aa799b3242917e4115e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa8971df0ae14505a5d87c0c8ed413bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93d7fd62dcbe4311bc560d0779372c68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f8870380eb74010bb8bfa51589ba57c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9a625fa4fe6497d99c1747fbc11b65a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5850f948d43c48cba9828e6282a679ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e5b4a843aae4f1392f9b8ede906bb37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5aad71d5bd514aed9fa81bbd201d7ddd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_566abe8b885d49f2b92ca76049cd07ee","IPY_MODEL_972ec385fb6b48b99947cacf3d3fe3ed","IPY_MODEL_834c063b6ede4979b217da60c45f3d5e"],"layout":"IPY_MODEL_aedde6a7e66644a0bb2f4b9b568fdd11"}},"566abe8b885d49f2b92ca76049cd07ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e17f9f46f949482fb22a47ea6fbe6129","placeholder":"​","style":"IPY_MODEL_ca18e46bfa1e4be691326110867110fe","value":"Generating validation split:  81%"}},"972ec385fb6b48b99947cacf3d3fe3ed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5fc59a562a840ec88e2ddf4b1c97548","max":924,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c5ff123b47e4efcafd881aea50dcbf3","value":924}},"834c063b6ede4979b217da60c45f3d5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abafaef8b5624b8b863b6097d7abd72e","placeholder":"​","style":"IPY_MODEL_c5808e4522c44a27a1887e1162f3d5d2","value":" 749/924 [00:00&lt;00:00, 1138.07 examples/s]"}},"aedde6a7e66644a0bb2f4b9b568fdd11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e17f9f46f949482fb22a47ea6fbe6129":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca18e46bfa1e4be691326110867110fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5fc59a562a840ec88e2ddf4b1c97548":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c5ff123b47e4efcafd881aea50dcbf3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"abafaef8b5624b8b863b6097d7abd72e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5808e4522c44a27a1887e1162f3d5d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b29f0425af5475092ffe87c9db3b9d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9525f147411543dabb311d4a25b5af4a","IPY_MODEL_960bec42c84148eeb59e208124df9d4f","IPY_MODEL_60e00b1140e14cbcbe94f00c1a580db8"],"layout":"IPY_MODEL_7d6728036a50468e80ec84765d6de529"}},"9525f147411543dabb311d4a25b5af4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16cbfa0cf6784639bbbaa56bb22d6291","placeholder":"​","style":"IPY_MODEL_a688227af280476a81fcfb6f953ccc0f","value":"Generating test split:  89%"}},"960bec42c84148eeb59e208124df9d4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e1070186ef04cd19e02c6ebbccc3672","max":941,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bdf2483a28404ab3ba59f1fc9859174b","value":941}},"60e00b1140e14cbcbe94f00c1a580db8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4b0a2736f214185981753af03d33851","placeholder":"​","style":"IPY_MODEL_c776c0d517a0453087a296b26f7b19ec","value":" 839/941 [00:00&lt;00:00, 1359.82 examples/s]"}},"7d6728036a50468e80ec84765d6de529":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16cbfa0cf6784639bbbaa56bb22d6291":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a688227af280476a81fcfb6f953ccc0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e1070186ef04cd19e02c6ebbccc3672":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdf2483a28404ab3ba59f1fc9859174b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d4b0a2736f214185981753af03d33851":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c776c0d517a0453087a296b26f7b19ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQ6H5sYFfFCM","executionInfo":{"status":"ok","timestamp":1638577821479,"user_tz":360,"elapsed":242,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"1c6f9353-196c-43fa-a996-765dca75925e"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data\n"]}]},{"cell_type":"markdown","metadata":{"id":"LyMVIljFrioE"},"source":["#External Databases"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeeP8a6fKXOX","executionInfo":{"status":"ok","timestamp":1649118558452,"user_tz":360,"elapsed":43364,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"21f5cf55-993e-4521-87a3-e6fca449026b"},"source":["! pip install git+https://github.com/huggingface/transformers.git"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-4qx67933\n","  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-4qx67933\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 8.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 37.8 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 42.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3958421 sha256=39422c2a24798545d033bfd70766b39e1fb75f3db4f5f39dcb3400c0a8671dc5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-26iyckio/wheels/90/a5/44/6bcd83827c8a60628c5ad602f429cd5076bcce5f2a90054947\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0.dev0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_L4bHxBGsZk_","executionInfo":{"status":"ok","timestamp":1649118583944,"user_tz":360,"elapsed":25497,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"7f0ca919-865f-4948-aa49-92c93b0bf000"},"source":["! pip install ray[tune]"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ray[tune]\n","  Downloading ray-1.11.0-cp37-cp37m-manylinux2014_x86_64.whl (52.7 MB)\n","\u001b[K     |████████████████████████████████| 52.7 MB 106 kB/s \n","\u001b[?25hCollecting grpcio<=1.43.0,>=1.28.1\n","  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 19.4 MB/s \n","\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.3.3)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.3)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.6.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (21.4.0)\n","Collecting redis>=3.5.0\n","  Downloading redis-4.2.2-py3-none-any.whl (226 kB)\n","\u001b[K     |████████████████████████████████| 226 kB 39.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.21.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.9)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.5)\n","Collecting tensorboardX>=1.9\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 35.2 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (3.10.0.2)\n","Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (4.11.3)\n","Collecting deprecated>=1.2.3\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Collecting async-timeout>=4.0.2\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (21.3)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[tune]) (1.14.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis>=3.5.0->ray[tune]) (3.0.7)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (5.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (0.18.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n","Installing collected packages: deprecated, async-timeout, redis, grpcio, tensorboardX, ray\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.44.0\n","    Uninstalling grpcio-1.44.0:\n","      Successfully uninstalled grpcio-1.44.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n","Successfully installed async-timeout-4.0.2 deprecated-1.2.13 grpcio-1.43.0 ray-1.11.0 redis-4.2.2 tensorboardX-2.5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UI0tseYdav7D","executionInfo":{"status":"ok","timestamp":1649118603981,"user_tz":360,"elapsed":20041,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"81424a98-671a-454a-825a-25609b08eac0"},"source":["!pip install datasets"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[K     |████████████████████████████████| 325 kB 8.1 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 47.3 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 45.7 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 32.9 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 37.1 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.0 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 41.4 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 35.6 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install seqeval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfUV5KeocjhS","executionInfo":{"status":"ok","timestamp":1649118617586,"user_tz":360,"elapsed":13612,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"c6308c36-ebb7-4958-9a14-827e73f84175"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=042a0660ea13a8668e51ce5e7d9df584ff2014d4f4587b93ee29ac552b0340d4\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"52t8l945rf1Y"},"source":["#Libraries"]},{"cell_type":"code","metadata":{"id":"qSbdGMYCgquq","colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["2a28a1cb32a2455f85c55133f8d94ee5","e20b83227d7f497c8611272fe6f84b0c","416430c6d9184944a5ddc5a775ee0ffa","15f6189536bd462081fe9561981ab94c","51056f5c2fc34d1a8377372e09a06acd","13893323ab8d462bbddc77fc3b10e4fa","12e38cb400214465b6fa4b6d18067c60","f77def132fc2413fad8e0d3559bb2a99","4fca3bdde2be4aa69dbe404e8d0e0f5b","162100ae9dca4676bd19d8bf0332c1a1","80e0c7f388ad4358b8fbc91c121851c0","a964d293c4874332bd7ee98f097786e6","0fd43643a42d4d4ab5c9a21364eb4172","863adfa245bc44f88db1487cc56d112e","e9b6d93c9d4a44879f330bfb931c9c69","e89e97574ebd43519904b24431ac5810","d7d5a4a2f5654dcf8365453adbe199d4","f1b8f2d6d4184ee1a303978456c901be","593489a81c864d51915c410c130a9f2c","09f850f21f5a49eb8c4217aac7de49a5","3e20e23b602242b0a5a2e9cda5488bca","a8bcdc3fe92f42c294bc37a0263db75a","f09309b947cd453e805209e7970b13f0","4912cc43b9204016bddb2aef37ba381c","1bfef27d85f94c3f9ec07c375026479b","2a87ca7647ad4b18a649a4c865d110d4","5348940775444e969a2bd90ca656a94f","e2f5a0da437b4d7fac1e5dbb1657d7fc","8abff36902174f0e9ccd0acebd846962","6bc97bfedcf944958ce39e7643a5d9cf","bd1afa9711cc4eb2943db5d378e057f2","464534dbbaf3452f826f4451f0bae884","2dad95aa3a4547509e605902b841a0c2"]},"executionInfo":{"status":"ok","timestamp":1649118652992,"user_tz":360,"elapsed":35409,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"1f43f3f6-c1d7-499e-d244-a037c99b6fcc"},"source":["from google.colab import files\n","\n","import os\n","import re\n","import json\n","import string\n","\n","import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","#import tensorflow as tf\n","from tensorflow import keras\n","#from tensorflow.keras import layers\n","\n","from datasets import load_metric\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from seqeval.metrics import classification_report\n","from sklearn.metrics import precision_recall_fscore_support as prfs\n","\n","import tensorflow_hub as hub\n","from keras import backend as K\n","\n","import transformers\n","print(transformers.__version__)\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers.trainer_utils import EvalLoopOutput\n","\n","from transformers import AutoTokenizer, AutoModel\n","BertTokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n","BertEmbModel = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["4.18.0.dev0\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a28a1cb32a2455f85c55133f8d94ee5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/223k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a964d293c4874332bd7ee98f097786e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/422M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f09309b947cd453e805209e7970b13f0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","metadata":{"id":"-DBrafz0KqEq"},"source":["#Loading Dataset"]},{"cell_type":"code","source":["class NCBIDataset(torch.utils.data.Dataset):\n","  def __init__(self, raw_dataset, max_length=180):\n","    self.raw_x = [x['tokens'] for x in raw_dataset]\n","    self.raw_y = [x['ner_tags'] for x in raw_dataset]\n","    \n","    self.max_length = max_length\n","\n","  def tokenize_and_preserve_labels(self, sentence, text_labels):\n","    \"\"\"\n","    The tokenizer can split single words into multiple tokens - this breaks\n","    the labels, so we need to keep track of this!\n","    \"\"\"\n","    tokenized_sentence = []\n","    labels = []\n","\n","    for word, label in zip(sentence, text_labels):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = BertTokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels\n","\n","  def __getitem__(self, idx):\n","    tokens = self.raw_x[idx]\n","    labels = np.zeros((len(tokens)))\n","    for i, label in enumerate(self.raw_y[idx]):\n","      if label == 2:\n","        labels[i] = 1\n","      else:\n","        labels[i] = label\n","\n","    # This could be moved to __init__ to save time?\n","    tokens, labels = self.tokenize_and_preserve_labels(tokens, labels)\n","\n","    # Convert each token to an id number \n","    input_ids = BertTokenizer.convert_tokens_to_ids(tokens)\n","    \n","    # add and adjust for special tokens\n","    input_ids = [BertTokenizer.cls_token_id] + input_ids + [BertTokenizer.sep_token_id]  \n","    labels = [0] + labels + [0]\n","\n","    # Pad inputs\n","    input_ids = torch.tensor(np.pad(input_ids, [0, self.max_length-len(input_ids)]))\n","    labels = torch.tensor(np.pad(labels, [0, self.max_length-len(labels)], constant_values=-100))\n","\n","    attention_mask = torch.tensor([int(i != 0) for i in input_ids])\n","\n","    #return {input_ids, labels}\n","    #return {'input_ids': [input_ids], 'labels': labels}\n","    return {'input_ids': [input_ids], 'attention_mask': [attention_mask], 'labels': labels}\n","\n","  def __len__(self):\n","    return len(self.raw_y)"],"metadata":{"id":"_vZ1ILIWq9do","executionInfo":{"status":"ok","timestamp":1649118652992,"user_tz":360,"elapsed":7,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Entity mapping, uses BIO tags\n","def Entity2ID(Sc_Entity):\n","  return {\n","        'B': 1,\n","        'I': 1,\n","        'O': 0,\n","    }[Sc_Entity]\n","\n","def ID2Entity(Sc_Entity):\n","  return {\n","        1: 'B',\n","        #1: 'I',\n","        0: 'None',\n","    }[Sc_Entity]\n"],"metadata":{"id":"gJBtnL2Jq_eQ","executionInfo":{"status":"ok","timestamp":1649118652993,"user_tz":360,"elapsed":7,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Download dataset using datasets, might need to install it!\n","from datasets import load_dataset\n","\n","raw_train = load_dataset('ncbi_disease', split='train')\n","raw_test = load_dataset('ncbi_disease', split='test')\n","raw_val = load_dataset('ncbi_disease', split='validation')\n","\n","# Convert to torch datasets\n","train = NCBIDataset(raw_train)\n","test = NCBIDataset(raw_test)\n","val = NCBIDataset(raw_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330,"referenced_widgets":["b14112d8b43e494eb42fb942fcd0fa56","84ceedb82bfc4f668d26f886aa09476e","2942e439976344fe803cc697974e6c0d","42e56086231a4d01931acd52b24fd1b2","1378501faeb44f2fb9fdd6a746f23eaa","f9b9d705c8df40b2880aa6bb528810a4","4f45438cfa4a456694469a29a79b4bc7","2b1e2f0f14444c88beacf9a82ff2f4e5","a67587ef3391414a9f08e6a854208bd9","933063e123f245d89682e1469b0bc362","2a0d42e3fb174e8eb26458bab652412e","6b153ada7a6f48d98314c40a2c1e10a2","f7cba806d2f842a9b06ca153b8ae7d34","c3be49dd23f243b294c45be6e5def385","98f109d3eb234db2ad6ede1f409a1be7","3062898faa7749e897b5c0cfaf9faee5","48e7394fc06842b89141d9901f668e9b","c3526a469559487e8c3a002920fa1039","b23ce2b74e294485ab248a4d4527aae3","ce4dc20994174cf384fb07c980510413","9e7e718ad0d341ac92b0dfee7e5178e9","10b6e25fca26499b872f25d2caecf305","b18fe8d640c24d379ecdb9b75a244d50","736b03aee07643c0b13ba5a88f603660","3622792b77c84219a0daa23724bebe40","262c5331e4a9489eb781cbdc4ce0bbb9","d82e61e4aebc47edabd14f8870f99fb3","9237167b3b344f84b0c19871b510a497","db60177a5469418ebae6366c52f56726","ae8f7b5d3a8d46d1a4dc0a2dc73355ca","79fd9f04ce6740aeaa00e5603ddd1428","211e01b459da4ffd817b9424d3d17eeb","6739ecb4543d49a4b3a02934825c3b44","2822e7cbe17a4331b1adf6152e795f90","80a65ff8621741a2bb3a8183d1a2a01c","f5d41bcd86b3477e9b023071b5c3776b","3d4be30874b64c638eb4016186fce5f3","8b289ef1e91949f49a7e29a3eee983d5","db172f4cf8b045f9b81320d26748cbc4","58fe2b00d3f24a93968a52ec7a92f527","2598b313a7714ca5ba020302357c7d3b","f1ff079b1ea8446cb69ae9ec86d76aa6","ed8877a88fa04248bd25f44caeb5fe71","99948c26e79145ff954a9e86ff9e0c97","72082e68039644f3acf8460c7a29e962","b066ab5bf7da4cf286474b1486bfc239","c5c78e2c7fe34ccca9ed724409cee264","e2690c8be389450f992b5cbc4914d24a","7321262b3ca843ad801a7c1395b31d7b","7dd214ee6ee5404a9248846629a385c6","5aa05b81a27049e489bb7acef01172bd","382ce164ce924bcebba9e7931b6e77e5","efb42bb26bb2498dbedca6d11c3adc05","599a0a7fe0ac4be48d66dd5c5c76c373","ae56f05e9dff4a91b86c96159b96ebeb","989ead36537844f49ba8dc0acf70613c","9b18d931ac2e4c178bc5fca88a776277","9a5f9ce2d8e54617b255445713c097ce","d518508be712437c852130effd0d8807","901f2d67948a423d91d881ac8275c0f2","fc2f5b836d864ad6997bb64c8c5f4041","d84bb4374697435a8c89efd41c5cf8bf","3ec6fef81a114ca4946654d0c2a89ea7","ad1cb29f5563458bbca33eb4014523ee","d5ab166961c64b2a9efdda974f1271a4","37e7ac5d0eb249d0bfa7a1d9a3bd3f5f","dbe78ba843e246debd7c71c5a5d4a9a2","1236d9ad04394f9887ede9962dd40ede","a2625d6217654452af75bf22a12f79c7","ea7f84b6ea544ec4b8d9d340adc5ae60","47de19d6475a44cfbace74a7646edb97","2f681a2e32224dc693f43ffaf637ee43","bc0c47a3258c49498ab0f3d95b368ec0","37c653c782a84469aabc1d49a40a1121","a54ee5edceec489185538b51fa7eb322","33011719e1584c28918149b7d90cb21d","e82c4914887c4d8bb825b90a278a37e2","b522e7c2fba5461f98f8691934619075","f7d3e7d40cf94812a646ca92fbb7d631","ce52cf0c792545f88e1456170692e6c7","01a8e59d38644cefb19d8caf6f7e0079","def8586d33364aa799b3242917e4115e","aa8971df0ae14505a5d87c0c8ed413bb","93d7fd62dcbe4311bc560d0779372c68","1f8870380eb74010bb8bfa51589ba57c","d9a625fa4fe6497d99c1747fbc11b65a","5850f948d43c48cba9828e6282a679ba","1e5b4a843aae4f1392f9b8ede906bb37","5aad71d5bd514aed9fa81bbd201d7ddd","566abe8b885d49f2b92ca76049cd07ee","972ec385fb6b48b99947cacf3d3fe3ed","834c063b6ede4979b217da60c45f3d5e","aedde6a7e66644a0bb2f4b9b568fdd11","e17f9f46f949482fb22a47ea6fbe6129","ca18e46bfa1e4be691326110867110fe","c5fc59a562a840ec88e2ddf4b1c97548","4c5ff123b47e4efcafd881aea50dcbf3","abafaef8b5624b8b863b6097d7abd72e","c5808e4522c44a27a1887e1162f3d5d2","1b29f0425af5475092ffe87c9db3b9d6","9525f147411543dabb311d4a25b5af4a","960bec42c84148eeb59e208124df9d4f","60e00b1140e14cbcbe94f00c1a580db8","7d6728036a50468e80ec84765d6de529","16cbfa0cf6784639bbbaa56bb22d6291","a688227af280476a81fcfb6f953ccc0f","1e1070186ef04cd19e02c6ebbccc3672","bdf2483a28404ab3ba59f1fc9859174b","d4b0a2736f214185981753af03d33851","c776c0d517a0453087a296b26f7b19ec"]},"id":"k2c1cCUnq_aP","executionInfo":{"status":"ok","timestamp":1649118667937,"user_tz":360,"elapsed":14950,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"064ea354-ede7-4e0e-8029-5c184af3a223"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.28k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b14112d8b43e494eb42fb942fcd0fa56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading metadata:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b153ada7a6f48d98314c40a2c1e10a2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset ncbi_disease/ncbi_disease (download: 1.47 MiB, generated: 3.04 MiB, post-processed: Unknown size, total: 4.52 MiB) to /root/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18fe8d640c24d379ecdb9b75a244d50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/284k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2822e7cbe17a4331b1adf6152e795f90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/51.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72082e68039644f3acf8460c7a29e962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/52.4k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"989ead36537844f49ba8dc0acf70613c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbe78ba843e246debd7c71c5a5d4a9a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/5433 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b522e7c2fba5461f98f8691934619075"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/924 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aad71d5bd514aed9fa81bbd201d7ddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/941 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b29f0425af5475092ffe87c9db3b9d6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset ncbi_disease downloaded and prepared to /root/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03. Subsequent calls will reuse this data.\n"]},{"output_type":"stream","name":"stderr","text":["Reusing dataset ncbi_disease (/root/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03)\n","Reusing dataset ncbi_disease (/root/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03)\n"]}]},{"cell_type":"code","source":["len(train), len(test), len(val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNLE_2GZH5ZX","executionInfo":{"status":"ok","timestamp":1649118668133,"user_tz":360,"elapsed":207,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"2ebecb04-d8c7-431e-d1bb-ba6dc86c5580"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5433, 941, 924)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["train[2739]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFPiiZ7vq_Vq","executionInfo":{"status":"ok","timestamp":1649118668134,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"d138dc79-bf76-487b-987a-21dd97a62735"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n"," 'input_ids': [tensor([  102,   407,   453,  3768,   198,   158,   546,  1211,   259,  1168,\n","           4662,   115, 30132,  7411,   165,  3111,   214,   106,  2317,   579,\n","           2971,  2147,  6110,  7459,   198,  4080, 24241,  5354,   131, 10906,\n","            370,   781,  2980, 13718,   422,  2429,   121,  3014,   131,   106,\n","          12462, 11458, 13348,   422,   170,   546,   111,  2317,   579,  2971,\n","           2147,  7459,   121,   111,  1211,   259,   115, 30132, 30147, 30131,\n","            983,   165,  3974,  1111,   190,   111,  9995, 16147,  1352,  5255,\n","           2665,   106,  1678,   422,   132,  1828,   422,   115, 30132, 30147,\n","          30131,   422, 11412, 30113,   422,   115, 30140, 30110, 30140,   422,\n","            115, 30140, 30125, 30132,   422,   741, 30128, 30132,   422,  3849,\n","            198,   355,   115, 30132,   579, 13869,  2374,   190,   238, 16147,\n","           1352,  5255,  2665,   650, 14480,   111,  2317,   579,  2971,  2147,\n","            115, 30132,   983,  7459,   422,   137,   239,   546,  1211,  1264,\n","            115, 30132,  7411,   165,  3111,   214,   106,   643,   422,   188,\n","           3481,  9508,  4522,  1669, 17000,  1581,   422,  2271,  2576,  7465,\n","            205,   205,   103,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0])],\n"," 'labels': tensor([   0.,    0.,    0.,    0.,    0.,    0.,    0.,    1.,    1.,    1.,\n","            1.,    1.,    1.,    1.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    1.,    1.,    1.,    1.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    1.,    1.,\n","            1.,    1.,    1.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    1.,    1.,\n","            0.,    0.,    0., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.],\n","        dtype=torch.float64)}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["val[73]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1x2XHDJyJFIS","executionInfo":{"status":"ok","timestamp":1649118852217,"user_tz":360,"elapsed":143,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"1bea28f1-74df-4599-f4ca-9b044b3c9d2b"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n"," 'input_ids': [tensor([ 102, 2074, 7342,  137, 4406,  131, 1168, 6625, 5722, 6224,  214,  106,\n","          2576, 8047, 1017, 1168, 6183,  566,  489,  114,  579,  735, 1199,  205,\n","           103,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])],\n"," 'labels': tensor([   0.,    1.,    0.,    0.,    0.,    0.,    0.,    1.,    1.,    0.,\n","            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","            0.,    0.,    0.,    0.,    0., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","         -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.],\n","        dtype=torch.float64)}"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["local_hp_train = []\n","local_hp_val = []\n","for x in range(500):\n","  local_hp_train.append(train[x])\n","\n","for x in range(180):\n","  local_hp_val.append(val[x])\n","\n","print(len(local_hp_train))\n","print(local_hp_train[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6TB6Ss1RH14","executionInfo":{"status":"ok","timestamp":1649118678769,"user_tz":360,"elapsed":10638,"user":{"displayName":"Andres Erazo","userId":"09584327625341777910"}},"outputId":"9e519700-c41a-4ec6-ab7b-ed67c70ad7cc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["500\n","{'input_ids': [tensor([  102,   111, 24243,   861,   153,  7503,  1070,  5703,   145, 16036,\n","          546,  7421,   579, 14957,   787,  3151,   111, 12157, 10636,  3430,\n","          214,  8437,   106,  1127,   190, 18260, 12242,  4655,   239, 12186,\n","          145, 20362,   579,   239, 12186,   546,   422, 10361, 30111,  1352,\n","         6030,   107,   137,  6130, 14025,   205,   103,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])], 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])], 'labels': tensor([   0.,    0.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,\n","           1.,    1.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n","           0.,    0.,    0.,    0.,    0.,    0.,    0., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n","        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.],\n","       dtype=torch.float64)}\n"]}]},{"cell_type":"markdown","metadata":{"id":"oGYV8ipcJ5fU"},"source":["# Pytorch Model Definition"]},{"cell_type":"code","metadata":{"id":"eFp2ZFd25KwK"},"source":["class NerModel(nn.Module):\n","  def __init__(self, b_embeddings, emb_dims=768, ff_dims=14, out_dims=2):\n","    super(NerModel, self).__init__()\n","    self.sci_embeddings = b_embeddings\n","    self.embd_dropout = nn.Dropout(0.1)\n","    self.ff_dropout = nn.Dropout(0.1)\n","    self.ff = nn.Linear(emb_dims, ff_dims)\n","    self.tanh = nn.Tanh()\n","    self.lstm = nn.LSTM(768, 100, 1, bidirectional=True)\n","    self.lstm_drop = nn.Dropout(0.4)\n","    self.ff = nn.Linear(200, 14)\n","    self.ff_act = nn.ReLU()\n","    self.classifier = nn.Linear(ff_dims, out_dims)\n","  def forward(self, **inputs):\n","    embds = self.sci_embeddings(**inputs)['last_hidden_state']\n","    out = self.embd_dropout(embds)\n","    out, _ = self.lstm(out)\n","    out = self.tanh(out)\n","    out = self.lstm_drop(out)\n","    out = self.ff(out)\n","    out = self.ff_act(out)\n","    out = self.ff_dropout(out)\n","    out = self.classifier(out)\n","    return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LthVVOhx_Sz"},"source":["# Metrics and Configs"]},{"cell_type":"code","metadata":{"id":"VZJjqYf2AKA8"},"source":["def reset_weights(m):\n","  '''\n","    Try resetting model weights to avoid\n","    weight leakage.\n","  '''\n","  for layer in m.children():\n","   if hasattr(layer, 'reset_parameters'):\n","    #print(f'Reset trainable parameters of layer = {layer}')\n","    layer.reset_parameters()\n","\n","import math\n","\n","def load_param():\n","  for n, v in best_run.hyperparameters.items():\n","    if n == 'seed':\n","      setattr(trainer.args, n, math.ceil(v))\n","      print(n, math.ceil(v))\n","    else:\n","      setattr(trainer.args, n, v)\n","      print(n, v)   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjMXqFluyLCW"},"source":["batch_size = 4\n","training_args = TrainingArguments(\n","    \"trained_scibert_ner_model\", # output dir\n","    learning_rate=1e-5, \n","    num_train_epochs=10, \n","    dataloader_drop_last=True,\n","    per_device_eval_batch_size=batch_size, \n","    per_device_train_batch_size=batch_size,\n","    save_steps=len(train) // batch_size,\n","    lr_scheduler_type='cosine',\n","    evaluation_strategy='steps',\n","    eval_steps=len(train) // batch_size\n","    )\n","#print(training_args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fabdHzMRybBR"},"source":["def collator(batch):\n","  out =  {\n","      'input_ids': torch.stack([(x['input_ids'][0]) for x in batch]),\n","      'attention_mask': torch.stack([x['attention_mask'][0] for x in batch]),\n","      'labels': torch.stack([x['labels'].clone().detach() for x in batch])\n","  }\n","  return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dZluJPsy2eb5"},"source":["import torch.nn.functional as F\n","\n","class FocalLoss(nn.modules.loss._WeightedLoss):\n","    def __init__(self, weight, gamma, reduction='mean'):\n","        super(FocalLoss, self).__init__(weight,reduction=reduction)\n","        self.gamma = gamma\n","        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n","\n","    def forward(self, input, target):\n","        ce_loss = F.cross_entropy(input, target, ignore_index=-100, reduction=self.reduction, weight=self.weight)\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n","        return focal_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLbpwNckLvV6"},"outputs":[],"source":["class MultilabelTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False, num_labels=2):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs\n","\n","        weights = torch.tensor([0.38, 1.65]).cuda()  # The no-class label has too many examples, we need to weight the loss - this probably needs further tuning \n","        gamma=5\n","        loss_fct = FocalLoss(weight=weights, gamma=gamma)\n","        loss = loss_fct(logits.view(-1, num_labels), labels.long().view(-1))\n","        return (loss, outputs) if return_outputs else loss\n","\n","    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, ignore_keys=None, metric_key_prefix=\"eval\", num_labels=4):\n","      args = self.args\n","      prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n","\n","      self.model.eval()\n","\n","      all_losses = []\n","      all_preds = []\n","      all_labels = []\n","      for step, sample in enumerate(dataloader):\n","        for i in range(0, len(sample['labels'])):\n","          inputs = {}\n","          inputs['input_ids'] = torch.stack([sample['input_ids'][i].cuda()])\n","          inputs['attention_mask'] = torch.stack([sample['attention_mask'][i].cuda()])\n","          inputs['labels'] = torch.stack([sample['labels'][i].cuda()])\n","          labels = inputs['labels'][0].cpu().numpy()\n","          \n","          (loss, logits) = self.compute_loss(self.model, inputs, return_outputs=True)\n","          logits = logits[0].cpu().detach().numpy()\n","          preds = np.argmax(nn.Softmax(dim=-1)(torch.tensor(logits)).numpy(), axis=-1)\n","\n","          all_losses = np.concatenate((all_losses, [loss.detach().cpu().numpy()]), axis=0)\n","\n","          preds = preds[labels != -100]\n","          labels = labels[labels != -100]\n","          all_preds = np.concatenate((all_preds, preds))\n","          all_labels = np.concatenate((all_labels, labels))\n","\n","      metrics = {}\n","      metrics['macro_f1'] = f1_score(all_labels, all_preds, average='macro')\n","      metrics['macro_precision'] = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n","      metrics['macro_recall'] = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n","      metrics['micro_f1'] = f1_score(all_labels, all_preds, average='micro')\n","      metrics['micro_precision'] = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n","      metrics['micro_recall'] = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n","\n","      metrics['macro_f1_no_o'] = f1_score(all_labels, all_preds, average='macro', labels=[1])\n","      metrics['macro_precision_no_o'] = precision_score(all_labels, all_preds, average='macro', labels=[1], zero_division=0)\n","      metrics['macro_recall_no_o'] = recall_score(all_labels, all_preds, average='macro', labels=[1], zero_division=0)\n","      metrics['micro_f1_no_o'] = f1_score(all_labels, all_preds, average='micro', labels=[1])\n","      metrics['micro_precision_no_o'] = precision_score(all_labels, all_preds, average='micro', labels=[1], zero_division=0)\n","      metrics['micro_recall_no_o'] = recall_score(all_labels, all_preds, average='micro', labels=[1], zero_division=0)\n","\n","      for key in list(metrics.keys()):\n","        if not key.startswith(metric_key_prefix):\n","          metrics[metric_key_prefix + '_' + key] = metrics.pop(key)\n","      \n","      metrics[metric_key_prefix + '_loss'] = all_losses.mean().item()\n","\n","      return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=len(dataloader))"]},{"cell_type":"code","metadata":{"id":"infNapI_OmFr"},"source":["def my_hp_space_ray(trial):\n","    from ray import tune\n","\n","    return {\n","        \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n","        \"num_train_epochs\": tune.choice(range(8, 20)),\n","        \"weight_decay\": tune.uniform(0.0, 0.5),\n","        \"per_device_train_batch_size\": tune.choice([4, 8, 16]),  #<16 definetly not working\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Nh8Lcpvb_F4"},"source":["# Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"id":"w6AqbEPyPK7O"},"source":["def model_init():\n","    x = NerModel(BertEmbModel)\n","    x.sci_embeddings.requires_grad = False\n","    return x\n","\n","trainer = MultilabelTrainer(model_init=model_init,\n","                            args=training_args,\n","                            train_dataset=train,\n","                            eval_dataset=val,\n","                            data_collator=collator  # defines how to merge data into batches, using the collator function above\n","                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGyHLY640NRz","colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1XdMn8ZXAC6VDk7LDF955CutlHne83RFY"},"executionInfo":{"status":"ok","timestamp":1645163629349,"user_tz":360,"elapsed":7193039,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"b043a7cc-3636-4532-fab6-0137ef2f65ef"},"source":["from ray.tune.suggest.hyperopt import HyperOptSearch\n","from ray.tune.schedulers import ASHAScheduler\n","\n","best_run = trainer.hyperparameter_search(backend=\"ray\", \n","                                         resources_per_trial={\"gpu\": 1, \"cpu\": 0},\n","                                         n_trials=16, \n","                                         direction=\"maximize\", \n","                                         hp_space=my_hp_space_ray,\n","                                         search_alg=HyperOptSearch(metric='eval_micro_recall_no_o', mode=\"max\"),  #'eval_*f1_micro'\n","                                         scheduler=ASHAScheduler(metric='eval_micro_recall_no_o', mode=\"max\")\n","                                         )"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zH0H8agAbzZi","executionInfo":{"status":"ok","timestamp":1645163634880,"user_tz":360,"elapsed":0,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"dd178f83-138a-405a-cde9-402ab3562e9f"},"source":["best_run"],"execution_count":null,"outputs":[{"data":{"text/plain":["BestRun(run_id='e55a3d20', objective=11.383094363560945, hyperparameters={'learning_rate': 1.1497917821120578e-05, 'num_train_epochs': 19, 'weight_decay': 0.3660243351216811, 'per_device_train_batch_size': 4})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"kv6pGzbGpv2D"},"source":["# Cossvalidation"]},{"cell_type":"code","metadata":{"id":"LhipElJSzRUF"},"source":["#import tensorflow as tf\n","from torch.utils.data import DataLoader, ConcatDataset\n","from sklearn.model_selection import KFold\n","from torch import nn\n","from transformers import Trainer\n","\n","#print('GPU detected:', tf.config.list_physical_devices('GPU'))\n","k_folds = 5\n","kfold = KFold(n_splits=k_folds, shuffle=True)\n","results = np.zeros(5)\n","resultss = np.zeros(5)\n","dataset = ConcatDataset([train, test])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zQyLOxyMjZq2","executionInfo":{"status":"ok","timestamp":1646876770211,"user_tz":360,"elapsed":5767547,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"6f468ef5-76e4-44b0-825d-7d5c779ec270"},"source":["for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n","    \n","    # Print\n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","    \n","    # Sample elements randomly from a given list of ids, no replacement.\n","    #train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n","    #test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n","    \n","    local_train = []\n","    #i = 0\n","    for idx in train_ids:\n","      #if i <= 1920:\n","      local_train.append(dataset[idx])\n","      #i += 1\n","    \n","    local_test = []\n","    #i = 0\n","    for idx in test_ids:\n","      #if i <= 480:\n","      local_test.append(dataset[idx])\n","      #i += 1\n","\n","    training_args = TrainingArguments(\"trained_scibert_ner_model\", # output dir\n","                                      learning_rate=1.1497917821120578e-05, \n","                                      weight_decay=0.3660243351216811,\n","                                      num_train_epochs=19, \n","                                      dataloader_drop_last=True,\n","                                      per_device_eval_batch_size=4, \n","                                      per_device_train_batch_size=4,\n","                                      logging_steps=50,\n","                                      save_steps=len(local_train) // batch_size,\n","                                      lr_scheduler_type='cosine',\n","                                      evaluation_strategy='steps',\n","                                      eval_steps=len(local_train) // batch_size\n","                                      )\n","\n","    #learning_rate 1.1497917821120578e-05\n","    #num_train_epochs 19\n","    #weight_decay 0.3660243351216811\n","    #per_device_train_batch_size 4\n","\n","    # Init the neural network\n","    ner_model = NerModel(BertEmbModel).to('cuda')  # make sure we move the model to the GPU for training\n","    \n","    trainer = MultilabelTrainer(model=ner_model,\n","                                args=training_args,\n","                                train_dataset=local_train,\n","                                eval_dataset=local_test,\n","                                data_collator=collator  # defines how to merge data into batches, using the collator function above\n","                                )\n","\n","    #Loading Best parameters\n","    #load_param()\n","\n","    #Training\n","    trainer.train()\n","          \n","    # Process is complete.\n","    print('Training process has finished. Saving trained model.')\n","\n","    # Print about testing\n","    print('Starting testing')\n","    \n","    # Saving the model\n","    save_path = f'./model-fold-{fold}.pth'\n","    torch.save(ner_model.state_dict(), save_path)\n","\n","    # Evaluationfor this fold\n","    #correct, total = 0, 0\n","    with torch.no_grad():\n","      result = trainer.evaluate(local_test)\n","      print(result)\n","\n","      # Print accuracy\n","      print('Accuracy for fold ', fold, ': ', result['eval_micro_f1_no_o'], ' -- ', result['eval_micro_f1'])\n","      print('--------------------------------')\n","      results[fold] = result['eval_micro_f1_no_o']\n","      resultss[fold] = result['eval_micro_f1']\n","      del result\n","    \n"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["FOLD 0\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5099\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 24206\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='24206' max='24206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24206/24206 59:10, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1274</td>\n","      <td>0.000200</td>\n","      <td>0.000428</td>\n","      <td>0.918685</td>\n","      <td>0.879130</td>\n","      <td>0.972647</td>\n","      <td>0.965172</td>\n","      <td>0.965172</td>\n","      <td>0.965172</td>\n","      <td>0.857203</td>\n","      <td>0.760463</td>\n","      <td>0.982143</td>\n","      <td>0.857203</td>\n","      <td>0.760463</td>\n","      <td>0.982143</td>\n","    </tr>\n","    <tr>\n","      <td>2548</td>\n","      <td>0.000100</td>\n","      <td>0.000421</td>\n","      <td>0.939151</td>\n","      <td>0.908817</td>\n","      <td>0.976606</td>\n","      <td>0.974906</td>\n","      <td>0.974906</td>\n","      <td>0.974906</td>\n","      <td>0.892507</td>\n","      <td>0.820222</td>\n","      <td>0.978764</td>\n","      <td>0.892507</td>\n","      <td>0.820222</td>\n","      <td>0.978764</td>\n","    </tr>\n","    <tr>\n","      <td>3822</td>\n","      <td>0.000000</td>\n","      <td>0.000540</td>\n","      <td>0.951303</td>\n","      <td>0.928546</td>\n","      <td>0.977662</td>\n","      <td>0.980403</td>\n","      <td>0.980403</td>\n","      <td>0.980403</td>\n","      <td>0.913658</td>\n","      <td>0.860217</td>\n","      <td>0.974180</td>\n","      <td>0.913658</td>\n","      <td>0.860217</td>\n","      <td>0.974180</td>\n","    </tr>\n","    <tr>\n","      <td>5096</td>\n","      <td>0.000000</td>\n","      <td>0.001126</td>\n","      <td>0.953800</td>\n","      <td>0.941618</td>\n","      <td>0.966925</td>\n","      <td>0.981918</td>\n","      <td>0.981918</td>\n","      <td>0.981918</td>\n","      <td>0.917757</td>\n","      <td>0.889493</td>\n","      <td>0.947876</td>\n","      <td>0.917757</td>\n","      <td>0.889493</td>\n","      <td>0.947876</td>\n","    </tr>\n","    <tr>\n","      <td>6370</td>\n","      <td>0.000000</td>\n","      <td>0.000652</td>\n","      <td>0.948867</td>\n","      <td>0.921113</td>\n","      <td>0.982257</td>\n","      <td>0.979119</td>\n","      <td>0.979119</td>\n","      <td>0.979119</td>\n","      <td>0.909536</td>\n","      <td>0.843898</td>\n","      <td>0.986245</td>\n","      <td>0.909536</td>\n","      <td>0.843898</td>\n","      <td>0.986245</td>\n","    </tr>\n","    <tr>\n","      <td>7644</td>\n","      <td>0.000000</td>\n","      <td>0.000436</td>\n","      <td>0.909807</td>\n","      <td>0.864953</td>\n","      <td>0.975273</td>\n","      <td>0.960369</td>\n","      <td>0.960369</td>\n","      <td>0.960369</td>\n","      <td>0.842277</td>\n","      <td>0.730626</td>\n","      <td>0.994208</td>\n","      <td>0.842277</td>\n","      <td>0.730626</td>\n","      <td>0.994208</td>\n","    </tr>\n","    <tr>\n","      <td>8918</td>\n","      <td>0.000000</td>\n","      <td>0.000994</td>\n","      <td>0.953778</td>\n","      <td>0.929020</td>\n","      <td>0.982841</td>\n","      <td>0.981302</td>\n","      <td>0.981302</td>\n","      <td>0.981302</td>\n","      <td>0.918110</td>\n","      <td>0.859882</td>\n","      <td>0.984797</td>\n","      <td>0.918110</td>\n","      <td>0.859882</td>\n","      <td>0.984797</td>\n","    </tr>\n","    <tr>\n","      <td>10192</td>\n","      <td>0.000000</td>\n","      <td>0.000878</td>\n","      <td>0.954018</td>\n","      <td>0.928863</td>\n","      <td>0.983628</td>\n","      <td>0.981379</td>\n","      <td>0.981379</td>\n","      <td>0.981379</td>\n","      <td>0.918548</td>\n","      <td>0.859365</td>\n","      <td>0.986486</td>\n","      <td>0.918548</td>\n","      <td>0.859365</td>\n","      <td>0.986486</td>\n","    </tr>\n","    <tr>\n","      <td>11466</td>\n","      <td>0.000000</td>\n","      <td>0.001047</td>\n","      <td>0.950970</td>\n","      <td>0.924655</td>\n","      <td>0.982258</td>\n","      <td>0.980069</td>\n","      <td>0.980069</td>\n","      <td>0.980069</td>\n","      <td>0.913199</td>\n","      <td>0.851126</td>\n","      <td>0.985039</td>\n","      <td>0.913199</td>\n","      <td>0.851126</td>\n","      <td>0.985039</td>\n","    </tr>\n","    <tr>\n","      <td>12740</td>\n","      <td>0.000000</td>\n","      <td>0.001271</td>\n","      <td>0.957918</td>\n","      <td>0.936141</td>\n","      <td>0.982905</td>\n","      <td>0.983125</td>\n","      <td>0.983125</td>\n","      <td>0.983125</td>\n","      <td>0.925349</td>\n","      <td>0.874383</td>\n","      <td>0.982625</td>\n","      <td>0.925349</td>\n","      <td>0.874383</td>\n","      <td>0.982625</td>\n","    </tr>\n","    <tr>\n","      <td>14014</td>\n","      <td>0.000000</td>\n","      <td>0.001179</td>\n","      <td>0.951883</td>\n","      <td>0.925311</td>\n","      <td>0.983522</td>\n","      <td>0.980428</td>\n","      <td>0.980428</td>\n","      <td>0.980428</td>\n","      <td>0.914822</td>\n","      <td>0.852145</td>\n","      <td>0.987452</td>\n","      <td>0.914822</td>\n","      <td>0.852145</td>\n","      <td>0.987452</td>\n","    </tr>\n","    <tr>\n","      <td>15288</td>\n","      <td>0.000000</td>\n","      <td>0.001291</td>\n","      <td>0.961501</td>\n","      <td>0.942785</td>\n","      <td>0.982506</td>\n","      <td>0.984692</td>\n","      <td>0.984692</td>\n","      <td>0.984692</td>\n","      <td>0.931620</td>\n","      <td>0.888014</td>\n","      <td>0.979730</td>\n","      <td>0.931620</td>\n","      <td>0.888014</td>\n","      <td>0.979730</td>\n","    </tr>\n","    <tr>\n","      <td>16562</td>\n","      <td>0.000000</td>\n","      <td>0.001439</td>\n","      <td>0.962803</td>\n","      <td>0.945971</td>\n","      <td>0.981455</td>\n","      <td>0.985283</td>\n","      <td>0.985283</td>\n","      <td>0.985283</td>\n","      <td>0.933887</td>\n","      <td>0.894760</td>\n","      <td>0.976593</td>\n","      <td>0.933887</td>\n","      <td>0.894760</td>\n","      <td>0.976593</td>\n","    </tr>\n","    <tr>\n","      <td>17836</td>\n","      <td>0.000000</td>\n","      <td>0.001554</td>\n","      <td>0.963514</td>\n","      <td>0.946695</td>\n","      <td>0.982145</td>\n","      <td>0.985565</td>\n","      <td>0.985565</td>\n","      <td>0.985565</td>\n","      <td>0.935149</td>\n","      <td>0.896064</td>\n","      <td>0.977799</td>\n","      <td>0.935149</td>\n","      <td>0.896064</td>\n","      <td>0.977799</td>\n","    </tr>\n","    <tr>\n","      <td>19110</td>\n","      <td>0.000000</td>\n","      <td>0.002253</td>\n","      <td>0.965405</td>\n","      <td>0.953038</td>\n","      <td>0.978715</td>\n","      <td>0.986464</td>\n","      <td>0.986464</td>\n","      <td>0.986464</td>\n","      <td>0.938413</td>\n","      <td>0.909812</td>\n","      <td>0.968871</td>\n","      <td>0.938413</td>\n","      <td>0.909812</td>\n","      <td>0.968871</td>\n","    </tr>\n","    <tr>\n","      <td>20384</td>\n","      <td>0.000000</td>\n","      <td>0.002128</td>\n","      <td>0.965468</td>\n","      <td>0.952335</td>\n","      <td>0.979672</td>\n","      <td>0.986464</td>\n","      <td>0.986464</td>\n","      <td>0.986464</td>\n","      <td>0.938542</td>\n","      <td>0.908147</td>\n","      <td>0.971042</td>\n","      <td>0.938542</td>\n","      <td>0.908147</td>\n","      <td>0.971042</td>\n","    </tr>\n","    <tr>\n","      <td>21658</td>\n","      <td>0.000000</td>\n","      <td>0.002022</td>\n","      <td>0.965106</td>\n","      <td>0.950850</td>\n","      <td>0.980634</td>\n","      <td>0.986284</td>\n","      <td>0.986284</td>\n","      <td>0.986284</td>\n","      <td>0.937921</td>\n","      <td>0.904890</td>\n","      <td>0.973456</td>\n","      <td>0.937921</td>\n","      <td>0.904890</td>\n","      <td>0.973456</td>\n","    </tr>\n","    <tr>\n","      <td>22932</td>\n","      <td>0.000000</td>\n","      <td>0.002092</td>\n","      <td>0.965786</td>\n","      <td>0.951969</td>\n","      <td>0.980792</td>\n","      <td>0.986567</td>\n","      <td>0.986567</td>\n","      <td>0.986567</td>\n","      <td>0.939122</td>\n","      <td>0.907128</td>\n","      <td>0.973456</td>\n","      <td>0.939122</td>\n","      <td>0.907128</td>\n","      <td>0.973456</td>\n","    </tr>\n","    <tr>\n","      <td>24206</td>\n","      <td>0.000000</td>\n","      <td>0.002068</td>\n","      <td>0.965601</td>\n","      <td>0.951663</td>\n","      <td>0.980749</td>\n","      <td>0.986490</td>\n","      <td>0.986490</td>\n","      <td>0.986490</td>\n","      <td>0.938795</td>\n","      <td>0.906517</td>\n","      <td>0.973456</td>\n","      <td>0.938795</td>\n","      <td>0.906517</td>\n","      <td>0.973456</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1274\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2548\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3822\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5096\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7644\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8918\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10192\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-11466\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12740\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14014\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-15288\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16562\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17836\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19110\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20384\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21658\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-22932\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24206\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9656007357238616, 'eval_macro_precision': 0.9516634843842251, 'eval_macro_recall': 0.9807490697078223, 'eval_micro_f1': 0.9864899573637438, 'eval_micro_precision': 0.9864899573637438, 'eval_micro_recall': 0.9864899573637438, 'eval_macro_f1_no_o': 0.9387945077961368, 'eval_macro_precision_no_o': 0.9065168539325843, 'eval_macro_recall_no_o': 0.9734555984555985, 'eval_micro_f1_no_o': 0.9387945077961368, 'eval_micro_precision_no_o': 0.9065168539325843, 'eval_micro_recall_no_o': 0.9734555984555985, 'eval_loss': 0.0020676296029555285, 'eval_runtime': 16.7895, 'eval_samples_per_second': 18.94, 'eval_steps_per_second': 4.765, 'epoch': 19.0}\n","Accuracy for fold  0 :  0.9387945077961368  --  0.9864899573637438\n","--------------------------------\n","FOLD 1\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5099\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 24206\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='24206' max='24206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24206/24206 58:58, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1274</td>\n","      <td>0.000000</td>\n","      <td>0.000251</td>\n","      <td>0.966152</td>\n","      <td>0.942657</td>\n","      <td>0.993181</td>\n","      <td>0.987662</td>\n","      <td>0.987662</td>\n","      <td>0.987662</td>\n","      <td>0.939169</td>\n","      <td>0.885314</td>\n","      <td>1.000000</td>\n","      <td>0.939169</td>\n","      <td>0.885314</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2548</td>\n","      <td>0.000100</td>\n","      <td>0.000209</td>\n","      <td>0.972368</td>\n","      <td>0.952924</td>\n","      <td>0.994123</td>\n","      <td>0.990040</td>\n","      <td>0.990040</td>\n","      <td>0.990040</td>\n","      <td>0.950270</td>\n","      <td>0.905936</td>\n","      <td>0.999168</td>\n","      <td>0.950270</td>\n","      <td>0.905936</td>\n","      <td>0.999168</td>\n","    </tr>\n","    <tr>\n","      <td>3822</td>\n","      <td>0.000000</td>\n","      <td>0.000097</td>\n","      <td>0.968339</td>\n","      <td>0.946328</td>\n","      <td>0.993401</td>\n","      <td>0.988507</td>\n","      <td>0.988507</td>\n","      <td>0.988507</td>\n","      <td>0.943070</td>\n","      <td>0.892716</td>\n","      <td>0.999445</td>\n","      <td>0.943070</td>\n","      <td>0.892716</td>\n","      <td>0.999445</td>\n","    </tr>\n","    <tr>\n","      <td>5096</td>\n","      <td>0.000000</td>\n","      <td>0.000406</td>\n","      <td>0.964988</td>\n","      <td>0.940817</td>\n","      <td>0.992933</td>\n","      <td>0.987213</td>\n","      <td>0.987213</td>\n","      <td>0.987213</td>\n","      <td>0.937094</td>\n","      <td>0.881634</td>\n","      <td>1.000000</td>\n","      <td>0.937094</td>\n","      <td>0.881634</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6370</td>\n","      <td>0.000000</td>\n","      <td>0.000267</td>\n","      <td>0.960918</td>\n","      <td>0.934442</td>\n","      <td>0.992057</td>\n","      <td>0.985627</td>\n","      <td>0.985627</td>\n","      <td>0.985627</td>\n","      <td>0.929843</td>\n","      <td>0.868884</td>\n","      <td>1.000000</td>\n","      <td>0.929843</td>\n","      <td>0.868884</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7644</td>\n","      <td>0.000000</td>\n","      <td>0.000361</td>\n","      <td>0.973690</td>\n","      <td>0.955277</td>\n","      <td>0.994152</td>\n","      <td>0.990542</td>\n","      <td>0.990542</td>\n","      <td>0.990542</td>\n","      <td>0.952633</td>\n","      <td>0.910701</td>\n","      <td>0.998613</td>\n","      <td>0.952633</td>\n","      <td>0.910701</td>\n","      <td>0.998613</td>\n","    </tr>\n","    <tr>\n","      <td>8918</td>\n","      <td>0.000000</td>\n","      <td>0.000293</td>\n","      <td>0.974747</td>\n","      <td>0.958221</td>\n","      <td>0.992897</td>\n","      <td>0.990964</td>\n","      <td>0.990964</td>\n","      <td>0.990964</td>\n","      <td>0.954509</td>\n","      <td>0.916944</td>\n","      <td>0.995284</td>\n","      <td>0.954509</td>\n","      <td>0.916944</td>\n","      <td>0.995284</td>\n","    </tr>\n","    <tr>\n","      <td>10192</td>\n","      <td>0.000000</td>\n","      <td>0.000369</td>\n","      <td>0.973594</td>\n","      <td>0.955519</td>\n","      <td>0.993641</td>\n","      <td>0.990515</td>\n","      <td>0.990515</td>\n","      <td>0.990515</td>\n","      <td>0.952457</td>\n","      <td>0.911303</td>\n","      <td>0.997503</td>\n","      <td>0.952457</td>\n","      <td>0.911303</td>\n","      <td>0.997503</td>\n","    </tr>\n","    <tr>\n","      <td>11466</td>\n","      <td>0.000000</td>\n","      <td>0.000338</td>\n","      <td>0.978648</td>\n","      <td>0.965146</td>\n","      <td>0.993204</td>\n","      <td>0.992417</td>\n","      <td>0.992417</td>\n","      <td>0.992417</td>\n","      <td>0.961502</td>\n","      <td>0.930909</td>\n","      <td>0.994175</td>\n","      <td>0.961502</td>\n","      <td>0.930909</td>\n","      <td>0.994175</td>\n","    </tr>\n","    <tr>\n","      <td>12740</td>\n","      <td>0.000000</td>\n","      <td>0.000287</td>\n","      <td>0.979187</td>\n","      <td>0.965311</td>\n","      <td>0.994174</td>\n","      <td>0.992602</td>\n","      <td>0.992602</td>\n","      <td>0.992602</td>\n","      <td>0.962477</td>\n","      <td>0.931034</td>\n","      <td>0.996117</td>\n","      <td>0.962477</td>\n","      <td>0.931034</td>\n","      <td>0.996117</td>\n","    </tr>\n","    <tr>\n","      <td>14014</td>\n","      <td>0.000000</td>\n","      <td>0.000168</td>\n","      <td>0.957263</td>\n","      <td>0.929142</td>\n","      <td>0.990772</td>\n","      <td>0.984201</td>\n","      <td>0.984201</td>\n","      <td>0.984201</td>\n","      <td>0.923333</td>\n","      <td>0.858403</td>\n","      <td>0.998890</td>\n","      <td>0.923333</td>\n","      <td>0.858403</td>\n","      <td>0.998890</td>\n","    </tr>\n","    <tr>\n","      <td>15288</td>\n","      <td>0.000000</td>\n","      <td>0.000102</td>\n","      <td>0.970176</td>\n","      <td>0.949759</td>\n","      <td>0.993174</td>\n","      <td>0.989221</td>\n","      <td>0.989221</td>\n","      <td>0.989221</td>\n","      <td>0.946344</td>\n","      <td>0.899725</td>\n","      <td>0.998058</td>\n","      <td>0.946344</td>\n","      <td>0.899725</td>\n","      <td>0.998058</td>\n","    </tr>\n","    <tr>\n","      <td>16562</td>\n","      <td>0.000000</td>\n","      <td>0.000097</td>\n","      <td>0.973947</td>\n","      <td>0.956097</td>\n","      <td>0.993715</td>\n","      <td>0.990647</td>\n","      <td>0.990647</td>\n","      <td>0.990647</td>\n","      <td>0.953088</td>\n","      <td>0.912459</td>\n","      <td>0.997503</td>\n","      <td>0.953088</td>\n","      <td>0.912459</td>\n","      <td>0.997503</td>\n","    </tr>\n","    <tr>\n","      <td>17836</td>\n","      <td>0.000000</td>\n","      <td>0.000120</td>\n","      <td>0.974730</td>\n","      <td>0.957283</td>\n","      <td>0.993999</td>\n","      <td>0.990938</td>\n","      <td>0.990938</td>\n","      <td>0.990938</td>\n","      <td>0.954491</td>\n","      <td>0.914802</td>\n","      <td>0.997781</td>\n","      <td>0.954491</td>\n","      <td>0.914802</td>\n","      <td>0.997781</td>\n","    </tr>\n","    <tr>\n","      <td>19110</td>\n","      <td>0.000000</td>\n","      <td>0.000149</td>\n","      <td>0.972897</td>\n","      <td>0.954278</td>\n","      <td>0.993620</td>\n","      <td>0.990251</td>\n","      <td>0.990251</td>\n","      <td>0.990251</td>\n","      <td>0.951210</td>\n","      <td>0.908792</td>\n","      <td>0.997781</td>\n","      <td>0.951210</td>\n","      <td>0.908792</td>\n","      <td>0.997781</td>\n","    </tr>\n","    <tr>\n","      <td>20384</td>\n","      <td>0.000000</td>\n","      <td>0.000172</td>\n","      <td>0.972265</td>\n","      <td>0.953247</td>\n","      <td>0.993488</td>\n","      <td>0.990013</td>\n","      <td>0.990013</td>\n","      <td>0.990013</td>\n","      <td>0.950079</td>\n","      <td>0.906731</td>\n","      <td>0.997781</td>\n","      <td>0.950079</td>\n","      <td>0.906731</td>\n","      <td>0.997781</td>\n","    </tr>\n","    <tr>\n","      <td>21658</td>\n","      <td>0.000000</td>\n","      <td>0.000176</td>\n","      <td>0.973460</td>\n","      <td>0.955198</td>\n","      <td>0.993736</td>\n","      <td>0.990462</td>\n","      <td>0.990462</td>\n","      <td>0.990462</td>\n","      <td>0.952217</td>\n","      <td>0.910633</td>\n","      <td>0.997781</td>\n","      <td>0.952217</td>\n","      <td>0.910633</td>\n","      <td>0.997781</td>\n","    </tr>\n","    <tr>\n","      <td>22932</td>\n","      <td>0.000000</td>\n","      <td>0.000177</td>\n","      <td>0.973671</td>\n","      <td>0.955545</td>\n","      <td>0.993780</td>\n","      <td>0.990542</td>\n","      <td>0.990542</td>\n","      <td>0.990542</td>\n","      <td>0.952595</td>\n","      <td>0.911325</td>\n","      <td>0.997781</td>\n","      <td>0.952595</td>\n","      <td>0.911325</td>\n","      <td>0.997781</td>\n","    </tr>\n","    <tr>\n","      <td>24206</td>\n","      <td>0.000000</td>\n","      <td>0.000177</td>\n","      <td>0.973671</td>\n","      <td>0.955545</td>\n","      <td>0.993780</td>\n","      <td>0.990542</td>\n","      <td>0.990542</td>\n","      <td>0.990542</td>\n","      <td>0.952595</td>\n","      <td>0.911325</td>\n","      <td>0.997781</td>\n","      <td>0.952595</td>\n","      <td>0.911325</td>\n","      <td>0.997781</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1274\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2548\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3822\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5096\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7644\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8918\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10192\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-11466\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12740\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14014\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-15288\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16562\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17836\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19110\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20384\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21658\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-22932\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24206\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9736710333466643, 'eval_macro_precision': 0.955544544843397, 'eval_macro_recall': 0.9937801948875779, 'eval_micro_f1': 0.9905416116248349, 'eval_micro_precision': 0.9905416116248349, 'eval_micro_recall': 0.9905416116248349, 'eval_macro_f1_no_o': 0.9525953389830508, 'eval_macro_precision_no_o': 0.9113250570053205, 'eval_macro_recall_no_o': 0.9977808599167822, 'eval_micro_f1_no_o': 0.9525953389830508, 'eval_micro_precision_no_o': 0.9113250570053205, 'eval_micro_recall_no_o': 0.9977808599167822, 'eval_loss': 0.00017697828472081878, 'eval_runtime': 16.3708, 'eval_samples_per_second': 19.425, 'eval_steps_per_second': 4.887, 'epoch': 19.0}\n","Accuracy for fold  1 :  0.9525953389830508  --  0.9905416116248349\n","--------------------------------\n","FOLD 2\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5099\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 24206\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='24206' max='24206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24206/24206 59:01, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1274</td>\n","      <td>0.000000</td>\n","      <td>0.000162</td>\n","      <td>0.970758</td>\n","      <td>0.950850</td>\n","      <td>0.993157</td>\n","      <td>0.988918</td>\n","      <td>0.988918</td>\n","      <td>0.988918</td>\n","      <td>0.947714</td>\n","      <td>0.901875</td>\n","      <td>0.998463</td>\n","      <td>0.947714</td>\n","      <td>0.901875</td>\n","      <td>0.998463</td>\n","    </tr>\n","    <tr>\n","      <td>2548</td>\n","      <td>0.000000</td>\n","      <td>0.000042</td>\n","      <td>0.987632</td>\n","      <td>0.978700</td>\n","      <td>0.997009</td>\n","      <td>0.995438</td>\n","      <td>0.995438</td>\n","      <td>0.995438</td>\n","      <td>0.977806</td>\n","      <td>0.957515</td>\n","      <td>0.998975</td>\n","      <td>0.977806</td>\n","      <td>0.957515</td>\n","      <td>0.998975</td>\n","    </tr>\n","    <tr>\n","      <td>3822</td>\n","      <td>0.000000</td>\n","      <td>0.000045</td>\n","      <td>0.988320</td>\n","      <td>0.979780</td>\n","      <td>0.997266</td>\n","      <td>0.995696</td>\n","      <td>0.995696</td>\n","      <td>0.995696</td>\n","      <td>0.979039</td>\n","      <td>0.959646</td>\n","      <td>0.999231</td>\n","      <td>0.979039</td>\n","      <td>0.959646</td>\n","      <td>0.999231</td>\n","    </tr>\n","    <tr>\n","      <td>5096</td>\n","      <td>0.000000</td>\n","      <td>0.000039</td>\n","      <td>0.988252</td>\n","      <td>0.979662</td>\n","      <td>0.997252</td>\n","      <td>0.995670</td>\n","      <td>0.995670</td>\n","      <td>0.995670</td>\n","      <td>0.978916</td>\n","      <td>0.959410</td>\n","      <td>0.999231</td>\n","      <td>0.978916</td>\n","      <td>0.959410</td>\n","      <td>0.999231</td>\n","    </tr>\n","    <tr>\n","      <td>6370</td>\n","      <td>0.000000</td>\n","      <td>0.000031</td>\n","      <td>0.986606</td>\n","      <td>0.976943</td>\n","      <td>0.996794</td>\n","      <td>0.995052</td>\n","      <td>0.995052</td>\n","      <td>0.995052</td>\n","      <td>0.975970</td>\n","      <td>0.954000</td>\n","      <td>0.998975</td>\n","      <td>0.975970</td>\n","      <td>0.954000</td>\n","      <td>0.998975</td>\n","    </tr>\n","    <tr>\n","      <td>7644</td>\n","      <td>0.000000</td>\n","      <td>0.000033</td>\n","      <td>0.987911</td>\n","      <td>0.978974</td>\n","      <td>0.997294</td>\n","      <td>0.995541</td>\n","      <td>0.995541</td>\n","      <td>0.995541</td>\n","      <td>0.978307</td>\n","      <td>0.958006</td>\n","      <td>0.999488</td>\n","      <td>0.978307</td>\n","      <td>0.958006</td>\n","      <td>0.999488</td>\n","    </tr>\n","    <tr>\n","      <td>8918</td>\n","      <td>0.000000</td>\n","      <td>0.000045</td>\n","      <td>0.986333</td>\n","      <td>0.976476</td>\n","      <td>0.996737</td>\n","      <td>0.994949</td>\n","      <td>0.994949</td>\n","      <td>0.994949</td>\n","      <td>0.975482</td>\n","      <td>0.953068</td>\n","      <td>0.998975</td>\n","      <td>0.975482</td>\n","      <td>0.953068</td>\n","      <td>0.998975</td>\n","    </tr>\n","    <tr>\n","      <td>10192</td>\n","      <td>0.000000</td>\n","      <td>0.000040</td>\n","      <td>0.988178</td>\n","      <td>0.979741</td>\n","      <td>0.997010</td>\n","      <td>0.995644</td>\n","      <td>0.995644</td>\n","      <td>0.995644</td>\n","      <td>0.978782</td>\n","      <td>0.959626</td>\n","      <td>0.998719</td>\n","      <td>0.978782</td>\n","      <td>0.959626</td>\n","      <td>0.998719</td>\n","    </tr>\n","    <tr>\n","      <td>11466</td>\n","      <td>0.000000</td>\n","      <td>0.000026</td>\n","      <td>0.988524</td>\n","      <td>0.980233</td>\n","      <td>0.997195</td>\n","      <td>0.995773</td>\n","      <td>0.995773</td>\n","      <td>0.995773</td>\n","      <td>0.979402</td>\n","      <td>0.960581</td>\n","      <td>0.998975</td>\n","      <td>0.979402</td>\n","      <td>0.960581</td>\n","      <td>0.998975</td>\n","    </tr>\n","    <tr>\n","      <td>12740</td>\n","      <td>0.000000</td>\n","      <td>0.000026</td>\n","      <td>0.988926</td>\n","      <td>0.981344</td>\n","      <td>0.996826</td>\n","      <td>0.995928</td>\n","      <td>0.995928</td>\n","      <td>0.995928</td>\n","      <td>0.980121</td>\n","      <td>0.962917</td>\n","      <td>0.997950</td>\n","      <td>0.980121</td>\n","      <td>0.962917</td>\n","      <td>0.997950</td>\n","    </tr>\n","    <tr>\n","      <td>14014</td>\n","      <td>0.000000</td>\n","      <td>0.000025</td>\n","      <td>0.989273</td>\n","      <td>0.981839</td>\n","      <td>0.997011</td>\n","      <td>0.996057</td>\n","      <td>0.996057</td>\n","      <td>0.996057</td>\n","      <td>0.980743</td>\n","      <td>0.963879</td>\n","      <td>0.998207</td>\n","      <td>0.980743</td>\n","      <td>0.963879</td>\n","      <td>0.998207</td>\n","    </tr>\n","    <tr>\n","      <td>15288</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.989409</td>\n","      <td>0.982178</td>\n","      <td>0.996926</td>\n","      <td>0.996108</td>\n","      <td>0.996108</td>\n","      <td>0.996108</td>\n","      <td>0.980985</td>\n","      <td>0.964586</td>\n","      <td>0.997950</td>\n","      <td>0.980985</td>\n","      <td>0.964586</td>\n","      <td>0.997950</td>\n","    </tr>\n","    <tr>\n","      <td>16562</td>\n","      <td>0.000000</td>\n","      <td>0.000023</td>\n","      <td>0.989275</td>\n","      <td>0.981739</td>\n","      <td>0.997125</td>\n","      <td>0.996057</td>\n","      <td>0.996057</td>\n","      <td>0.996057</td>\n","      <td>0.980747</td>\n","      <td>0.963650</td>\n","      <td>0.998463</td>\n","      <td>0.980747</td>\n","      <td>0.963650</td>\n","      <td>0.998463</td>\n","    </tr>\n","    <tr>\n","      <td>17836</td>\n","      <td>0.000000</td>\n","      <td>0.000024</td>\n","      <td>0.989275</td>\n","      <td>0.981739</td>\n","      <td>0.997125</td>\n","      <td>0.996057</td>\n","      <td>0.996057</td>\n","      <td>0.996057</td>\n","      <td>0.980747</td>\n","      <td>0.963650</td>\n","      <td>0.998463</td>\n","      <td>0.980747</td>\n","      <td>0.963650</td>\n","      <td>0.998463</td>\n","    </tr>\n","    <tr>\n","      <td>19110</td>\n","      <td>0.000000</td>\n","      <td>0.000023</td>\n","      <td>0.988934</td>\n","      <td>0.981044</td>\n","      <td>0.997167</td>\n","      <td>0.995928</td>\n","      <td>0.995928</td>\n","      <td>0.995928</td>\n","      <td>0.980136</td>\n","      <td>0.962232</td>\n","      <td>0.998719</td>\n","      <td>0.980136</td>\n","      <td>0.962232</td>\n","      <td>0.998719</td>\n","    </tr>\n","    <tr>\n","      <td>20384</td>\n","      <td>0.000000</td>\n","      <td>0.000023</td>\n","      <td>0.989416</td>\n","      <td>0.981877</td>\n","      <td>0.997268</td>\n","      <td>0.996108</td>\n","      <td>0.996108</td>\n","      <td>0.996108</td>\n","      <td>0.980999</td>\n","      <td>0.963897</td>\n","      <td>0.998719</td>\n","      <td>0.980999</td>\n","      <td>0.963897</td>\n","      <td>0.998719</td>\n","    </tr>\n","    <tr>\n","      <td>21658</td>\n","      <td>0.000000</td>\n","      <td>0.000024</td>\n","      <td>0.988867</td>\n","      <td>0.980826</td>\n","      <td>0.997267</td>\n","      <td>0.995902</td>\n","      <td>0.995902</td>\n","      <td>0.995902</td>\n","      <td>0.980018</td>\n","      <td>0.961766</td>\n","      <td>0.998975</td>\n","      <td>0.980018</td>\n","      <td>0.961766</td>\n","      <td>0.998975</td>\n","    </tr>\n","    <tr>\n","      <td>22932</td>\n","      <td>0.000000</td>\n","      <td>0.000024</td>\n","      <td>0.988796</td>\n","      <td>0.980806</td>\n","      <td>0.997139</td>\n","      <td>0.995876</td>\n","      <td>0.995876</td>\n","      <td>0.995876</td>\n","      <td>0.979889</td>\n","      <td>0.961757</td>\n","      <td>0.998719</td>\n","      <td>0.979889</td>\n","      <td>0.961757</td>\n","      <td>0.998719</td>\n","    </tr>\n","    <tr>\n","      <td>24206</td>\n","      <td>0.000000</td>\n","      <td>0.000024</td>\n","      <td>0.988796</td>\n","      <td>0.980806</td>\n","      <td>0.997139</td>\n","      <td>0.995876</td>\n","      <td>0.995876</td>\n","      <td>0.995876</td>\n","      <td>0.979889</td>\n","      <td>0.961757</td>\n","      <td>0.998719</td>\n","      <td>0.979889</td>\n","      <td>0.961757</td>\n","      <td>0.998719</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1274\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2548\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3822\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5096\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7644\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8918\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10192\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-11466\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12740\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14014\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-15288\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16562\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17836\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19110\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20384\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21658\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-22932\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24206\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9887960297155707, 'eval_macro_precision': 0.9808064151205234, 'eval_macro_recall': 0.997138709440067, 'eval_micro_f1': 0.9958763949382747, 'eval_micro_precision': 0.9958763949382747, 'eval_micro_recall': 0.9958763949382747, 'eval_macro_f1_no_o': 0.9798893916540974, 'eval_macro_precision_no_o': 0.9617567234147545, 'eval_macro_recall_no_o': 0.9987189341532154, 'eval_micro_f1_no_o': 0.9798893916540974, 'eval_micro_precision_no_o': 0.9617567234147545, 'eval_micro_recall_no_o': 0.9987189341532154, 'eval_loss': 2.420706463107108e-05, 'eval_runtime': 16.3092, 'eval_samples_per_second': 19.498, 'eval_steps_per_second': 4.905, 'epoch': 19.0}\n","Accuracy for fold  2 :  0.9798893916540974  --  0.9958763949382747\n","--------------------------------\n","FOLD 3\n","--------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5099\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 24206\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9153' max='24206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 9153/24206 22:09 < 36:26, 6.89 it/s, Epoch 7.18/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1274</td>\n","      <td>0.000000</td>\n","      <td>0.000052</td>\n","      <td>0.969874</td>\n","      <td>0.950072</td>\n","      <td>0.992203</td>\n","      <td>0.987981</td>\n","      <td>0.987981</td>\n","      <td>0.987981</td>\n","      <td>0.946519</td>\n","      <td>0.900439</td>\n","      <td>0.997570</td>\n","      <td>0.946519</td>\n","      <td>0.900439</td>\n","      <td>0.997570</td>\n","    </tr>\n","    <tr>\n","      <td>2548</td>\n","      <td>0.000000</td>\n","      <td>0.000034</td>\n","      <td>0.990686</td>\n","      <td>0.983762</td>\n","      <td>0.997878</td>\n","      <td>0.996399</td>\n","      <td>0.996399</td>\n","      <td>0.996399</td>\n","      <td>0.983391</td>\n","      <td>0.967552</td>\n","      <td>0.999757</td>\n","      <td>0.983391</td>\n","      <td>0.967552</td>\n","      <td>0.999757</td>\n","    </tr>\n","    <tr>\n","      <td>3822</td>\n","      <td>0.000000</td>\n","      <td>0.000028</td>\n","      <td>0.992740</td>\n","      <td>0.987314</td>\n","      <td>0.998327</td>\n","      <td>0.997202</td>\n","      <td>0.997202</td>\n","      <td>0.997202</td>\n","      <td>0.987047</td>\n","      <td>0.974657</td>\n","      <td>0.999757</td>\n","      <td>0.987047</td>\n","      <td>0.974657</td>\n","      <td>0.999757</td>\n","    </tr>\n","    <tr>\n","      <td>5096</td>\n","      <td>0.000000</td>\n","      <td>0.000023</td>\n","      <td>0.992207</td>\n","      <td>0.986489</td>\n","      <td>0.998104</td>\n","      <td>0.996995</td>\n","      <td>0.996995</td>\n","      <td>0.996995</td>\n","      <td>0.986098</td>\n","      <td>0.973037</td>\n","      <td>0.999514</td>\n","      <td>0.986098</td>\n","      <td>0.973037</td>\n","      <td>0.999514</td>\n","    </tr>\n","    <tr>\n","      <td>6370</td>\n","      <td>0.000000</td>\n","      <td>0.000047</td>\n","      <td>0.992805</td>\n","      <td>0.987527</td>\n","      <td>0.998235</td>\n","      <td>0.997228</td>\n","      <td>0.997228</td>\n","      <td>0.997228</td>\n","      <td>0.987163</td>\n","      <td>0.975113</td>\n","      <td>0.999514</td>\n","      <td>0.987163</td>\n","      <td>0.975113</td>\n","      <td>0.999514</td>\n","    </tr>\n","    <tr>\n","      <td>7644</td>\n","      <td>0.000000</td>\n","      <td>0.000026</td>\n","      <td>0.993803</td>\n","      <td>0.989267</td>\n","      <td>0.998452</td>\n","      <td>0.997617</td>\n","      <td>0.997617</td>\n","      <td>0.997617</td>\n","      <td>0.988942</td>\n","      <td>0.978592</td>\n","      <td>0.999514</td>\n","      <td>0.988942</td>\n","      <td>0.978592</td>\n","      <td>0.999514</td>\n","    </tr>\n","    <tr>\n","      <td>8918</td>\n","      <td>0.000000</td>\n","      <td>0.000034</td>\n","      <td>0.980231</td>\n","      <td>0.966116</td>\n","      <td>0.995544</td>\n","      <td>0.992229</td>\n","      <td>0.992229</td>\n","      <td>0.992229</td>\n","      <td>0.964830</td>\n","      <td>0.932261</td>\n","      <td>0.999757</td>\n","      <td>0.964830</td>\n","      <td>0.932261</td>\n","      <td>0.999757</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1274\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2548\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3822\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5096\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7644\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8918\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='24206' max='24206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24206/24206 58:50, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1274</td>\n","      <td>0.000000</td>\n","      <td>0.000052</td>\n","      <td>0.969874</td>\n","      <td>0.950072</td>\n","      <td>0.992203</td>\n","      <td>0.987981</td>\n","      <td>0.987981</td>\n","      <td>0.987981</td>\n","      <td>0.946519</td>\n","      <td>0.900439</td>\n","      <td>0.997570</td>\n","      <td>0.946519</td>\n","      <td>0.900439</td>\n","      <td>0.997570</td>\n","    </tr>\n","    <tr>\n","      <td>2548</td>\n","      <td>0.000000</td>\n","      <td>0.000034</td>\n","      <td>0.990686</td>\n","      <td>0.983762</td>\n","      <td>0.997878</td>\n","      <td>0.996399</td>\n","      <td>0.996399</td>\n","      <td>0.996399</td>\n","      <td>0.983391</td>\n","      <td>0.967552</td>\n","      <td>0.999757</td>\n","      <td>0.983391</td>\n","      <td>0.967552</td>\n","      <td>0.999757</td>\n","    </tr>\n","    <tr>\n","      <td>3822</td>\n","      <td>0.000000</td>\n","      <td>0.000028</td>\n","      <td>0.992740</td>\n","      <td>0.987314</td>\n","      <td>0.998327</td>\n","      <td>0.997202</td>\n","      <td>0.997202</td>\n","      <td>0.997202</td>\n","      <td>0.987047</td>\n","      <td>0.974657</td>\n","      <td>0.999757</td>\n","      <td>0.987047</td>\n","      <td>0.974657</td>\n","      <td>0.999757</td>\n","    </tr>\n","    <tr>\n","      <td>5096</td>\n","      <td>0.000000</td>\n","      <td>0.000023</td>\n","      <td>0.992207</td>\n","      <td>0.986489</td>\n","      <td>0.998104</td>\n","      <td>0.996995</td>\n","      <td>0.996995</td>\n","      <td>0.996995</td>\n","      <td>0.986098</td>\n","      <td>0.973037</td>\n","      <td>0.999514</td>\n","      <td>0.986098</td>\n","      <td>0.973037</td>\n","      <td>0.999514</td>\n","    </tr>\n","    <tr>\n","      <td>6370</td>\n","      <td>0.000000</td>\n","      <td>0.000047</td>\n","      <td>0.992805</td>\n","      <td>0.987527</td>\n","      <td>0.998235</td>\n","      <td>0.997228</td>\n","      <td>0.997228</td>\n","      <td>0.997228</td>\n","      <td>0.987163</td>\n","      <td>0.975113</td>\n","      <td>0.999514</td>\n","      <td>0.987163</td>\n","      <td>0.975113</td>\n","      <td>0.999514</td>\n","    </tr>\n","    <tr>\n","      <td>7644</td>\n","      <td>0.000000</td>\n","      <td>0.000026</td>\n","      <td>0.993803</td>\n","      <td>0.989267</td>\n","      <td>0.998452</td>\n","      <td>0.997617</td>\n","      <td>0.997617</td>\n","      <td>0.997617</td>\n","      <td>0.988942</td>\n","      <td>0.978592</td>\n","      <td>0.999514</td>\n","      <td>0.988942</td>\n","      <td>0.978592</td>\n","      <td>0.999514</td>\n","    </tr>\n","    <tr>\n","      <td>8918</td>\n","      <td>0.000000</td>\n","      <td>0.000034</td>\n","      <td>0.980231</td>\n","      <td>0.966116</td>\n","      <td>0.995544</td>\n","      <td>0.992229</td>\n","      <td>0.992229</td>\n","      <td>0.992229</td>\n","      <td>0.964830</td>\n","      <td>0.932261</td>\n","      <td>0.999757</td>\n","      <td>0.964830</td>\n","      <td>0.932261</td>\n","      <td>0.999757</td>\n","    </tr>\n","    <tr>\n","      <td>10192</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.985512</td>\n","      <td>0.974939</td>\n","      <td>0.996733</td>\n","      <td>0.994353</td>\n","      <td>0.994353</td>\n","      <td>0.994353</td>\n","      <td>0.974195</td>\n","      <td>0.949908</td>\n","      <td>0.999757</td>\n","      <td>0.974195</td>\n","      <td>0.949908</td>\n","      <td>0.999757</td>\n","    </tr>\n","    <tr>\n","      <td>11466</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.987922</td>\n","      <td>0.979123</td>\n","      <td>0.997162</td>\n","      <td>0.995311</td>\n","      <td>0.995311</td>\n","      <td>0.995311</td>\n","      <td>0.978475</td>\n","      <td>0.958304</td>\n","      <td>0.999514</td>\n","      <td>0.978475</td>\n","      <td>0.958304</td>\n","      <td>0.999514</td>\n","    </tr>\n","    <tr>\n","      <td>12740</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.988449</td>\n","      <td>0.979925</td>\n","      <td>0.997385</td>\n","      <td>0.995519</td>\n","      <td>0.995519</td>\n","      <td>0.995519</td>\n","      <td>0.979412</td>\n","      <td>0.959879</td>\n","      <td>0.999757</td>\n","      <td>0.979412</td>\n","      <td>0.959879</td>\n","      <td>0.999757</td>\n","    </tr>\n","    <tr>\n","      <td>14014</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.988444</td>\n","      <td>0.980110</td>\n","      <td>0.997171</td>\n","      <td>0.995519</td>\n","      <td>0.995519</td>\n","      <td>0.995519</td>\n","      <td>0.979402</td>\n","      <td>0.960308</td>\n","      <td>0.999271</td>\n","      <td>0.979402</td>\n","      <td>0.960308</td>\n","      <td>0.999271</td>\n","    </tr>\n","    <tr>\n","      <td>15288</td>\n","      <td>0.000000</td>\n","      <td>0.000023</td>\n","      <td>0.987723</td>\n","      <td>0.978880</td>\n","      <td>0.997012</td>\n","      <td>0.995234</td>\n","      <td>0.995234</td>\n","      <td>0.995234</td>\n","      <td>0.978121</td>\n","      <td>0.957848</td>\n","      <td>0.999271</td>\n","      <td>0.978121</td>\n","      <td>0.957848</td>\n","      <td>0.999271</td>\n","    </tr>\n","    <tr>\n","      <td>16562</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.988313</td>\n","      <td>0.979886</td>\n","      <td>0.997142</td>\n","      <td>0.995467</td>\n","      <td>0.995467</td>\n","      <td>0.995467</td>\n","      <td>0.979169</td>\n","      <td>0.959860</td>\n","      <td>0.999271</td>\n","      <td>0.979169</td>\n","      <td>0.959860</td>\n","      <td>0.999271</td>\n","    </tr>\n","    <tr>\n","      <td>17836</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.989302</td>\n","      <td>0.981385</td>\n","      <td>0.997573</td>\n","      <td>0.995855</td>\n","      <td>0.995855</td>\n","      <td>0.995855</td>\n","      <td>0.980930</td>\n","      <td>0.962798</td>\n","      <td>0.999757</td>\n","      <td>0.980930</td>\n","      <td>0.962798</td>\n","      <td>0.999757</td>\n","    </tr>\n","    <tr>\n","      <td>19110</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.989691</td>\n","      <td>0.982344</td>\n","      <td>0.997339</td>\n","      <td>0.996011</td>\n","      <td>0.996011</td>\n","      <td>0.996011</td>\n","      <td>0.981619</td>\n","      <td>0.964805</td>\n","      <td>0.999028</td>\n","      <td>0.981619</td>\n","      <td>0.964805</td>\n","      <td>0.999028</td>\n","    </tr>\n","    <tr>\n","      <td>20384</td>\n","      <td>0.000000</td>\n","      <td>0.000021</td>\n","      <td>0.989427</td>\n","      <td>0.981892</td>\n","      <td>0.997281</td>\n","      <td>0.995907</td>\n","      <td>0.995907</td>\n","      <td>0.995907</td>\n","      <td>0.981150</td>\n","      <td>0.963901</td>\n","      <td>0.999028</td>\n","      <td>0.981150</td>\n","      <td>0.963901</td>\n","      <td>0.999028</td>\n","    </tr>\n","    <tr>\n","      <td>21658</td>\n","      <td>0.000000</td>\n","      <td>0.000021</td>\n","      <td>0.989561</td>\n","      <td>0.982024</td>\n","      <td>0.997417</td>\n","      <td>0.995959</td>\n","      <td>0.995959</td>\n","      <td>0.995959</td>\n","      <td>0.981389</td>\n","      <td>0.964135</td>\n","      <td>0.999271</td>\n","      <td>0.981389</td>\n","      <td>0.964135</td>\n","      <td>0.999271</td>\n","    </tr>\n","    <tr>\n","      <td>22932</td>\n","      <td>0.000000</td>\n","      <td>0.000021</td>\n","      <td>0.989493</td>\n","      <td>0.982005</td>\n","      <td>0.997296</td>\n","      <td>0.995933</td>\n","      <td>0.995933</td>\n","      <td>0.995933</td>\n","      <td>0.981267</td>\n","      <td>0.964127</td>\n","      <td>0.999028</td>\n","      <td>0.981267</td>\n","      <td>0.964127</td>\n","      <td>0.999028</td>\n","    </tr>\n","    <tr>\n","      <td>24206</td>\n","      <td>0.000000</td>\n","      <td>0.000021</td>\n","      <td>0.989493</td>\n","      <td>0.982005</td>\n","      <td>0.997296</td>\n","      <td>0.995933</td>\n","      <td>0.995933</td>\n","      <td>0.995933</td>\n","      <td>0.981267</td>\n","      <td>0.964127</td>\n","      <td>0.999028</td>\n","      <td>0.981267</td>\n","      <td>0.964127</td>\n","      <td>0.999028</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-10192\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-11466\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12740\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14014\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-15288\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16562\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17836\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19110\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20384\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21658\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-22932\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24206\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9894930683107281, 'eval_macro_precision': 0.9820050648606685, 'eval_macro_recall': 0.9972959928266589, 'eval_micro_f1': 0.9959331692785909, 'eval_micro_precision': 0.9959331692785909, 'eval_micro_recall': 0.9959331692785909, 'eval_macro_f1_no_o': 0.9812671518911825, 'eval_macro_precision_no_o': 0.964126611957796, 'eval_macro_recall_no_o': 0.9990281827016521, 'eval_micro_f1_no_o': 0.9812671518911825, 'eval_micro_precision_no_o': 0.964126611957796, 'eval_micro_recall_no_o': 0.9990281827016521, 'eval_loss': 2.1027348815078e-05, 'eval_runtime': 16.0243, 'eval_samples_per_second': 19.845, 'eval_steps_per_second': 4.992, 'epoch': 19.0}\n","Accuracy for fold  3 :  0.9812671518911825  --  0.9959331692785909\n","--------------------------------\n","FOLD 4\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5100\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 24225\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='24225' max='24225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [24225/24225 58:21, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1275</td>\n","      <td>0.000000</td>\n","      <td>0.000012</td>\n","      <td>0.987071</td>\n","      <td>0.977531</td>\n","      <td>0.997121</td>\n","      <td>0.995226</td>\n","      <td>0.995226</td>\n","      <td>0.995226</td>\n","      <td>0.976802</td>\n","      <td>0.955119</td>\n","      <td>0.999492</td>\n","      <td>0.976802</td>\n","      <td>0.955119</td>\n","      <td>0.999492</td>\n","    </tr>\n","    <tr>\n","      <td>2550</td>\n","      <td>0.000000</td>\n","      <td>0.000005</td>\n","      <td>0.992467</td>\n","      <td>0.986862</td>\n","      <td>0.998242</td>\n","      <td>0.997243</td>\n","      <td>0.997243</td>\n","      <td>0.997243</td>\n","      <td>0.986470</td>\n","      <td>0.973782</td>\n","      <td>0.999492</td>\n","      <td>0.986470</td>\n","      <td>0.973782</td>\n","      <td>0.999492</td>\n","    </tr>\n","    <tr>\n","      <td>3825</td>\n","      <td>0.000000</td>\n","      <td>0.000004</td>\n","      <td>0.992950</td>\n","      <td>0.987707</td>\n","      <td>0.998341</td>\n","      <td>0.997422</td>\n","      <td>0.997422</td>\n","      <td>0.997422</td>\n","      <td>0.987335</td>\n","      <td>0.975471</td>\n","      <td>0.999492</td>\n","      <td>0.987335</td>\n","      <td>0.975471</td>\n","      <td>0.999492</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>0.000000</td>\n","      <td>0.000004</td>\n","      <td>0.992330</td>\n","      <td>0.986622</td>\n","      <td>0.998214</td>\n","      <td>0.997192</td>\n","      <td>0.997192</td>\n","      <td>0.997192</td>\n","      <td>0.986222</td>\n","      <td>0.973300</td>\n","      <td>0.999492</td>\n","      <td>0.986222</td>\n","      <td>0.973300</td>\n","      <td>0.999492</td>\n","    </tr>\n","    <tr>\n","      <td>6375</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.992607</td>\n","      <td>0.987000</td>\n","      <td>0.998383</td>\n","      <td>0.997294</td>\n","      <td>0.997294</td>\n","      <td>0.997294</td>\n","      <td>0.986720</td>\n","      <td>0.974029</td>\n","      <td>0.999746</td>\n","      <td>0.986720</td>\n","      <td>0.974029</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>7650</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.992676</td>\n","      <td>0.987121</td>\n","      <td>0.998397</td>\n","      <td>0.997320</td>\n","      <td>0.997320</td>\n","      <td>0.997320</td>\n","      <td>0.986844</td>\n","      <td>0.974270</td>\n","      <td>0.999746</td>\n","      <td>0.986844</td>\n","      <td>0.974270</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>8925</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.993090</td>\n","      <td>0.987845</td>\n","      <td>0.998482</td>\n","      <td>0.997473</td>\n","      <td>0.997473</td>\n","      <td>0.997473</td>\n","      <td>0.987586</td>\n","      <td>0.975719</td>\n","      <td>0.999746</td>\n","      <td>0.987586</td>\n","      <td>0.975719</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>10200</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.990820</td>\n","      <td>0.983888</td>\n","      <td>0.998014</td>\n","      <td>0.996630</td>\n","      <td>0.996630</td>\n","      <td>0.996630</td>\n","      <td>0.983516</td>\n","      <td>0.967805</td>\n","      <td>0.999746</td>\n","      <td>0.983516</td>\n","      <td>0.967805</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>11475</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.993090</td>\n","      <td>0.987845</td>\n","      <td>0.998482</td>\n","      <td>0.997473</td>\n","      <td>0.997473</td>\n","      <td>0.997473</td>\n","      <td>0.987586</td>\n","      <td>0.975719</td>\n","      <td>0.999746</td>\n","      <td>0.987586</td>\n","      <td>0.975719</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>12750</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.987955</td>\n","      <td>0.978945</td>\n","      <td>0.997418</td>\n","      <td>0.995558</td>\n","      <td>0.995558</td>\n","      <td>0.995558</td>\n","      <td>0.978385</td>\n","      <td>0.957918</td>\n","      <td>0.999746</td>\n","      <td>0.978385</td>\n","      <td>0.957918</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>14025</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.989521</td>\n","      <td>0.981639</td>\n","      <td>0.997744</td>\n","      <td>0.996145</td>\n","      <td>0.996145</td>\n","      <td>0.996145</td>\n","      <td>0.981188</td>\n","      <td>0.963307</td>\n","      <td>0.999746</td>\n","      <td>0.981188</td>\n","      <td>0.963307</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>15300</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.989725</td>\n","      <td>0.981993</td>\n","      <td>0.997787</td>\n","      <td>0.996222</td>\n","      <td>0.996222</td>\n","      <td>0.996222</td>\n","      <td>0.981555</td>\n","      <td>0.964015</td>\n","      <td>0.999746</td>\n","      <td>0.981555</td>\n","      <td>0.964015</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>16575</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.991026</td>\n","      <td>0.984245</td>\n","      <td>0.998057</td>\n","      <td>0.996707</td>\n","      <td>0.996707</td>\n","      <td>0.996707</td>\n","      <td>0.983885</td>\n","      <td>0.968519</td>\n","      <td>0.999746</td>\n","      <td>0.983885</td>\n","      <td>0.968519</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>17850</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.990477</td>\n","      <td>0.983295</td>\n","      <td>0.997943</td>\n","      <td>0.996503</td>\n","      <td>0.996503</td>\n","      <td>0.996503</td>\n","      <td>0.982903</td>\n","      <td>0.966618</td>\n","      <td>0.999746</td>\n","      <td>0.982903</td>\n","      <td>0.966618</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>19125</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.990546</td>\n","      <td>0.983413</td>\n","      <td>0.997957</td>\n","      <td>0.996528</td>\n","      <td>0.996528</td>\n","      <td>0.996528</td>\n","      <td>0.983025</td>\n","      <td>0.966855</td>\n","      <td>0.999746</td>\n","      <td>0.983025</td>\n","      <td>0.966855</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>20400</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.991300</td>\n","      <td>0.984722</td>\n","      <td>0.998113</td>\n","      <td>0.996809</td>\n","      <td>0.996809</td>\n","      <td>0.996809</td>\n","      <td>0.984377</td>\n","      <td>0.969473</td>\n","      <td>0.999746</td>\n","      <td>0.984377</td>\n","      <td>0.969473</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>21675</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.991231</td>\n","      <td>0.984603</td>\n","      <td>0.998099</td>\n","      <td>0.996784</td>\n","      <td>0.996784</td>\n","      <td>0.996784</td>\n","      <td>0.984254</td>\n","      <td>0.969235</td>\n","      <td>0.999746</td>\n","      <td>0.984254</td>\n","      <td>0.969235</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>22950</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.991437</td>\n","      <td>0.984961</td>\n","      <td>0.998142</td>\n","      <td>0.996860</td>\n","      <td>0.996860</td>\n","      <td>0.996860</td>\n","      <td>0.984623</td>\n","      <td>0.969951</td>\n","      <td>0.999746</td>\n","      <td>0.984623</td>\n","      <td>0.969951</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <td>24225</td>\n","      <td>0.000000</td>\n","      <td>0.000003</td>\n","      <td>0.991437</td>\n","      <td>0.984961</td>\n","      <td>0.998142</td>\n","      <td>0.996860</td>\n","      <td>0.996860</td>\n","      <td>0.996860</td>\n","      <td>0.984623</td>\n","      <td>0.969951</td>\n","      <td>0.999746</td>\n","      <td>0.984623</td>\n","      <td>0.969951</td>\n","      <td>0.999746</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1275\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2550\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-3825\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5100\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6375\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-7650\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8925\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10200\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-11475\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12750\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14025\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-15300\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16575\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17850\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19125\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20400\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21675\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-22950\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24225\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished. Saving trained model.\n","Starting testing\n","{'eval_macro_f1': 0.9914373260893067, 'eval_macro_precision': 0.9849611301233898, 'eval_macro_recall': 0.9981418310806423, 'eval_micro_f1': 0.9968601623525808, 'eval_micro_precision': 0.9968601623525808, 'eval_micro_recall': 0.9968601623525808, 'eval_macro_f1_no_o': 0.9846230778847357, 'eval_macro_precision_no_o': 0.9699507389162562, 'eval_macro_recall_no_o': 0.9997461284589998, 'eval_micro_f1_no_o': 0.9846230778847357, 'eval_micro_precision_no_o': 0.9699507389162562, 'eval_micro_recall_no_o': 0.9997461284589998, 'eval_loss': 2.779848126853917e-06, 'eval_runtime': 15.7269, 'eval_samples_per_second': 20.22, 'eval_steps_per_second': 5.087, 'epoch': 19.0}\n","Accuracy for fold  4 :  0.9846230778847357  --  0.9968601623525808\n","--------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"BBA4owXH8oS1"},"source":["# Results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhusoblW-oCA","executionInfo":{"status":"ok","timestamp":1646876770212,"user_tz":360,"elapsed":8,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"253c3c6c-aaef-4033-a960-cb00edf7d55a"},"source":["results"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.93879451, 0.95259534, 0.97988939, 0.98126715, 0.98462308])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["resultss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BB3kG6PDP9_c","executionInfo":{"status":"ok","timestamp":1646876770212,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"be8d9de3-eea1-48c2-bc6c-23d7d184af59"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.98648996, 0.99054161, 0.99587639, 0.99593317, 0.99686016])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPIH-gt_-foF","executionInfo":{"status":"ok","timestamp":1646876770213,"user_tz":360,"elapsed":4,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"991b1ca2-ef52-40b8-b484-a3abb7a61575"},"source":["# Print fold results\n","print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n","print('--------------------------------')\n","sum = 0.0\n","key = 0\n","for value in results:\n","  print(f'Fold {key}: {value} %')\n","  sum += value\n","  key += 1\n","print(f'Average: {sum/len(results)} %')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n","--------------------------------\n","Fold 0: 0.9387945077961368 %\n","Fold 1: 0.9525953389830508 %\n","Fold 2: 0.9798893916540974 %\n","Fold 3: 0.9812671518911825 %\n","Fold 4: 0.9846230778847357 %\n","Average: 0.9674338936418406 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"1DkDHptAhniL"},"source":["# Pytorch Training - Loop UPDT"]},{"cell_type":"code","metadata":{"id":"rirS-e83GVn0"},"source":["device = 'cuda'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSDdzo_so0vk"},"source":["training_args = TrainingArguments(\"trained_scibert_ner_model\", # output dir\n","                                      learning_rate=1.1497917821120578e-05, \n","                                      num_train_epochs=19, \n","                                      dataloader_drop_last=True,\n","                                      per_device_eval_batch_size=4, \n","                                      per_device_train_batch_size=4,\n","                                      logging_steps=50,\n","                                      save_steps=len(train) // batch_size,\n","                                      lr_scheduler_type='cosine',\n","                                      evaluation_strategy='steps',\n","                                      weight_decay=0.3660243351216811,\n","                                      eval_steps=len(train) // batch_size,\n","                                      report_to='all'\n","                                      )\n","\n","#learning_rate 1.1497917821120578e-05\n","#num_train_epochs 19\n","#weight_decay 0.3660243351216811\n","#per_device_train_batch_size 4\n","\n","#load_param()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"fa00d9f7-e58f-4133-f62f-f99087693e1a","id":"nZR3aDuKyUqy","executionInfo":{"status":"ok","timestamp":1647112306593,"user_tz":360,"elapsed":43480812,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}}},"source":["loop_val = 10\n","loop_results = np.zeros(loop_val)\n","loop_resultss = np.zeros(loop_val)\n","for r in range(loop_val):\n","\n","  ner_model = NerModel(BertEmbModel).to('cuda')  # make sure we move the model to the GPU for training\n","\n","  trainer = MultilabelTrainer(\n","      model=ner_model, \n","      args=training_args, \n","      train_dataset=train, \n","      eval_dataset=test,\n","      data_collator=collator  # defines how to merge data into batches, using the collator function above\n","  )\n","\n","  # Print\n","  print(f'Train run #{r}')\n","  print('--------------------------------')\n","\n","  trainer.train()\n","\n","  # Process is complete.\n","  print('Training process has finished.')\n","\n","  # Print about testing\n","  print('Starting testing')\n","\n","  with torch.no_grad():\n","    result = trainer.evaluate(test)\n","    print(result)\n","\n","    # Print accuracy\n","    print('Accuracy for fold ', r, ': ', result['eval_micro_f1_no_o'], ' -- ', result['eval_micro_f1'])\n","    print('--------------------------------')\n","    loop_results[r] = result['eval_micro_f1_no_o']\n","    loop_resultss[r] = result['eval_micro_f1']\n","    del result\n","\n","  if r > 0:\n","    if loop_results[r] < loop_results[r-1]:\n","      save_path = f'./model-fold-{r}.pth'\n","      torch.save(ner_model.state_dict(), save_path)\n","\n","  print('Testing process has finished.')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"stream","name":"stdout","text":["Train run #0\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:10:18, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000100</td>\n","      <td>0.000231</td>\n","      <td>0.923174</td>\n","      <td>0.883593</td>\n","      <td>0.976410</td>\n","      <td>0.968958</td>\n","      <td>0.968958</td>\n","      <td>0.968958</td>\n","      <td>0.863867</td>\n","      <td>0.768823</td>\n","      <td>0.985724</td>\n","      <td>0.863867</td>\n","      <td>0.768823</td>\n","      <td>0.985724</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000100</td>\n","      <td>0.000255</td>\n","      <td>0.911743</td>\n","      <td>0.866943</td>\n","      <td>0.976055</td>\n","      <td>0.963422</td>\n","      <td>0.963422</td>\n","      <td>0.963422</td>\n","      <td>0.844207</td>\n","      <td>0.734827</td>\n","      <td>0.991842</td>\n","      <td>0.844207</td>\n","      <td>0.734827</td>\n","      <td>0.991842</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.000153</td>\n","      <td>0.937894</td>\n","      <td>0.904510</td>\n","      <td>0.979919</td>\n","      <td>0.975547</td>\n","      <td>0.975547</td>\n","      <td>0.975547</td>\n","      <td>0.889537</td>\n","      <td>0.810682</td>\n","      <td>0.985384</td>\n","      <td>0.889537</td>\n","      <td>0.810682</td>\n","      <td>0.985384</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.000156</td>\n","      <td>0.937783</td>\n","      <td>0.904532</td>\n","      <td>0.979598</td>\n","      <td>0.975513</td>\n","      <td>0.975513</td>\n","      <td>0.975513</td>\n","      <td>0.889332</td>\n","      <td>0.810803</td>\n","      <td>0.984704</td>\n","      <td>0.889332</td>\n","      <td>0.810803</td>\n","      <td>0.984704</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.000209</td>\n","      <td>0.941372</td>\n","      <td>0.910853</td>\n","      <td>0.978842</td>\n","      <td>0.977143</td>\n","      <td>0.977143</td>\n","      <td>0.977143</td>\n","      <td>0.895578</td>\n","      <td>0.823865</td>\n","      <td>0.980965</td>\n","      <td>0.895578</td>\n","      <td>0.823865</td>\n","      <td>0.980965</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.000258</td>\n","      <td>0.952566</td>\n","      <td>0.930717</td>\n","      <td>0.977612</td>\n","      <td>0.982000</td>\n","      <td>0.982000</td>\n","      <td>0.982000</td>\n","      <td>0.915200</td>\n","      <td>0.864571</td>\n","      <td>0.972128</td>\n","      <td>0.915200</td>\n","      <td>0.864571</td>\n","      <td>0.972128</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.000152</td>\n","      <td>0.946956</td>\n","      <td>0.917627</td>\n","      <td>0.982523</td>\n","      <td>0.979419</td>\n","      <td>0.979419</td>\n","      <td>0.979419</td>\n","      <td>0.905460</td>\n","      <td>0.836794</td>\n","      <td>0.986404</td>\n","      <td>0.905460</td>\n","      <td>0.836794</td>\n","      <td>0.986404</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.000200</td>\n","      <td>0.935472</td>\n","      <td>0.900288</td>\n","      <td>0.980506</td>\n","      <td>0.974426</td>\n","      <td>0.974426</td>\n","      <td>0.974426</td>\n","      <td>0.885336</td>\n","      <td>0.801931</td>\n","      <td>0.988103</td>\n","      <td>0.885336</td>\n","      <td>0.801931</td>\n","      <td>0.988103</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.000117</td>\n","      <td>0.937711</td>\n","      <td>0.902705</td>\n","      <td>0.982375</td>\n","      <td>0.975343</td>\n","      <td>0.975343</td>\n","      <td>0.975343</td>\n","      <td>0.889296</td>\n","      <td>0.806416</td>\n","      <td>0.991162</td>\n","      <td>0.889296</td>\n","      <td>0.806416</td>\n","      <td>0.991162</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.000193</td>\n","      <td>0.949190</td>\n","      <td>0.919989</td>\n","      <td>0.984525</td>\n","      <td>0.980302</td>\n","      <td>0.980302</td>\n","      <td>0.980302</td>\n","      <td>0.909432</td>\n","      <td>0.841132</td>\n","      <td>0.989803</td>\n","      <td>0.909432</td>\n","      <td>0.841132</td>\n","      <td>0.989803</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.000185</td>\n","      <td>0.949998</td>\n","      <td>0.921207</td>\n","      <td>0.984714</td>\n","      <td>0.980641</td>\n","      <td>0.980641</td>\n","      <td>0.980641</td>\n","      <td>0.910854</td>\n","      <td>0.843569</td>\n","      <td>0.989803</td>\n","      <td>0.910854</td>\n","      <td>0.843569</td>\n","      <td>0.989803</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.000192</td>\n","      <td>0.949124</td>\n","      <td>0.919788</td>\n","      <td>0.984657</td>\n","      <td>0.980268</td>\n","      <td>0.980268</td>\n","      <td>0.980268</td>\n","      <td>0.909318</td>\n","      <td>0.840693</td>\n","      <td>0.990143</td>\n","      <td>0.909318</td>\n","      <td>0.840693</td>\n","      <td>0.990143</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.000227</td>\n","      <td>0.953240</td>\n","      <td>0.928903</td>\n","      <td>0.981615</td>\n","      <td>0.982136</td>\n","      <td>0.982136</td>\n","      <td>0.982136</td>\n","      <td>0.916481</td>\n","      <td>0.859952</td>\n","      <td>0.980965</td>\n","      <td>0.916481</td>\n","      <td>0.859952</td>\n","      <td>0.980965</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.000227</td>\n","      <td>0.952317</td>\n","      <td>0.926297</td>\n","      <td>0.983032</td>\n","      <td>0.981694</td>\n","      <td>0.981694</td>\n","      <td>0.981694</td>\n","      <td>0.914890</td>\n","      <td>0.854320</td>\n","      <td>0.984704</td>\n","      <td>0.914890</td>\n","      <td>0.854320</td>\n","      <td>0.984704</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.000213</td>\n","      <td>0.953699</td>\n","      <td>0.928537</td>\n","      <td>0.983202</td>\n","      <td>0.982271</td>\n","      <td>0.982271</td>\n","      <td>0.982271</td>\n","      <td>0.917327</td>\n","      <td>0.858837</td>\n","      <td>0.984364</td>\n","      <td>0.917327</td>\n","      <td>0.858837</td>\n","      <td>0.984364</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.000236</td>\n","      <td>0.954656</td>\n","      <td>0.931003</td>\n","      <td>0.982087</td>\n","      <td>0.982713</td>\n","      <td>0.982713</td>\n","      <td>0.982713</td>\n","      <td>0.918988</td>\n","      <td>0.864113</td>\n","      <td>0.981305</td>\n","      <td>0.918988</td>\n","      <td>0.864113</td>\n","      <td>0.981305</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.000297</td>\n","      <td>0.955692</td>\n","      <td>0.933743</td>\n","      <td>0.980840</td>\n","      <td>0.983188</td>\n","      <td>0.983188</td>\n","      <td>0.983188</td>\n","      <td>0.920787</td>\n","      <td>0.869973</td>\n","      <td>0.977906</td>\n","      <td>0.920787</td>\n","      <td>0.869973</td>\n","      <td>0.977906</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.000275</td>\n","      <td>0.955555</td>\n","      <td>0.932522</td>\n","      <td>0.982144</td>\n","      <td>0.983087</td>\n","      <td>0.983087</td>\n","      <td>0.983087</td>\n","      <td>0.920574</td>\n","      <td>0.867188</td>\n","      <td>0.980965</td>\n","      <td>0.920574</td>\n","      <td>0.867188</td>\n","      <td>0.980965</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.000280</td>\n","      <td>0.955804</td>\n","      <td>0.932913</td>\n","      <td>0.982200</td>\n","      <td>0.983188</td>\n","      <td>0.983188</td>\n","      <td>0.983188</td>\n","      <td>0.921015</td>\n","      <td>0.867970</td>\n","      <td>0.980965</td>\n","      <td>0.921015</td>\n","      <td>0.867970</td>\n","      <td>0.980965</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.9558039744255709, 'eval_macro_precision': 0.9329129458663312, 'eval_macro_recall': 0.9822002710722395, 'eval_micro_f1': 0.9831884254856678, 'eval_micro_precision': 0.9831884254856678, 'eval_micro_recall': 0.9831884254856678, 'eval_macro_f1_no_o': 0.9210148396361895, 'eval_macro_precision_no_o': 0.8679699248120301, 'eval_macro_recall_no_o': 0.9809653297076818, 'eval_micro_f1_no_o': 0.9210148396361895, 'eval_micro_precision_no_o': 0.8679699248120301, 'eval_micro_recall_no_o': 0.9809653297076818, 'eval_loss': 0.00027959382999607606, 'eval_runtime': 16.9163, 'eval_samples_per_second': 13.892, 'eval_steps_per_second': 3.488, 'epoch': 19.0}\n","Accuracy for fold  0 :  0.9210148396361895  --  0.9831884254856678\n","--------------------------------\n","Testing process has finished.\n","Train run #1\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:10:15, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000000</td>\n","      <td>0.000535</td>\n","      <td>0.956909</td>\n","      <td>0.945123</td>\n","      <td>0.969546</td>\n","      <td>0.984071</td>\n","      <td>0.984071</td>\n","      <td>0.984071</td>\n","      <td>0.922697</td>\n","      <td>0.895680</td>\n","      <td>0.951394</td>\n","      <td>0.922697</td>\n","      <td>0.895680</td>\n","      <td>0.951394</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000000</td>\n","      <td>0.000674</td>\n","      <td>0.955438</td>\n","      <td>0.937979</td>\n","      <td>0.974854</td>\n","      <td>0.983290</td>\n","      <td>0.983290</td>\n","      <td>0.983290</td>\n","      <td>0.920208</td>\n","      <td>0.879963</td>\n","      <td>0.964310</td>\n","      <td>0.920208</td>\n","      <td>0.879963</td>\n","      <td>0.964310</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.000516</td>\n","      <td>0.956887</td>\n","      <td>0.939372</td>\n","      <td>0.976364</td>\n","      <td>0.983834</td>\n","      <td>0.983834</td>\n","      <td>0.983834</td>\n","      <td>0.922802</td>\n","      <td>0.882444</td>\n","      <td>0.967029</td>\n","      <td>0.922802</td>\n","      <td>0.882444</td>\n","      <td>0.967029</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.000278</td>\n","      <td>0.948839</td>\n","      <td>0.922009</td>\n","      <td>0.980748</td>\n","      <td>0.980302</td>\n","      <td>0.980302</td>\n","      <td>0.980302</td>\n","      <td>0.908719</td>\n","      <td>0.846131</td>\n","      <td>0.981305</td>\n","      <td>0.908719</td>\n","      <td>0.846131</td>\n","      <td>0.981305</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.000638</td>\n","      <td>0.953885</td>\n","      <td>0.933606</td>\n","      <td>0.976875</td>\n","      <td>0.982577</td>\n","      <td>0.982577</td>\n","      <td>0.982577</td>\n","      <td>0.917511</td>\n","      <td>0.870613</td>\n","      <td>0.969748</td>\n","      <td>0.917511</td>\n","      <td>0.870613</td>\n","      <td>0.969748</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.000847</td>\n","      <td>0.960026</td>\n","      <td>0.944497</td>\n","      <td>0.977062</td>\n","      <td>0.985090</td>\n","      <td>0.985090</td>\n","      <td>0.985090</td>\n","      <td>0.928373</td>\n","      <td>0.892689</td>\n","      <td>0.967029</td>\n","      <td>0.928373</td>\n","      <td>0.892689</td>\n","      <td>0.967029</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.000381</td>\n","      <td>0.933268</td>\n","      <td>0.896278</td>\n","      <td>0.981432</td>\n","      <td>0.973373</td>\n","      <td>0.973373</td>\n","      <td>0.973373</td>\n","      <td>0.881535</td>\n","      <td>0.793526</td>\n","      <td>0.991502</td>\n","      <td>0.881535</td>\n","      <td>0.793526</td>\n","      <td>0.991502</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.000215</td>\n","      <td>0.943068</td>\n","      <td>0.912696</td>\n","      <td>0.980277</td>\n","      <td>0.977822</td>\n","      <td>0.977822</td>\n","      <td>0.977822</td>\n","      <td>0.898587</td>\n","      <td>0.827281</td>\n","      <td>0.983345</td>\n","      <td>0.898587</td>\n","      <td>0.827281</td>\n","      <td>0.983345</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.000241</td>\n","      <td>0.941140</td>\n","      <td>0.908135</td>\n","      <td>0.982468</td>\n","      <td>0.976871</td>\n","      <td>0.976871</td>\n","      <td>0.976871</td>\n","      <td>0.895279</td>\n","      <td>0.817467</td>\n","      <td>0.989463</td>\n","      <td>0.895279</td>\n","      <td>0.817467</td>\n","      <td>0.989463</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.000241</td>\n","      <td>0.945101</td>\n","      <td>0.914945</td>\n","      <td>0.981938</td>\n","      <td>0.978637</td>\n","      <td>0.978637</td>\n","      <td>0.978637</td>\n","      <td>0.902193</td>\n","      <td>0.831470</td>\n","      <td>0.986064</td>\n","      <td>0.902193</td>\n","      <td>0.831470</td>\n","      <td>0.986064</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.000274</td>\n","      <td>0.945927</td>\n","      <td>0.917078</td>\n","      <td>0.980805</td>\n","      <td>0.979045</td>\n","      <td>0.979045</td>\n","      <td>0.979045</td>\n","      <td>0.903609</td>\n","      <td>0.836080</td>\n","      <td>0.983005</td>\n","      <td>0.903609</td>\n","      <td>0.836080</td>\n","      <td>0.983005</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.000268</td>\n","      <td>0.945366</td>\n","      <td>0.916233</td>\n","      <td>0.980673</td>\n","      <td>0.978807</td>\n","      <td>0.978807</td>\n","      <td>0.978807</td>\n","      <td>0.902622</td>\n","      <td>0.834391</td>\n","      <td>0.983005</td>\n","      <td>0.902622</td>\n","      <td>0.834391</td>\n","      <td>0.983005</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.000315</td>\n","      <td>0.950384</td>\n","      <td>0.925013</td>\n","      <td>0.980219</td>\n","      <td>0.980981</td>\n","      <td>0.980981</td>\n","      <td>0.980981</td>\n","      <td>0.911420</td>\n","      <td>0.852367</td>\n","      <td>0.979266</td>\n","      <td>0.911420</td>\n","      <td>0.852367</td>\n","      <td>0.979266</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.000269</td>\n","      <td>0.948239</td>\n","      <td>0.920168</td>\n","      <td>0.981938</td>\n","      <td>0.979996</td>\n","      <td>0.979996</td>\n","      <td>0.979996</td>\n","      <td>0.907695</td>\n","      <td>0.842105</td>\n","      <td>0.984364</td>\n","      <td>0.907695</td>\n","      <td>0.842105</td>\n","      <td>0.984364</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.000291</td>\n","      <td>0.950970</td>\n","      <td>0.925812</td>\n","      <td>0.980502</td>\n","      <td>0.981219</td>\n","      <td>0.981219</td>\n","      <td>0.981219</td>\n","      <td>0.912458</td>\n","      <td>0.853926</td>\n","      <td>0.979606</td>\n","      <td>0.912458</td>\n","      <td>0.853926</td>\n","      <td>0.979606</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.000306</td>\n","      <td>0.949882</td>\n","      <td>0.923712</td>\n","      <td>0.980842</td>\n","      <td>0.980743</td>\n","      <td>0.980743</td>\n","      <td>0.980743</td>\n","      <td>0.910554</td>\n","      <td>0.849573</td>\n","      <td>0.980965</td>\n","      <td>0.910554</td>\n","      <td>0.849573</td>\n","      <td>0.980965</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.000319</td>\n","      <td>0.950071</td>\n","      <td>0.924425</td>\n","      <td>0.980294</td>\n","      <td>0.980845</td>\n","      <td>0.980845</td>\n","      <td>0.980845</td>\n","      <td>0.910872</td>\n","      <td>0.851152</td>\n","      <td>0.979606</td>\n","      <td>0.910872</td>\n","      <td>0.851152</td>\n","      <td>0.979606</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.000325</td>\n","      <td>0.950492</td>\n","      <td>0.925608</td>\n","      <td>0.979652</td>\n","      <td>0.981049</td>\n","      <td>0.981049</td>\n","      <td>0.981049</td>\n","      <td>0.911597</td>\n","      <td>0.853709</td>\n","      <td>0.977906</td>\n","      <td>0.911597</td>\n","      <td>0.853709</td>\n","      <td>0.977906</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.000326</td>\n","      <td>0.950546</td>\n","      <td>0.925907</td>\n","      <td>0.979369</td>\n","      <td>0.981083</td>\n","      <td>0.981083</td>\n","      <td>0.981083</td>\n","      <td>0.911685</td>\n","      <td>0.854383</td>\n","      <td>0.977226</td>\n","      <td>0.911685</td>\n","      <td>0.854383</td>\n","      <td>0.977226</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.9505461244212561, 'eval_macro_precision': 0.9259071205920382, 'eval_macro_recall': 0.979368602992958, 'eval_micro_f1': 0.9810827333242766, 'eval_micro_precision': 0.9810827333242766, 'eval_micro_recall': 0.9810827333242766, 'eval_macro_f1_no_o': 0.9116854288885364, 'eval_macro_precision_no_o': 0.8543833580980683, 'eval_macro_recall_no_o': 0.9772263766145479, 'eval_micro_f1_no_o': 0.9116854288885364, 'eval_micro_precision_no_o': 0.8543833580980683, 'eval_micro_recall_no_o': 0.9772263766145479, 'eval_loss': 0.0003258758522068709, 'eval_runtime': 17.7347, 'eval_samples_per_second': 13.251, 'eval_steps_per_second': 3.327, 'epoch': 19.0}\n","Accuracy for fold  1 :  0.9116854288885364  --  0.9810827333242766\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"stream","name":"stdout","text":["Testing process has finished.\n","Train run #2\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:10:41, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000000</td>\n","      <td>0.000908</td>\n","      <td>0.958418</td>\n","      <td>0.949914</td>\n","      <td>0.967355</td>\n","      <td>0.984751</td>\n","      <td>0.984751</td>\n","      <td>0.984751</td>\n","      <td>0.925328</td>\n","      <td>0.905894</td>\n","      <td>0.945615</td>\n","      <td>0.925328</td>\n","      <td>0.905894</td>\n","      <td>0.945615</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000000</td>\n","      <td>0.000436</td>\n","      <td>0.961583</td>\n","      <td>0.944965</td>\n","      <td>0.979933</td>\n","      <td>0.985634</td>\n","      <td>0.985634</td>\n","      <td>0.985634</td>\n","      <td>0.931186</td>\n","      <td>0.892980</td>\n","      <td>0.972808</td>\n","      <td>0.931186</td>\n","      <td>0.892980</td>\n","      <td>0.972808</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.000478</td>\n","      <td>0.940134</td>\n","      <td>0.910275</td>\n","      <td>0.976633</td>\n","      <td>0.976702</td>\n","      <td>0.976702</td>\n","      <td>0.976702</td>\n","      <td>0.893346</td>\n","      <td>0.823209</td>\n","      <td>0.976547</td>\n","      <td>0.893346</td>\n","      <td>0.823209</td>\n","      <td>0.976547</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.000276</td>\n","      <td>0.950685</td>\n","      <td>0.923791</td>\n","      <td>0.982655</td>\n","      <td>0.981015</td>\n","      <td>0.981015</td>\n","      <td>0.981015</td>\n","      <td>0.912010</td>\n","      <td>0.849311</td>\n","      <td>0.984704</td>\n","      <td>0.912010</td>\n","      <td>0.849311</td>\n","      <td>0.984704</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.000368</td>\n","      <td>0.949178</td>\n","      <td>0.922423</td>\n","      <td>0.980974</td>\n","      <td>0.980437</td>\n","      <td>0.980437</td>\n","      <td>0.980437</td>\n","      <td>0.909320</td>\n","      <td>0.846921</td>\n","      <td>0.981645</td>\n","      <td>0.909320</td>\n","      <td>0.846921</td>\n","      <td>0.981645</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.000231</td>\n","      <td>0.953732</td>\n","      <td>0.933247</td>\n","      <td>0.976988</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.917242</td>\n","      <td>0.869857</td>\n","      <td>0.970088</td>\n","      <td>0.917242</td>\n","      <td>0.869857</td>\n","      <td>0.970088</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.000317</td>\n","      <td>0.947720</td>\n","      <td>0.920199</td>\n","      <td>0.980635</td>\n","      <td>0.979826</td>\n","      <td>0.979826</td>\n","      <td>0.979826</td>\n","      <td>0.906750</td>\n","      <td>0.842474</td>\n","      <td>0.981645</td>\n","      <td>0.906750</td>\n","      <td>0.842474</td>\n","      <td>0.981645</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.000267</td>\n","      <td>0.953413</td>\n","      <td>0.930489</td>\n","      <td>0.979878</td>\n","      <td>0.982271</td>\n","      <td>0.982271</td>\n","      <td>0.982271</td>\n","      <td>0.916746</td>\n","      <td>0.863582</td>\n","      <td>0.976886</td>\n","      <td>0.916746</td>\n","      <td>0.863582</td>\n","      <td>0.976886</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.000297</td>\n","      <td>0.950726</td>\n","      <td>0.923542</td>\n","      <td>0.983108</td>\n","      <td>0.981015</td>\n","      <td>0.981015</td>\n","      <td>0.981015</td>\n","      <td>0.912093</td>\n","      <td>0.848698</td>\n","      <td>0.985724</td>\n","      <td>0.912093</td>\n","      <td>0.848698</td>\n","      <td>0.985724</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.000492</td>\n","      <td>0.957291</td>\n","      <td>0.935370</td>\n","      <td>0.982389</td>\n","      <td>0.983800</td>\n","      <td>0.983800</td>\n","      <td>0.983800</td>\n","      <td>0.923643</td>\n","      <td>0.872920</td>\n","      <td>0.980625</td>\n","      <td>0.923643</td>\n","      <td>0.872920</td>\n","      <td>0.980625</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.000526</td>\n","      <td>0.955538</td>\n","      <td>0.933387</td>\n","      <td>0.980954</td>\n","      <td>0.983120</td>\n","      <td>0.983120</td>\n","      <td>0.983120</td>\n","      <td>0.920518</td>\n","      <td>0.869224</td>\n","      <td>0.978246</td>\n","      <td>0.920518</td>\n","      <td>0.869224</td>\n","      <td>0.978246</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.000787</td>\n","      <td>0.956629</td>\n","      <td>0.936602</td>\n","      <td>0.979273</td>\n","      <td>0.983630</td>\n","      <td>0.983630</td>\n","      <td>0.983630</td>\n","      <td>0.922408</td>\n","      <td>0.876147</td>\n","      <td>0.973827</td>\n","      <td>0.922408</td>\n","      <td>0.876147</td>\n","      <td>0.973827</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.000793</td>\n","      <td>0.957145</td>\n","      <td>0.937311</td>\n","      <td>0.979537</td>\n","      <td>0.983834</td>\n","      <td>0.983834</td>\n","      <td>0.983834</td>\n","      <td>0.923325</td>\n","      <td>0.877526</td>\n","      <td>0.974167</td>\n","      <td>0.923325</td>\n","      <td>0.877526</td>\n","      <td>0.974167</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.000658</td>\n","      <td>0.956445</td>\n","      <td>0.934145</td>\n","      <td>0.982049</td>\n","      <td>0.983460</td>\n","      <td>0.983460</td>\n","      <td>0.983460</td>\n","      <td>0.922142</td>\n","      <td>0.870510</td>\n","      <td>0.980286</td>\n","      <td>0.922142</td>\n","      <td>0.870510</td>\n","      <td>0.980286</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.000534</td>\n","      <td>0.955178</td>\n","      <td>0.931599</td>\n","      <td>0.982503</td>\n","      <td>0.982917</td>\n","      <td>0.982917</td>\n","      <td>0.982917</td>\n","      <td>0.919917</td>\n","      <td>0.865229</td>\n","      <td>0.981985</td>\n","      <td>0.919917</td>\n","      <td>0.865229</td>\n","      <td>0.981985</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.000577</td>\n","      <td>0.955899</td>\n","      <td>0.932952</td>\n","      <td>0.982370</td>\n","      <td>0.983222</td>\n","      <td>0.983222</td>\n","      <td>0.983222</td>\n","      <td>0.921187</td>\n","      <td>0.868010</td>\n","      <td>0.981305</td>\n","      <td>0.921187</td>\n","      <td>0.868010</td>\n","      <td>0.981305</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.000614</td>\n","      <td>0.956349</td>\n","      <td>0.934107</td>\n","      <td>0.981879</td>\n","      <td>0.983426</td>\n","      <td>0.983426</td>\n","      <td>0.983426</td>\n","      <td>0.921970</td>\n","      <td>0.870471</td>\n","      <td>0.979946</td>\n","      <td>0.921970</td>\n","      <td>0.870471</td>\n","      <td>0.979946</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.000622</td>\n","      <td>0.956325</td>\n","      <td>0.934293</td>\n","      <td>0.981577</td>\n","      <td>0.983426</td>\n","      <td>0.983426</td>\n","      <td>0.983426</td>\n","      <td>0.921920</td>\n","      <td>0.870919</td>\n","      <td>0.979266</td>\n","      <td>0.921920</td>\n","      <td>0.870919</td>\n","      <td>0.979266</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.000627</td>\n","      <td>0.956229</td>\n","      <td>0.934254</td>\n","      <td>0.981407</td>\n","      <td>0.983392</td>\n","      <td>0.983392</td>\n","      <td>0.983392</td>\n","      <td>0.921747</td>\n","      <td>0.870880</td>\n","      <td>0.978926</td>\n","      <td>0.921747</td>\n","      <td>0.870880</td>\n","      <td>0.978926</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.9562288947215236, 'eval_macro_precision': 0.9342539177303043, 'eval_macro_recall': 0.9814069545999915, 'eval_micro_f1': 0.9833922021464475, 'eval_micro_precision': 0.9833922021464475, 'eval_micro_recall': 0.9833922021464475, 'eval_macro_f1_no_o': 0.9217474795967355, 'eval_macro_precision_no_o': 0.8708799516177804, 'eval_macro_recall_no_o': 0.9789259007477906, 'eval_micro_f1_no_o': 0.9217474795967355, 'eval_micro_precision_no_o': 0.8708799516177804, 'eval_micro_recall_no_o': 0.9789259007477906, 'eval_loss': 0.0006267097897756206, 'eval_runtime': 16.0087, 'eval_samples_per_second': 14.68, 'eval_steps_per_second': 3.686, 'epoch': 19.0}\n","Accuracy for fold  2 :  0.9217474795967355  --  0.9833922021464475\n","--------------------------------\n","Testing process has finished.\n","Train run #3\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:10:22, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000000</td>\n","      <td>0.000432</td>\n","      <td>0.947999</td>\n","      <td>0.922715</td>\n","      <td>0.977745</td>\n","      <td>0.980064</td>\n","      <td>0.980064</td>\n","      <td>0.980064</td>\n","      <td>0.907164</td>\n","      <td>0.848270</td>\n","      <td>0.974847</td>\n","      <td>0.907164</td>\n","      <td>0.848270</td>\n","      <td>0.974847</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000000</td>\n","      <td>0.000425</td>\n","      <td>0.952812</td>\n","      <td>0.927699</td>\n","      <td>0.982258</td>\n","      <td>0.981932</td>\n","      <td>0.981932</td>\n","      <td>0.981932</td>\n","      <td>0.915743</td>\n","      <td>0.857355</td>\n","      <td>0.982665</td>\n","      <td>0.915743</td>\n","      <td>0.857355</td>\n","      <td>0.982665</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.000468</td>\n","      <td>0.956580</td>\n","      <td>0.936988</td>\n","      <td>0.978668</td>\n","      <td>0.983630</td>\n","      <td>0.983630</td>\n","      <td>0.983630</td>\n","      <td>0.922308</td>\n","      <td>0.877069</td>\n","      <td>0.972468</td>\n","      <td>0.922308</td>\n","      <td>0.877069</td>\n","      <td>0.972468</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.000608</td>\n","      <td>0.958422</td>\n","      <td>0.941749</td>\n","      <td>0.976855</td>\n","      <td>0.984445</td>\n","      <td>0.984445</td>\n","      <td>0.984445</td>\n","      <td>0.925528</td>\n","      <td>0.887157</td>\n","      <td>0.967369</td>\n","      <td>0.925528</td>\n","      <td>0.887157</td>\n","      <td>0.967369</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.000626</td>\n","      <td>0.958821</td>\n","      <td>0.941794</td>\n","      <td>0.977686</td>\n","      <td>0.984581</td>\n","      <td>0.984581</td>\n","      <td>0.984581</td>\n","      <td>0.926251</td>\n","      <td>0.887057</td>\n","      <td>0.969069</td>\n","      <td>0.926251</td>\n","      <td>0.887057</td>\n","      <td>0.969069</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.000684</td>\n","      <td>0.958507</td>\n","      <td>0.941887</td>\n","      <td>0.976874</td>\n","      <td>0.984479</td>\n","      <td>0.984479</td>\n","      <td>0.984479</td>\n","      <td>0.925679</td>\n","      <td>0.887434</td>\n","      <td>0.967369</td>\n","      <td>0.925679</td>\n","      <td>0.887434</td>\n","      <td>0.967369</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.000400</td>\n","      <td>0.954893</td>\n","      <td>0.934409</td>\n","      <td>0.978140</td>\n","      <td>0.982951</td>\n","      <td>0.982951</td>\n","      <td>0.982951</td>\n","      <td>0.919319</td>\n","      <td>0.871951</td>\n","      <td>0.972128</td>\n","      <td>0.919319</td>\n","      <td>0.871951</td>\n","      <td>0.972128</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.000436</td>\n","      <td>0.954420</td>\n","      <td>0.931961</td>\n","      <td>0.980255</td>\n","      <td>0.982679</td>\n","      <td>0.982679</td>\n","      <td>0.982679</td>\n","      <td>0.918530</td>\n","      <td>0.866486</td>\n","      <td>0.977226</td>\n","      <td>0.918530</td>\n","      <td>0.866486</td>\n","      <td>0.977226</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.000649</td>\n","      <td>0.957154</td>\n","      <td>0.938852</td>\n","      <td>0.977611</td>\n","      <td>0.983902</td>\n","      <td>0.983902</td>\n","      <td>0.983902</td>\n","      <td>0.923301</td>\n","      <td>0.881099</td>\n","      <td>0.969748</td>\n","      <td>0.923301</td>\n","      <td>0.881099</td>\n","      <td>0.969748</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.000692</td>\n","      <td>0.957335</td>\n","      <td>0.939025</td>\n","      <td>0.977799</td>\n","      <td>0.983970</td>\n","      <td>0.983970</td>\n","      <td>0.983970</td>\n","      <td>0.923625</td>\n","      <td>0.881408</td>\n","      <td>0.970088</td>\n","      <td>0.923625</td>\n","      <td>0.881408</td>\n","      <td>0.970088</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.000740</td>\n","      <td>0.957516</td>\n","      <td>0.939199</td>\n","      <td>0.977988</td>\n","      <td>0.984037</td>\n","      <td>0.984037</td>\n","      <td>0.984037</td>\n","      <td>0.923948</td>\n","      <td>0.881717</td>\n","      <td>0.970428</td>\n","      <td>0.923948</td>\n","      <td>0.881717</td>\n","      <td>0.970428</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.000808</td>\n","      <td>0.958336</td>\n","      <td>0.940766</td>\n","      <td>0.977875</td>\n","      <td>0.984377</td>\n","      <td>0.984377</td>\n","      <td>0.984377</td>\n","      <td>0.925397</td>\n","      <td>0.884926</td>\n","      <td>0.969748</td>\n","      <td>0.925397</td>\n","      <td>0.884926</td>\n","      <td>0.969748</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.000766</td>\n","      <td>0.957310</td>\n","      <td>0.939223</td>\n","      <td>0.977497</td>\n","      <td>0.983970</td>\n","      <td>0.983970</td>\n","      <td>0.983970</td>\n","      <td>0.923575</td>\n","      <td>0.881880</td>\n","      <td>0.969409</td>\n","      <td>0.923575</td>\n","      <td>0.881880</td>\n","      <td>0.969409</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.000770</td>\n","      <td>0.957925</td>\n","      <td>0.939980</td>\n","      <td>0.977932</td>\n","      <td>0.984207</td>\n","      <td>0.984207</td>\n","      <td>0.984207</td>\n","      <td>0.924672</td>\n","      <td>0.883318</td>\n","      <td>0.970088</td>\n","      <td>0.924672</td>\n","      <td>0.883318</td>\n","      <td>0.970088</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.000775</td>\n","      <td>0.957925</td>\n","      <td>0.939980</td>\n","      <td>0.977932</td>\n","      <td>0.984207</td>\n","      <td>0.984207</td>\n","      <td>0.984207</td>\n","      <td>0.924672</td>\n","      <td>0.883318</td>\n","      <td>0.970088</td>\n","      <td>0.924672</td>\n","      <td>0.883318</td>\n","      <td>0.970088</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.000732</td>\n","      <td>0.957528</td>\n","      <td>0.939100</td>\n","      <td>0.978139</td>\n","      <td>0.984037</td>\n","      <td>0.984037</td>\n","      <td>0.984037</td>\n","      <td>0.923973</td>\n","      <td>0.881481</td>\n","      <td>0.970768</td>\n","      <td>0.923973</td>\n","      <td>0.881481</td>\n","      <td>0.970768</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.000749</td>\n","      <td>0.958118</td>\n","      <td>0.940054</td>\n","      <td>0.978271</td>\n","      <td>0.984275</td>\n","      <td>0.984275</td>\n","      <td>0.984275</td>\n","      <td>0.925020</td>\n","      <td>0.883390</td>\n","      <td>0.970768</td>\n","      <td>0.925020</td>\n","      <td>0.883390</td>\n","      <td>0.970768</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.000756</td>\n","      <td>0.958203</td>\n","      <td>0.940191</td>\n","      <td>0.978290</td>\n","      <td>0.984309</td>\n","      <td>0.984309</td>\n","      <td>0.984309</td>\n","      <td>0.925170</td>\n","      <td>0.883663</td>\n","      <td>0.970768</td>\n","      <td>0.925170</td>\n","      <td>0.883663</td>\n","      <td>0.970768</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.000755</td>\n","      <td>0.958203</td>\n","      <td>0.940191</td>\n","      <td>0.978290</td>\n","      <td>0.984309</td>\n","      <td>0.984309</td>\n","      <td>0.984309</td>\n","      <td>0.925170</td>\n","      <td>0.883663</td>\n","      <td>0.970768</td>\n","      <td>0.925170</td>\n","      <td>0.883663</td>\n","      <td>0.970768</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.9582028964410441, 'eval_macro_precision': 0.9401912131545827, 'eval_macro_recall': 0.9782902882129235, 'eval_micro_f1': 0.9843091971199566, 'eval_micro_precision': 0.9843091971199566, 'eval_micro_recall': 0.9843091971199566, 'eval_macro_f1_no_o': 0.9251700680272108, 'eval_macro_precision_no_o': 0.8836633663366337, 'eval_macro_recall_no_o': 0.9707681849082257, 'eval_micro_f1_no_o': 0.9251700680272108, 'eval_micro_precision_no_o': 0.8836633663366337, 'eval_micro_recall_no_o': 0.9707681849082257, 'eval_loss': 0.0007548030963815601, 'eval_runtime': 17.604, 'eval_samples_per_second': 13.349, 'eval_steps_per_second': 3.352, 'epoch': 19.0}\n","Accuracy for fold  3 :  0.9251700680272108  --  0.9843091971199566\n","--------------------------------\n","Testing process has finished.\n","Train run #4\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:10:23, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000000</td>\n","      <td>0.000207</td>\n","      <td>0.945855</td>\n","      <td>0.916371</td>\n","      <td>0.981674</td>\n","      <td>0.978977</td>\n","      <td>0.978977</td>\n","      <td>0.978977</td>\n","      <td>0.903507</td>\n","      <td>0.834437</td>\n","      <td>0.985044</td>\n","      <td>0.903507</td>\n","      <td>0.834437</td>\n","      <td>0.985044</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000000</td>\n","      <td>0.000303</td>\n","      <td>0.950709</td>\n","      <td>0.926810</td>\n","      <td>0.978519</td>\n","      <td>0.981185</td>\n","      <td>0.981185</td>\n","      <td>0.981185</td>\n","      <td>0.911952</td>\n","      <td>0.856418</td>\n","      <td>0.975187</td>\n","      <td>0.911952</td>\n","      <td>0.856418</td>\n","      <td>0.975187</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.000363</td>\n","      <td>0.953872</td>\n","      <td>0.933700</td>\n","      <td>0.976724</td>\n","      <td>0.982577</td>\n","      <td>0.982577</td>\n","      <td>0.982577</td>\n","      <td>0.917484</td>\n","      <td>0.870840</td>\n","      <td>0.969409</td>\n","      <td>0.917484</td>\n","      <td>0.870840</td>\n","      <td>0.969409</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.000495</td>\n","      <td>0.953719</td>\n","      <td>0.935555</td>\n","      <td>0.974023</td>\n","      <td>0.982611</td>\n","      <td>0.982611</td>\n","      <td>0.982611</td>\n","      <td>0.917152</td>\n","      <td>0.875232</td>\n","      <td>0.963290</td>\n","      <td>0.917152</td>\n","      <td>0.875232</td>\n","      <td>0.963290</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.000553</td>\n","      <td>0.954046</td>\n","      <td>0.936919</td>\n","      <td>0.973060</td>\n","      <td>0.982781</td>\n","      <td>0.982781</td>\n","      <td>0.982781</td>\n","      <td>0.917708</td>\n","      <td>0.878223</td>\n","      <td>0.960911</td>\n","      <td>0.917708</td>\n","      <td>0.878223</td>\n","      <td>0.960911</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.000624</td>\n","      <td>0.954765</td>\n","      <td>0.938451</td>\n","      <td>0.972776</td>\n","      <td>0.983087</td>\n","      <td>0.983087</td>\n","      <td>0.983087</td>\n","      <td>0.918972</td>\n","      <td>0.881398</td>\n","      <td>0.959891</td>\n","      <td>0.918972</td>\n","      <td>0.881398</td>\n","      <td>0.959891</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.000739</td>\n","      <td>0.954162</td>\n","      <td>0.937589</td>\n","      <td>0.972493</td>\n","      <td>0.982849</td>\n","      <td>0.982849</td>\n","      <td>0.982849</td>\n","      <td>0.917900</td>\n","      <td>0.879713</td>\n","      <td>0.959551</td>\n","      <td>0.917900</td>\n","      <td>0.879713</td>\n","      <td>0.959551</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.000747</td>\n","      <td>0.954973</td>\n","      <td>0.940016</td>\n","      <td>0.971341</td>\n","      <td>0.983222</td>\n","      <td>0.983222</td>\n","      <td>0.983222</td>\n","      <td>0.919307</td>\n","      <td>0.884906</td>\n","      <td>0.956492</td>\n","      <td>0.919307</td>\n","      <td>0.884906</td>\n","      <td>0.956492</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.000836</td>\n","      <td>0.953586</td>\n","      <td>0.936532</td>\n","      <td>0.972512</td>\n","      <td>0.982611</td>\n","      <td>0.982611</td>\n","      <td>0.982611</td>\n","      <td>0.916883</td>\n","      <td>0.877564</td>\n","      <td>0.959891</td>\n","      <td>0.916883</td>\n","      <td>0.877564</td>\n","      <td>0.959891</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.000990</td>\n","      <td>0.955697</td>\n","      <td>0.941583</td>\n","      <td>0.971057</td>\n","      <td>0.983528</td>\n","      <td>0.983528</td>\n","      <td>0.983528</td>\n","      <td>0.920583</td>\n","      <td>0.888152</td>\n","      <td>0.955472</td>\n","      <td>0.920583</td>\n","      <td>0.888152</td>\n","      <td>0.955472</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.000984</td>\n","      <td>0.954771</td>\n","      <td>0.939193</td>\n","      <td>0.971889</td>\n","      <td>0.983120</td>\n","      <td>0.983120</td>\n","      <td>0.983120</td>\n","      <td>0.918963</td>\n","      <td>0.883109</td>\n","      <td>0.957852</td>\n","      <td>0.918963</td>\n","      <td>0.883109</td>\n","      <td>0.957852</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.001009</td>\n","      <td>0.954660</td>\n","      <td>0.939257</td>\n","      <td>0.971568</td>\n","      <td>0.983087</td>\n","      <td>0.983087</td>\n","      <td>0.983087</td>\n","      <td>0.918760</td>\n","      <td>0.883312</td>\n","      <td>0.957172</td>\n","      <td>0.918760</td>\n","      <td>0.883312</td>\n","      <td>0.957172</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.000932</td>\n","      <td>0.954388</td>\n","      <td>0.938200</td>\n","      <td>0.972248</td>\n","      <td>0.982951</td>\n","      <td>0.982951</td>\n","      <td>0.982951</td>\n","      <td>0.918294</td>\n","      <td>0.881012</td>\n","      <td>0.958872</td>\n","      <td>0.918294</td>\n","      <td>0.881012</td>\n","      <td>0.958872</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.000979</td>\n","      <td>0.954564</td>\n","      <td>0.937640</td>\n","      <td>0.973324</td>\n","      <td>0.982985</td>\n","      <td>0.982985</td>\n","      <td>0.982985</td>\n","      <td>0.918629</td>\n","      <td>0.879627</td>\n","      <td>0.961251</td>\n","      <td>0.918629</td>\n","      <td>0.879627</td>\n","      <td>0.961251</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.001022</td>\n","      <td>0.954525</td>\n","      <td>0.937938</td>\n","      <td>0.972871</td>\n","      <td>0.982985</td>\n","      <td>0.982985</td>\n","      <td>0.982985</td>\n","      <td>0.918550</td>\n","      <td>0.880337</td>\n","      <td>0.960231</td>\n","      <td>0.918550</td>\n","      <td>0.880337</td>\n","      <td>0.960231</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.001004</td>\n","      <td>0.954499</td>\n","      <td>0.938138</td>\n","      <td>0.972569</td>\n","      <td>0.982985</td>\n","      <td>0.982985</td>\n","      <td>0.982985</td>\n","      <td>0.918497</td>\n","      <td>0.880811</td>\n","      <td>0.959551</td>\n","      <td>0.918497</td>\n","      <td>0.880811</td>\n","      <td>0.959551</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.001005</td>\n","      <td>0.954401</td>\n","      <td>0.938100</td>\n","      <td>0.972399</td>\n","      <td>0.982951</td>\n","      <td>0.982951</td>\n","      <td>0.982951</td>\n","      <td>0.918321</td>\n","      <td>0.880774</td>\n","      <td>0.959211</td>\n","      <td>0.918321</td>\n","      <td>0.880774</td>\n","      <td>0.959211</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.001002</td>\n","      <td>0.954220</td>\n","      <td>0.937925</td>\n","      <td>0.972210</td>\n","      <td>0.982883</td>\n","      <td>0.982883</td>\n","      <td>0.982883</td>\n","      <td>0.917995</td>\n","      <td>0.880462</td>\n","      <td>0.958872</td>\n","      <td>0.917995</td>\n","      <td>0.880462</td>\n","      <td>0.958872</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.001003</td>\n","      <td>0.954220</td>\n","      <td>0.937925</td>\n","      <td>0.972210</td>\n","      <td>0.982883</td>\n","      <td>0.982883</td>\n","      <td>0.982883</td>\n","      <td>0.917995</td>\n","      <td>0.880462</td>\n","      <td>0.958872</td>\n","      <td>0.917995</td>\n","      <td>0.880462</td>\n","      <td>0.958872</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.9542197462889145, 'eval_macro_precision': 0.9379253210544746, 'eval_macro_recall': 0.9722098882420839, 'eval_micro_f1': 0.9828827604944981, 'eval_micro_precision': 0.9828827604944981, 'eval_micro_recall': 0.9828827604944981, 'eval_macro_f1_no_o': 0.9179954441913438, 'eval_macro_precision_no_o': 0.880461922596754, 'eval_macro_recall_no_o': 0.9588715159755269, 'eval_micro_f1_no_o': 0.9179954441913438, 'eval_micro_precision_no_o': 0.880461922596754, 'eval_micro_recall_no_o': 0.9588715159755269, 'eval_loss': 0.001003265920440541, 'eval_runtime': 16.8881, 'eval_samples_per_second': 13.915, 'eval_steps_per_second': 3.494, 'epoch': 19.0}\n","Accuracy for fold  4 :  0.9179954441913438  --  0.9828827604944981\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"stream","name":"stdout","text":["Testing process has finished.\n","Train run #5\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:09:29, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000000</td>\n","      <td>0.000330</td>\n","      <td>0.929301</td>\n","      <td>0.894272</td>\n","      <td>0.974294</td>\n","      <td>0.971947</td>\n","      <td>0.971947</td>\n","      <td>0.971947</td>\n","      <td>0.874392</td>\n","      <td>0.791139</td>\n","      <td>0.977226</td>\n","      <td>0.874392</td>\n","      <td>0.791139</td>\n","      <td>0.977226</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000000</td>\n","      <td>0.000480</td>\n","      <td>0.946972</td>\n","      <td>0.924478</td>\n","      <td>0.972929</td>\n","      <td>0.979826</td>\n","      <td>0.979826</td>\n","      <td>0.979826</td>\n","      <td>0.905233</td>\n","      <td>0.852977</td>\n","      <td>0.964310</td>\n","      <td>0.905233</td>\n","      <td>0.852977</td>\n","      <td>0.964310</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.000619</td>\n","      <td>0.948808</td>\n","      <td>0.929074</td>\n","      <td>0.971135</td>\n","      <td>0.980675</td>\n","      <td>0.980675</td>\n","      <td>0.980675</td>\n","      <td>0.908418</td>\n","      <td>0.862733</td>\n","      <td>0.959211</td>\n","      <td>0.908418</td>\n","      <td>0.862733</td>\n","      <td>0.959211</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.000737</td>\n","      <td>0.951452</td>\n","      <td>0.932728</td>\n","      <td>0.972475</td>\n","      <td>0.981728</td>\n","      <td>0.981728</td>\n","      <td>0.981728</td>\n","      <td>0.913114</td>\n","      <td>0.869846</td>\n","      <td>0.960911</td>\n","      <td>0.913114</td>\n","      <td>0.869846</td>\n","      <td>0.960911</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.000748</td>\n","      <td>0.953348</td>\n","      <td>0.936025</td>\n","      <td>0.972607</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.916464</td>\n","      <td>0.876513</td>\n","      <td>0.960231</td>\n","      <td>0.916464</td>\n","      <td>0.876513</td>\n","      <td>0.960231</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.000845</td>\n","      <td>0.954278</td>\n","      <td>0.938263</td>\n","      <td>0.971927</td>\n","      <td>0.982917</td>\n","      <td>0.982917</td>\n","      <td>0.982917</td>\n","      <td>0.918092</td>\n","      <td>0.881213</td>\n","      <td>0.958192</td>\n","      <td>0.918092</td>\n","      <td>0.881213</td>\n","      <td>0.958192</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.000972</td>\n","      <td>0.953941</td>\n","      <td>0.937713</td>\n","      <td>0.971851</td>\n","      <td>0.982781</td>\n","      <td>0.982781</td>\n","      <td>0.982781</td>\n","      <td>0.917494</td>\n","      <td>0.880112</td>\n","      <td>0.958192</td>\n","      <td>0.917494</td>\n","      <td>0.880112</td>\n","      <td>0.958192</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.000963</td>\n","      <td>0.954051</td>\n","      <td>0.937651</td>\n","      <td>0.972172</td>\n","      <td>0.982815</td>\n","      <td>0.982815</td>\n","      <td>0.982815</td>\n","      <td>0.917697</td>\n","      <td>0.879913</td>\n","      <td>0.958872</td>\n","      <td>0.917697</td>\n","      <td>0.879913</td>\n","      <td>0.958872</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.001189</td>\n","      <td>0.953817</td>\n","      <td>0.937875</td>\n","      <td>0.971379</td>\n","      <td>0.982747</td>\n","      <td>0.982747</td>\n","      <td>0.982747</td>\n","      <td>0.917264</td>\n","      <td>0.880550</td>\n","      <td>0.957172</td>\n","      <td>0.917264</td>\n","      <td>0.880550</td>\n","      <td>0.957172</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.001368</td>\n","      <td>0.954458</td>\n","      <td>0.940036</td>\n","      <td>0.970189</td>\n","      <td>0.983053</td>\n","      <td>0.983053</td>\n","      <td>0.983053</td>\n","      <td>0.918371</td>\n","      <td>0.885210</td>\n","      <td>0.954113</td>\n","      <td>0.918371</td>\n","      <td>0.885210</td>\n","      <td>0.954113</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.001165</td>\n","      <td>0.953462</td>\n","      <td>0.936691</td>\n","      <td>0.972040</td>\n","      <td>0.982577</td>\n","      <td>0.982577</td>\n","      <td>0.982577</td>\n","      <td>0.916653</td>\n","      <td>0.877996</td>\n","      <td>0.958872</td>\n","      <td>0.916653</td>\n","      <td>0.877996</td>\n","      <td>0.958872</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.001092</td>\n","      <td>0.953194</td>\n","      <td>0.935656</td>\n","      <td>0.972720</td>\n","      <td>0.982441</td>\n","      <td>0.982441</td>\n","      <td>0.982441</td>\n","      <td>0.916194</td>\n","      <td>0.875736</td>\n","      <td>0.960571</td>\n","      <td>0.916194</td>\n","      <td>0.875736</td>\n","      <td>0.960571</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.001452</td>\n","      <td>0.953803</td>\n","      <td>0.937975</td>\n","      <td>0.971228</td>\n","      <td>0.982747</td>\n","      <td>0.982747</td>\n","      <td>0.982747</td>\n","      <td>0.917237</td>\n","      <td>0.880788</td>\n","      <td>0.956832</td>\n","      <td>0.917237</td>\n","      <td>0.880788</td>\n","      <td>0.956832</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.001411</td>\n","      <td>0.953914</td>\n","      <td>0.937912</td>\n","      <td>0.971549</td>\n","      <td>0.982781</td>\n","      <td>0.982781</td>\n","      <td>0.982781</td>\n","      <td>0.917440</td>\n","      <td>0.880588</td>\n","      <td>0.957512</td>\n","      <td>0.917440</td>\n","      <td>0.880588</td>\n","      <td>0.957512</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.001105</td>\n","      <td>0.952788</td>\n","      <td>0.934881</td>\n","      <td>0.972777</td>\n","      <td>0.982271</td>\n","      <td>0.982271</td>\n","      <td>0.982271</td>\n","      <td>0.915479</td>\n","      <td>0.874150</td>\n","      <td>0.960911</td>\n","      <td>0.915479</td>\n","      <td>0.874150</td>\n","      <td>0.960911</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.001241</td>\n","      <td>0.953613</td>\n","      <td>0.936336</td>\n","      <td>0.972814</td>\n","      <td>0.982611</td>\n","      <td>0.982611</td>\n","      <td>0.982611</td>\n","      <td>0.916937</td>\n","      <td>0.877095</td>\n","      <td>0.960571</td>\n","      <td>0.916937</td>\n","      <td>0.877095</td>\n","      <td>0.960571</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.001234</td>\n","      <td>0.953361</td>\n","      <td>0.935927</td>\n","      <td>0.972758</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.916491</td>\n","      <td>0.876279</td>\n","      <td>0.960571</td>\n","      <td>0.916491</td>\n","      <td>0.876279</td>\n","      <td>0.960571</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.001230</td>\n","      <td>0.953361</td>\n","      <td>0.935927</td>\n","      <td>0.972758</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.916491</td>\n","      <td>0.876279</td>\n","      <td>0.960571</td>\n","      <td>0.916491</td>\n","      <td>0.876279</td>\n","      <td>0.960571</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.001238</td>\n","      <td>0.953361</td>\n","      <td>0.935927</td>\n","      <td>0.972758</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.982509</td>\n","      <td>0.916491</td>\n","      <td>0.876279</td>\n","      <td>0.960571</td>\n","      <td>0.916491</td>\n","      <td>0.876279</td>\n","      <td>0.960571</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.9533612985019884, 'eval_macro_precision': 0.935927398646641, 'eval_macro_recall': 0.9727577862984418, 'eval_micro_f1': 0.9825091699497351, 'eval_micro_precision': 0.9825091699497351, 'eval_micro_recall': 0.9825091699497351, 'eval_macro_f1_no_o': 0.9164910004864603, 'eval_macro_precision_no_o': 0.8762790697674419, 'eval_macro_recall_no_o': 0.9605710401087696, 'eval_micro_f1_no_o': 0.9164910004864603, 'eval_micro_precision_no_o': 0.8762790697674419, 'eval_micro_recall_no_o': 0.9605710401087696, 'eval_loss': 0.001237668144521315, 'eval_runtime': 16.0065, 'eval_samples_per_second': 14.682, 'eval_steps_per_second': 3.686, 'epoch': 19.0}\n","Accuracy for fold  5 :  0.9164910004864603  --  0.9825091699497351\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"stream","name":"stdout","text":["Testing process has finished.\n","Train run #6\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:10:53, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000000</td>\n","      <td>0.000552</td>\n","      <td>0.948043</td>\n","      <td>0.925495</td>\n","      <td>0.974062</td>\n","      <td>0.980234</td>\n","      <td>0.980234</td>\n","      <td>0.980234</td>\n","      <td>0.907147</td>\n","      <td>0.854781</td>\n","      <td>0.966349</td>\n","      <td>0.907147</td>\n","      <td>0.854781</td>\n","      <td>0.966349</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000000</td>\n","      <td>0.000784</td>\n","      <td>0.952191</td>\n","      <td>0.934747</td>\n","      <td>0.971606</td>\n","      <td>0.982068</td>\n","      <td>0.982068</td>\n","      <td>0.982068</td>\n","      <td>0.914397</td>\n","      <td>0.874148</td>\n","      <td>0.958532</td>\n","      <td>0.914397</td>\n","      <td>0.874148</td>\n","      <td>0.958532</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.000749</td>\n","      <td>0.940614</td>\n","      <td>0.909536</td>\n","      <td>0.978956</td>\n","      <td>0.976803</td>\n","      <td>0.976803</td>\n","      <td>0.976803</td>\n","      <td>0.894256</td>\n","      <td>0.821154</td>\n","      <td>0.981645</td>\n","      <td>0.894256</td>\n","      <td>0.821154</td>\n","      <td>0.981645</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.000849</td>\n","      <td>0.953207</td>\n","      <td>0.935558</td>\n","      <td>0.972871</td>\n","      <td>0.982441</td>\n","      <td>0.982441</td>\n","      <td>0.982441</td>\n","      <td>0.916221</td>\n","      <td>0.875503</td>\n","      <td>0.960911</td>\n","      <td>0.916221</td>\n","      <td>0.875503</td>\n","      <td>0.960911</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.001056</td>\n","      <td>0.954141</td>\n","      <td>0.938527</td>\n","      <td>0.971303</td>\n","      <td>0.982883</td>\n","      <td>0.982883</td>\n","      <td>0.982883</td>\n","      <td>0.917835</td>\n","      <td>0.881892</td>\n","      <td>0.956832</td>\n","      <td>0.917835</td>\n","      <td>0.881892</td>\n","      <td>0.956832</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.001033</td>\n","      <td>0.954407</td>\n","      <td>0.938841</td>\n","      <td>0.971511</td>\n","      <td>0.982985</td>\n","      <td>0.982985</td>\n","      <td>0.982985</td>\n","      <td>0.918311</td>\n","      <td>0.882482</td>\n","      <td>0.957172</td>\n","      <td>0.918311</td>\n","      <td>0.882482</td>\n","      <td>0.957172</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.001151</td>\n","      <td>0.953861</td>\n","      <td>0.938314</td>\n","      <td>0.970945</td>\n","      <td>0.982781</td>\n","      <td>0.982781</td>\n","      <td>0.982781</td>\n","      <td>0.917332</td>\n","      <td>0.881542</td>\n","      <td>0.956152</td>\n","      <td>0.917332</td>\n","      <td>0.881542</td>\n","      <td>0.956152</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.001065</td>\n","      <td>0.949599</td>\n","      <td>0.929302</td>\n","      <td>0.972645</td>\n","      <td>0.980947</td>\n","      <td>0.980947</td>\n","      <td>0.980947</td>\n","      <td>0.909851</td>\n","      <td>0.862847</td>\n","      <td>0.962271</td>\n","      <td>0.909851</td>\n","      <td>0.862847</td>\n","      <td>0.962271</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.001174</td>\n","      <td>0.948967</td>\n","      <td>0.928071</td>\n","      <td>0.972797</td>\n","      <td>0.980675</td>\n","      <td>0.980675</td>\n","      <td>0.980675</td>\n","      <td>0.908741</td>\n","      <td>0.860310</td>\n","      <td>0.962950</td>\n","      <td>0.908741</td>\n","      <td>0.860310</td>\n","      <td>0.962950</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.001505</td>\n","      <td>0.951577</td>\n","      <td>0.933282</td>\n","      <td>0.972059</td>\n","      <td>0.981796</td>\n","      <td>0.981796</td>\n","      <td>0.981796</td>\n","      <td>0.913325</td>\n","      <td>0.871067</td>\n","      <td>0.959891</td>\n","      <td>0.913325</td>\n","      <td>0.871067</td>\n","      <td>0.959891</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.001407</td>\n","      <td>0.951646</td>\n","      <td>0.932806</td>\n","      <td>0.972815</td>\n","      <td>0.981796</td>\n","      <td>0.981796</td>\n","      <td>0.981796</td>\n","      <td>0.913465</td>\n","      <td>0.869926</td>\n","      <td>0.961591</td>\n","      <td>0.913465</td>\n","      <td>0.869926</td>\n","      <td>0.961591</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.000695</td>\n","      <td>0.946484</td>\n","      <td>0.922513</td>\n","      <td>0.974440</td>\n","      <td>0.979554</td>\n","      <td>0.979554</td>\n","      <td>0.979554</td>\n","      <td>0.904414</td>\n","      <td>0.848629</td>\n","      <td>0.968049</td>\n","      <td>0.904414</td>\n","      <td>0.848629</td>\n","      <td>0.968049</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.001299</td>\n","      <td>0.951646</td>\n","      <td>0.932806</td>\n","      <td>0.972815</td>\n","      <td>0.981796</td>\n","      <td>0.981796</td>\n","      <td>0.981796</td>\n","      <td>0.913465</td>\n","      <td>0.869926</td>\n","      <td>0.961591</td>\n","      <td>0.913465</td>\n","      <td>0.869926</td>\n","      <td>0.961591</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.001292</td>\n","      <td>0.951119</td>\n","      <td>0.932193</td>\n","      <td>0.972399</td>\n","      <td>0.981592</td>\n","      <td>0.981592</td>\n","      <td>0.981592</td>\n","      <td>0.912524</td>\n","      <td>0.868777</td>\n","      <td>0.960911</td>\n","      <td>0.912524</td>\n","      <td>0.868777</td>\n","      <td>0.960911</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.001339</td>\n","      <td>0.951202</td>\n","      <td>0.933031</td>\n","      <td>0.971531</td>\n","      <td>0.981660</td>\n","      <td>0.981660</td>\n","      <td>0.981660</td>\n","      <td>0.912650</td>\n","      <td>0.870679</td>\n","      <td>0.958872</td>\n","      <td>0.912650</td>\n","      <td>0.870679</td>\n","      <td>0.958872</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.001258</td>\n","      <td>0.950828</td>\n","      <td>0.931378</td>\n","      <td>0.972777</td>\n","      <td>0.981456</td>\n","      <td>0.981456</td>\n","      <td>0.981456</td>\n","      <td>0.912021</td>\n","      <td>0.867034</td>\n","      <td>0.961931</td>\n","      <td>0.912021</td>\n","      <td>0.867034</td>\n","      <td>0.961931</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.001223</td>\n","      <td>0.950579</td>\n","      <td>0.930980</td>\n","      <td>0.972721</td>\n","      <td>0.981354</td>\n","      <td>0.981354</td>\n","      <td>0.981354</td>\n","      <td>0.911580</td>\n","      <td>0.866238</td>\n","      <td>0.961931</td>\n","      <td>0.911580</td>\n","      <td>0.866238</td>\n","      <td>0.961931</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.001250</td>\n","      <td>0.950828</td>\n","      <td>0.931378</td>\n","      <td>0.972777</td>\n","      <td>0.981456</td>\n","      <td>0.981456</td>\n","      <td>0.981456</td>\n","      <td>0.912021</td>\n","      <td>0.867034</td>\n","      <td>0.961931</td>\n","      <td>0.912021</td>\n","      <td>0.867034</td>\n","      <td>0.961931</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.001252</td>\n","      <td>0.950828</td>\n","      <td>0.931378</td>\n","      <td>0.972777</td>\n","      <td>0.981456</td>\n","      <td>0.981456</td>\n","      <td>0.981456</td>\n","      <td>0.912021</td>\n","      <td>0.867034</td>\n","      <td>0.961931</td>\n","      <td>0.912021</td>\n","      <td>0.867034</td>\n","      <td>0.961931</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.9508282769908233, 'eval_macro_precision': 0.93137811942959, 'eval_macro_recall': 0.9727772684292877, 'eval_micro_f1': 0.9814563238690396, 'eval_micro_precision': 0.9814563238690396, 'eval_micro_recall': 0.9814563238690396, 'eval_macro_f1_no_o': 0.912020625201418, 'eval_macro_precision_no_o': 0.8670343137254902, 'eval_macro_recall_no_o': 0.9619306594153637, 'eval_micro_f1_no_o': 0.912020625201418, 'eval_micro_precision_no_o': 0.8670343137254902, 'eval_micro_recall_no_o': 0.9619306594153637, 'eval_loss': 0.0012517669018561072, 'eval_runtime': 17.064, 'eval_samples_per_second': 13.772, 'eval_steps_per_second': 3.458, 'epoch': 19.0}\n","Accuracy for fold  6 :  0.912020625201418  --  0.9814563238690396\n","--------------------------------\n","Testing process has finished.\n","Train run #7\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:16:30, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000000</td>\n","      <td>0.000638</td>\n","      <td>0.950030</td>\n","      <td>0.933276</td>\n","      <td>0.968604</td>\n","      <td>0.981287</td>\n","      <td>0.981287</td>\n","      <td>0.981287</td>\n","      <td>0.910508</td>\n","      <td>0.871851</td>\n","      <td>0.952753</td>\n","      <td>0.910508</td>\n","      <td>0.871851</td>\n","      <td>0.952753</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000000</td>\n","      <td>0.000744</td>\n","      <td>0.949973</td>\n","      <td>0.929553</td>\n","      <td>0.973174</td>\n","      <td>0.981083</td>\n","      <td>0.981083</td>\n","      <td>0.981083</td>\n","      <td>0.910522</td>\n","      <td>0.863235</td>\n","      <td>0.963290</td>\n","      <td>0.910522</td>\n","      <td>0.863235</td>\n","      <td>0.963290</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.000710</td>\n","      <td>0.919956</td>\n","      <td>0.883352</td>\n","      <td>0.968007</td>\n","      <td>0.967973</td>\n","      <td>0.967973</td>\n","      <td>0.967973</td>\n","      <td>0.857961</td>\n","      <td>0.770354</td>\n","      <td>0.968049</td>\n","      <td>0.857961</td>\n","      <td>0.770354</td>\n","      <td>0.968049</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.000721</td>\n","      <td>0.935092</td>\n","      <td>0.907674</td>\n","      <td>0.968078</td>\n","      <td>0.974902</td>\n","      <td>0.974902</td>\n","      <td>0.974902</td>\n","      <td>0.884260</td>\n","      <td>0.819924</td>\n","      <td>0.959551</td>\n","      <td>0.884260</td>\n","      <td>0.819924</td>\n","      <td>0.959551</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.000772</td>\n","      <td>0.937677</td>\n","      <td>0.912745</td>\n","      <td>0.967077</td>\n","      <td>0.976090</td>\n","      <td>0.976090</td>\n","      <td>0.976090</td>\n","      <td>0.888748</td>\n","      <td>0.830478</td>\n","      <td>0.955812</td>\n","      <td>0.888748</td>\n","      <td>0.830478</td>\n","      <td>0.955812</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.000847</td>\n","      <td>0.940620</td>\n","      <td>0.919607</td>\n","      <td>0.964678</td>\n","      <td>0.977483</td>\n","      <td>0.977483</td>\n","      <td>0.977483</td>\n","      <td>0.893835</td>\n","      <td>0.844989</td>\n","      <td>0.948674</td>\n","      <td>0.893835</td>\n","      <td>0.844989</td>\n","      <td>0.948674</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.000665</td>\n","      <td>0.937183</td>\n","      <td>0.909722</td>\n","      <td>0.970193</td>\n","      <td>0.975717</td>\n","      <td>0.975717</td>\n","      <td>0.975717</td>\n","      <td>0.887984</td>\n","      <td>0.823598</td>\n","      <td>0.963290</td>\n","      <td>0.887984</td>\n","      <td>0.823598</td>\n","      <td>0.963290</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.000977</td>\n","      <td>0.943945</td>\n","      <td>0.923284</td>\n","      <td>0.967510</td>\n","      <td>0.978773</td>\n","      <td>0.978773</td>\n","      <td>0.978773</td>\n","      <td>0.899759</td>\n","      <td>0.851807</td>\n","      <td>0.953433</td>\n","      <td>0.899759</td>\n","      <td>0.851807</td>\n","      <td>0.953433</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.000656</td>\n","      <td>0.932474</td>\n","      <td>0.899838</td>\n","      <td>0.973462</td>\n","      <td>0.973441</td>\n","      <td>0.973441</td>\n","      <td>0.973441</td>\n","      <td>0.879877</td>\n","      <td>0.802691</td>\n","      <td>0.973487</td>\n","      <td>0.879877</td>\n","      <td>0.802691</td>\n","      <td>0.973487</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.001113</td>\n","      <td>0.944634</td>\n","      <td>0.924720</td>\n","      <td>0.967226</td>\n","      <td>0.979079</td>\n","      <td>0.979079</td>\n","      <td>0.979079</td>\n","      <td>0.900965</td>\n","      <td>0.854790</td>\n","      <td>0.952413</td>\n","      <td>0.900965</td>\n","      <td>0.854790</td>\n","      <td>0.952413</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.001109</td>\n","      <td>0.945001</td>\n","      <td>0.923828</td>\n","      <td>0.969228</td>\n","      <td>0.979147</td>\n","      <td>0.979147</td>\n","      <td>0.979147</td>\n","      <td>0.901666</td>\n","      <td>0.852514</td>\n","      <td>0.956832</td>\n","      <td>0.901666</td>\n","      <td>0.852514</td>\n","      <td>0.956832</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.001110</td>\n","      <td>0.944514</td>\n","      <td>0.923621</td>\n","      <td>0.968378</td>\n","      <td>0.978977</td>\n","      <td>0.978977</td>\n","      <td>0.978977</td>\n","      <td>0.900785</td>\n","      <td>0.852290</td>\n","      <td>0.955133</td>\n","      <td>0.900785</td>\n","      <td>0.852290</td>\n","      <td>0.955133</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.001194</td>\n","      <td>0.943929</td>\n","      <td>0.923372</td>\n","      <td>0.967359</td>\n","      <td>0.978773</td>\n","      <td>0.978773</td>\n","      <td>0.978773</td>\n","      <td>0.899727</td>\n","      <td>0.852021</td>\n","      <td>0.953093</td>\n","      <td>0.899727</td>\n","      <td>0.852021</td>\n","      <td>0.953093</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.001083</td>\n","      <td>0.942897</td>\n","      <td>0.919861</td>\n","      <td>0.969625</td>\n","      <td>0.978230</td>\n","      <td>0.978230</td>\n","      <td>0.978230</td>\n","      <td>0.897979</td>\n","      <td>0.844358</td>\n","      <td>0.958872</td>\n","      <td>0.897979</td>\n","      <td>0.844358</td>\n","      <td>0.958872</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.001142</td>\n","      <td>0.943044</td>\n","      <td>0.920750</td>\n","      <td>0.968775</td>\n","      <td>0.978332</td>\n","      <td>0.978332</td>\n","      <td>0.978332</td>\n","      <td>0.898213</td>\n","      <td>0.846362</td>\n","      <td>0.956832</td>\n","      <td>0.898213</td>\n","      <td>0.846362</td>\n","      <td>0.956832</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.001183</td>\n","      <td>0.943566</td>\n","      <td>0.921901</td>\n","      <td>0.968454</td>\n","      <td>0.978569</td>\n","      <td>0.978569</td>\n","      <td>0.978569</td>\n","      <td>0.899121</td>\n","      <td>0.848778</td>\n","      <td>0.955812</td>\n","      <td>0.899121</td>\n","      <td>0.848778</td>\n","      <td>0.955812</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.001229</td>\n","      <td>0.944203</td>\n","      <td>0.923017</td>\n","      <td>0.968454</td>\n","      <td>0.978841</td>\n","      <td>0.978841</td>\n","      <td>0.978841</td>\n","      <td>0.900240</td>\n","      <td>0.851045</td>\n","      <td>0.955472</td>\n","      <td>0.900240</td>\n","      <td>0.851045</td>\n","      <td>0.955472</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.001242</td>\n","      <td>0.944074</td>\n","      <td>0.923150</td>\n","      <td>0.967982</td>\n","      <td>0.978807</td>\n","      <td>0.978807</td>\n","      <td>0.978807</td>\n","      <td>0.900000</td>\n","      <td>0.851425</td>\n","      <td>0.954453</td>\n","      <td>0.900000</td>\n","      <td>0.851425</td>\n","      <td>0.954453</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.001248</td>\n","      <td>0.944156</td>\n","      <td>0.923279</td>\n","      <td>0.968001</td>\n","      <td>0.978841</td>\n","      <td>0.978841</td>\n","      <td>0.978841</td>\n","      <td>0.900144</td>\n","      <td>0.851683</td>\n","      <td>0.954453</td>\n","      <td>0.900144</td>\n","      <td>0.851683</td>\n","      <td>0.954453</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.944155585318698, 'eval_macro_precision': 0.9232792387882705, 'eval_macro_recall': 0.9680006578008735, 'eval_micro_f1': 0.9788411900556989, 'eval_micro_precision': 0.9788411900556989, 'eval_micro_recall': 0.9788411900556989, 'eval_macro_f1_no_o': 0.9001442538868407, 'eval_macro_precision_no_o': 0.8516833484986351, 'eval_macro_recall_no_o': 0.9544527532290958, 'eval_micro_f1_no_o': 0.9001442538868407, 'eval_micro_precision_no_o': 0.8516833484986351, 'eval_micro_recall_no_o': 0.9544527532290958, 'eval_loss': 0.0012476394446297868, 'eval_runtime': 20.0081, 'eval_samples_per_second': 11.745, 'eval_steps_per_second': 2.949, 'epoch': 19.0}\n","Accuracy for fold  7 :  0.9001442538868407  --  0.9788411900556989\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"stream","name":"stdout","text":["Testing process has finished.\n","Train run #8\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:15:50, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000000</td>\n","      <td>0.000682</td>\n","      <td>0.943580</td>\n","      <td>0.921258</td>\n","      <td>0.969342</td>\n","      <td>0.978536</td>\n","      <td>0.978536</td>\n","      <td>0.978536</td>\n","      <td>0.899170</td>\n","      <td>0.847264</td>\n","      <td>0.957852</td>\n","      <td>0.899170</td>\n","      <td>0.847264</td>\n","      <td>0.957852</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000000</td>\n","      <td>0.000696</td>\n","      <td>0.945114</td>\n","      <td>0.923781</td>\n","      <td>0.969549</td>\n","      <td>0.979181</td>\n","      <td>0.979181</td>\n","      <td>0.979181</td>\n","      <td>0.901873</td>\n","      <td>0.852345</td>\n","      <td>0.957512</td>\n","      <td>0.901873</td>\n","      <td>0.852345</td>\n","      <td>0.957512</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.000883</td>\n","      <td>0.949293</td>\n","      <td>0.931962</td>\n","      <td>0.968585</td>\n","      <td>0.980981</td>\n","      <td>0.980981</td>\n","      <td>0.980981</td>\n","      <td>0.909209</td>\n","      <td>0.869188</td>\n","      <td>0.953093</td>\n","      <td>0.909209</td>\n","      <td>0.869188</td>\n","      <td>0.953093</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.000985</td>\n","      <td>0.950270</td>\n","      <td>0.932354</td>\n","      <td>0.970284</td>\n","      <td>0.981320</td>\n","      <td>0.981320</td>\n","      <td>0.981320</td>\n","      <td>0.910974</td>\n","      <td>0.869592</td>\n","      <td>0.956492</td>\n","      <td>0.910974</td>\n","      <td>0.869592</td>\n","      <td>0.956492</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.000852</td>\n","      <td>0.936687</td>\n","      <td>0.907286</td>\n","      <td>0.972573</td>\n","      <td>0.975377</td>\n","      <td>0.975377</td>\n","      <td>0.975377</td>\n","      <td>0.887195</td>\n","      <td>0.818077</td>\n","      <td>0.969069</td>\n","      <td>0.887195</td>\n","      <td>0.818077</td>\n","      <td>0.969069</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.001430</td>\n","      <td>0.951243</td>\n","      <td>0.935617</td>\n","      <td>0.968433</td>\n","      <td>0.981796</td>\n","      <td>0.981796</td>\n","      <td>0.981796</td>\n","      <td>0.912647</td>\n","      <td>0.876644</td>\n","      <td>0.951734</td>\n","      <td>0.912647</td>\n","      <td>0.876644</td>\n","      <td>0.951734</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.001333</td>\n","      <td>0.949272</td>\n","      <td>0.930747</td>\n","      <td>0.970058</td>\n","      <td>0.980913</td>\n","      <td>0.980913</td>\n","      <td>0.980913</td>\n","      <td>0.909208</td>\n","      <td>0.866379</td>\n","      <td>0.956492</td>\n","      <td>0.909208</td>\n","      <td>0.866379</td>\n","      <td>0.956492</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.001656</td>\n","      <td>0.950501</td>\n","      <td>0.935013</td>\n","      <td>0.967527</td>\n","      <td>0.981524</td>\n","      <td>0.981524</td>\n","      <td>0.981524</td>\n","      <td>0.911314</td>\n","      <td>0.875627</td>\n","      <td>0.950034</td>\n","      <td>0.911314</td>\n","      <td>0.875627</td>\n","      <td>0.950034</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.001831</td>\n","      <td>0.950068</td>\n","      <td>0.934427</td>\n","      <td>0.967282</td>\n","      <td>0.981354</td>\n","      <td>0.981354</td>\n","      <td>0.981354</td>\n","      <td>0.910543</td>\n","      <td>0.874491</td>\n","      <td>0.949694</td>\n","      <td>0.910543</td>\n","      <td>0.874491</td>\n","      <td>0.949694</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.001937</td>\n","      <td>0.949805</td>\n","      <td>0.933392</td>\n","      <td>0.967962</td>\n","      <td>0.981219</td>\n","      <td>0.981219</td>\n","      <td>0.981219</td>\n","      <td>0.910096</td>\n","      <td>0.872234</td>\n","      <td>0.951394</td>\n","      <td>0.910096</td>\n","      <td>0.872234</td>\n","      <td>0.951394</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.002086</td>\n","      <td>0.950474</td>\n","      <td>0.934483</td>\n","      <td>0.968112</td>\n","      <td>0.981490</td>\n","      <td>0.981490</td>\n","      <td>0.981490</td>\n","      <td>0.911281</td>\n","      <td>0.874414</td>\n","      <td>0.951394</td>\n","      <td>0.911281</td>\n","      <td>0.874414</td>\n","      <td>0.951394</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.002105</td>\n","      <td>0.949946</td>\n","      <td>0.933140</td>\n","      <td>0.968585</td>\n","      <td>0.981253</td>\n","      <td>0.981253</td>\n","      <td>0.981253</td>\n","      <td>0.910361</td>\n","      <td>0.871580</td>\n","      <td>0.952753</td>\n","      <td>0.910361</td>\n","      <td>0.871580</td>\n","      <td>0.952753</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.002096</td>\n","      <td>0.948917</td>\n","      <td>0.931709</td>\n","      <td>0.968056</td>\n","      <td>0.980845</td>\n","      <td>0.980845</td>\n","      <td>0.980845</td>\n","      <td>0.908531</td>\n","      <td>0.868797</td>\n","      <td>0.952073</td>\n","      <td>0.908531</td>\n","      <td>0.868797</td>\n","      <td>0.952073</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.002193</td>\n","      <td>0.949946</td>\n","      <td>0.933140</td>\n","      <td>0.968585</td>\n","      <td>0.981253</td>\n","      <td>0.981253</td>\n","      <td>0.981253</td>\n","      <td>0.910361</td>\n","      <td>0.871580</td>\n","      <td>0.952753</td>\n","      <td>0.910361</td>\n","      <td>0.871580</td>\n","      <td>0.952753</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.002157</td>\n","      <td>0.949960</td>\n","      <td>0.933043</td>\n","      <td>0.968736</td>\n","      <td>0.981253</td>\n","      <td>0.981253</td>\n","      <td>0.981253</td>\n","      <td>0.910390</td>\n","      <td>0.871349</td>\n","      <td>0.953093</td>\n","      <td>0.910390</td>\n","      <td>0.871349</td>\n","      <td>0.953093</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.002174</td>\n","      <td>0.949779</td>\n","      <td>0.932869</td>\n","      <td>0.968547</td>\n","      <td>0.981185</td>\n","      <td>0.981185</td>\n","      <td>0.981185</td>\n","      <td>0.910065</td>\n","      <td>0.871038</td>\n","      <td>0.952753</td>\n","      <td>0.910065</td>\n","      <td>0.871038</td>\n","      <td>0.952753</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.002105</td>\n","      <td>0.949406</td>\n","      <td>0.931906</td>\n","      <td>0.968906</td>\n","      <td>0.981015</td>\n","      <td>0.981015</td>\n","      <td>0.981015</td>\n","      <td>0.909415</td>\n","      <td>0.869000</td>\n","      <td>0.953773</td>\n","      <td>0.909415</td>\n","      <td>0.869000</td>\n","      <td>0.953773</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.002051</td>\n","      <td>0.949268</td>\n","      <td>0.931447</td>\n","      <td>0.969170</td>\n","      <td>0.980947</td>\n","      <td>0.980947</td>\n","      <td>0.980947</td>\n","      <td>0.909179</td>\n","      <td>0.868006</td>\n","      <td>0.954453</td>\n","      <td>0.909179</td>\n","      <td>0.868006</td>\n","      <td>0.954453</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.002051</td>\n","      <td>0.949351</td>\n","      <td>0.931581</td>\n","      <td>0.969189</td>\n","      <td>0.980981</td>\n","      <td>0.980981</td>\n","      <td>0.980981</td>\n","      <td>0.909326</td>\n","      <td>0.868275</td>\n","      <td>0.954453</td>\n","      <td>0.909326</td>\n","      <td>0.868275</td>\n","      <td>0.954453</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5433\n","  Num Epochs = 19\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 25802\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_macro_f1': 0.9493513289931326, 'eval_macro_precision': 0.931581015049702, 'eval_macro_recall': 0.9691892473412855, 'eval_micro_f1': 0.9809808449938867, 'eval_micro_precision': 0.9809808449938867, 'eval_micro_recall': 0.9809808449938867, 'eval_macro_f1_no_o': 0.9093264248704662, 'eval_macro_precision_no_o': 0.8682745825602969, 'eval_macro_recall_no_o': 0.9544527532290958, 'eval_micro_f1_no_o': 0.9093264248704662, 'eval_micro_precision_no_o': 0.8682745825602969, 'eval_micro_recall_no_o': 0.9544527532290958, 'eval_loss': 0.0020507536155610425, 'eval_runtime': 19.524, 'eval_samples_per_second': 12.036, 'eval_steps_per_second': 3.022, 'epoch': 19.0}\n","Accuracy for fold  8 :  0.9093264248704662  --  0.9809808449938867\n","--------------------------------\n","Testing process has finished.\n","Train run #9\n","--------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='25802' max='25802' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25802/25802 1:16:30, Epoch 19/19]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Macro F1</th>\n","      <th>Macro Precision</th>\n","      <th>Macro Recall</th>\n","      <th>Micro F1</th>\n","      <th>Micro Precision</th>\n","      <th>Micro Recall</th>\n","      <th>Macro F1 No O</th>\n","      <th>Macro Precision No O</th>\n","      <th>Macro Recall No O</th>\n","      <th>Micro F1 No O</th>\n","      <th>Micro Precision No O</th>\n","      <th>Micro Recall No O</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1358</td>\n","      <td>0.000000</td>\n","      <td>0.001235</td>\n","      <td>0.948104</td>\n","      <td>0.944654</td>\n","      <td>0.951625</td>\n","      <td>0.981185</td>\n","      <td>0.981185</td>\n","      <td>0.981185</td>\n","      <td>0.906671</td>\n","      <td>0.898798</td>\n","      <td>0.914684</td>\n","      <td>0.906671</td>\n","      <td>0.898798</td>\n","      <td>0.914684</td>\n","    </tr>\n","    <tr>\n","      <td>2716</td>\n","      <td>0.000000</td>\n","      <td>0.001585</td>\n","      <td>0.949126</td>\n","      <td>0.936604</td>\n","      <td>0.962636</td>\n","      <td>0.981151</td>\n","      <td>0.981151</td>\n","      <td>0.981151</td>\n","      <td>0.908762</td>\n","      <td>0.879975</td>\n","      <td>0.939497</td>\n","      <td>0.908762</td>\n","      <td>0.879975</td>\n","      <td>0.939497</td>\n","    </tr>\n","    <tr>\n","      <td>4074</td>\n","      <td>0.000000</td>\n","      <td>0.001182</td>\n","      <td>0.947527</td>\n","      <td>0.928646</td>\n","      <td>0.968774</td>\n","      <td>0.980234</td>\n","      <td>0.980234</td>\n","      <td>0.980234</td>\n","      <td>0.906099</td>\n","      <td>0.862408</td>\n","      <td>0.954453</td>\n","      <td>0.906099</td>\n","      <td>0.862408</td>\n","      <td>0.954453</td>\n","    </tr>\n","    <tr>\n","      <td>5432</td>\n","      <td>0.000000</td>\n","      <td>0.001503</td>\n","      <td>0.949854</td>\n","      <td>0.935180</td>\n","      <td>0.965903</td>\n","      <td>0.981320</td>\n","      <td>0.981320</td>\n","      <td>0.981320</td>\n","      <td>0.910131</td>\n","      <td>0.876337</td>\n","      <td>0.946635</td>\n","      <td>0.910131</td>\n","      <td>0.876337</td>\n","      <td>0.946635</td>\n","    </tr>\n","    <tr>\n","      <td>6790</td>\n","      <td>0.000000</td>\n","      <td>0.001409</td>\n","      <td>0.947173</td>\n","      <td>0.928896</td>\n","      <td>0.967660</td>\n","      <td>0.980132</td>\n","      <td>0.980132</td>\n","      <td>0.980132</td>\n","      <td>0.905447</td>\n","      <td>0.863174</td>\n","      <td>0.952073</td>\n","      <td>0.905447</td>\n","      <td>0.863174</td>\n","      <td>0.952073</td>\n","    </tr>\n","    <tr>\n","      <td>8148</td>\n","      <td>0.000000</td>\n","      <td>0.001606</td>\n","      <td>0.942591</td>\n","      <td>0.921482</td>\n","      <td>0.966755</td>\n","      <td>0.978230</td>\n","      <td>0.978230</td>\n","      <td>0.978230</td>\n","      <td>0.897358</td>\n","      <td>0.848320</td>\n","      <td>0.952413</td>\n","      <td>0.897358</td>\n","      <td>0.848320</td>\n","      <td>0.952413</td>\n","    </tr>\n","    <tr>\n","      <td>9506</td>\n","      <td>0.000000</td>\n","      <td>0.001333</td>\n","      <td>0.940192</td>\n","      <td>0.914402</td>\n","      <td>0.970759</td>\n","      <td>0.977007</td>\n","      <td>0.977007</td>\n","      <td>0.977007</td>\n","      <td>0.893268</td>\n","      <td>0.832990</td>\n","      <td>0.962950</td>\n","      <td>0.893268</td>\n","      <td>0.832990</td>\n","      <td>0.962950</td>\n","    </tr>\n","    <tr>\n","      <td>10864</td>\n","      <td>0.000000</td>\n","      <td>0.001876</td>\n","      <td>0.945775</td>\n","      <td>0.927235</td>\n","      <td>0.966603</td>\n","      <td>0.979588</td>\n","      <td>0.979588</td>\n","      <td>0.979588</td>\n","      <td>0.902955</td>\n","      <td>0.860043</td>\n","      <td>0.950374</td>\n","      <td>0.902955</td>\n","      <td>0.860043</td>\n","      <td>0.950374</td>\n","    </tr>\n","    <tr>\n","      <td>12222</td>\n","      <td>0.000000</td>\n","      <td>0.001789</td>\n","      <td>0.945092</td>\n","      <td>0.925104</td>\n","      <td>0.967774</td>\n","      <td>0.979249</td>\n","      <td>0.979249</td>\n","      <td>0.979249</td>\n","      <td>0.901784</td>\n","      <td>0.855444</td>\n","      <td>0.953433</td>\n","      <td>0.901784</td>\n","      <td>0.855444</td>\n","      <td>0.953433</td>\n","    </tr>\n","    <tr>\n","      <td>13580</td>\n","      <td>0.000000</td>\n","      <td>0.001863</td>\n","      <td>0.944955</td>\n","      <td>0.924092</td>\n","      <td>0.968775</td>\n","      <td>0.979147</td>\n","      <td>0.979147</td>\n","      <td>0.979147</td>\n","      <td>0.901571</td>\n","      <td>0.853155</td>\n","      <td>0.955812</td>\n","      <td>0.901571</td>\n","      <td>0.853155</td>\n","      <td>0.955812</td>\n","    </tr>\n","    <tr>\n","      <td>14938</td>\n","      <td>0.000000</td>\n","      <td>0.001106</td>\n","      <td>0.934992</td>\n","      <td>0.903335</td>\n","      <td>0.974368</td>\n","      <td>0.974528</td>\n","      <td>0.974528</td>\n","      <td>0.974528</td>\n","      <td>0.884295</td>\n","      <td>0.809605</td>\n","      <td>0.974167</td>\n","      <td>0.884295</td>\n","      <td>0.809605</td>\n","      <td>0.974167</td>\n","    </tr>\n","    <tr>\n","      <td>16296</td>\n","      <td>0.000000</td>\n","      <td>0.001781</td>\n","      <td>0.944865</td>\n","      <td>0.922831</td>\n","      <td>0.970229</td>\n","      <td>0.979045</td>\n","      <td>0.979045</td>\n","      <td>0.979045</td>\n","      <td>0.901453</td>\n","      <td>0.850256</td>\n","      <td>0.959211</td>\n","      <td>0.901453</td>\n","      <td>0.850256</td>\n","      <td>0.959211</td>\n","    </tr>\n","    <tr>\n","      <td>17654</td>\n","      <td>0.000000</td>\n","      <td>0.002005</td>\n","      <td>0.945626</td>\n","      <td>0.925041</td>\n","      <td>0.969077</td>\n","      <td>0.979419</td>\n","      <td>0.979419</td>\n","      <td>0.979419</td>\n","      <td>0.902760</td>\n","      <td>0.855015</td>\n","      <td>0.956152</td>\n","      <td>0.902760</td>\n","      <td>0.855015</td>\n","      <td>0.956152</td>\n","    </tr>\n","    <tr>\n","      <td>19012</td>\n","      <td>0.000000</td>\n","      <td>0.002096</td>\n","      <td>0.945857</td>\n","      <td>0.925521</td>\n","      <td>0.968982</td>\n","      <td>0.979520</td>\n","      <td>0.979520</td>\n","      <td>0.979520</td>\n","      <td>0.903164</td>\n","      <td>0.856012</td>\n","      <td>0.955812</td>\n","      <td>0.903164</td>\n","      <td>0.856012</td>\n","      <td>0.955812</td>\n","    </tr>\n","    <tr>\n","      <td>20370</td>\n","      <td>0.000000</td>\n","      <td>0.002104</td>\n","      <td>0.945549</td>\n","      <td>0.925488</td>\n","      <td>0.968321</td>\n","      <td>0.979419</td>\n","      <td>0.979419</td>\n","      <td>0.979419</td>\n","      <td>0.902604</td>\n","      <td>0.856098</td>\n","      <td>0.954453</td>\n","      <td>0.902604</td>\n","      <td>0.856098</td>\n","      <td>0.954453</td>\n","    </tr>\n","    <tr>\n","      <td>21728</td>\n","      <td>0.000000</td>\n","      <td>0.001893</td>\n","      <td>0.944426</td>\n","      <td>0.922364</td>\n","      <td>0.969833</td>\n","      <td>0.978875</td>\n","      <td>0.978875</td>\n","      <td>0.978875</td>\n","      <td>0.900671</td>\n","      <td>0.849398</td>\n","      <td>0.958532</td>\n","      <td>0.900671</td>\n","      <td>0.849398</td>\n","      <td>0.958532</td>\n","    </tr>\n","    <tr>\n","      <td>23086</td>\n","      <td>0.000000</td>\n","      <td>0.001924</td>\n","      <td>0.944507</td>\n","      <td>0.922492</td>\n","      <td>0.969851</td>\n","      <td>0.978909</td>\n","      <td>0.978909</td>\n","      <td>0.978909</td>\n","      <td>0.900815</td>\n","      <td>0.849654</td>\n","      <td>0.958532</td>\n","      <td>0.900815</td>\n","      <td>0.849654</td>\n","      <td>0.958532</td>\n","    </tr>\n","    <tr>\n","      <td>24444</td>\n","      <td>0.000000</td>\n","      <td>0.001905</td>\n","      <td>0.944702</td>\n","      <td>0.922575</td>\n","      <td>0.970191</td>\n","      <td>0.978977</td>\n","      <td>0.978977</td>\n","      <td>0.978977</td>\n","      <td>0.901166</td>\n","      <td>0.849744</td>\n","      <td>0.959211</td>\n","      <td>0.901166</td>\n","      <td>0.849744</td>\n","      <td>0.959211</td>\n","    </tr>\n","    <tr>\n","      <td>25802</td>\n","      <td>0.000000</td>\n","      <td>0.001911</td>\n","      <td>0.944702</td>\n","      <td>0.922575</td>\n","      <td>0.970191</td>\n","      <td>0.978977</td>\n","      <td>0.978977</td>\n","      <td>0.978977</td>\n","      <td>0.901166</td>\n","      <td>0.849744</td>\n","      <td>0.959211</td>\n","      <td>0.901166</td>\n","      <td>0.849744</td>\n","      <td>0.959211</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to trained_scibert_ner_model/checkpoint-1358\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-2716\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-4074\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-5432\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-6790\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-8148\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-9506\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-10864\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-12222\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-13580\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-14938\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-16296\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-17654\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-19012\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-20370\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-21728\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-23086\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-24444\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","Saving model checkpoint to trained_scibert_ner_model/checkpoint-25802\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Training process has finished.\n","Starting testing\n","{'eval_macro_f1': 0.9447015526469789, 'eval_macro_precision': 0.9225751999467278, 'eval_macro_recall': 0.9701913265809987, 'eval_micro_f1': 0.9789770411628855, 'eval_micro_precision': 0.9789770411628855, 'eval_micro_recall': 0.9789770411628855, 'eval_macro_f1_no_o': 0.9011655756027462, 'eval_macro_precision_no_o': 0.8497440529960855, 'eval_macro_recall_no_o': 0.9592114208021754, 'eval_micro_f1_no_o': 0.9011655756027462, 'eval_micro_precision_no_o': 0.8497440529960855, 'eval_micro_recall_no_o': 0.9592114208021754, 'eval_loss': 0.001911111617969928, 'eval_runtime': 19.7715, 'eval_samples_per_second': 11.886, 'eval_steps_per_second': 2.984, 'epoch': 19.0}\n","Accuracy for fold  9 :  0.9011655756027462  --  0.9789770411628855\n","--------------------------------\n","Testing process has finished.\n"]}]},{"cell_type":"code","source":["loop_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGpNOJ075T6k","executionInfo":{"status":"ok","timestamp":1647112306593,"user_tz":360,"elapsed":14,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"8bc7f618-5c6f-49fd-c831-f6591b2dc9d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.92101484, 0.91168543, 0.92174748, 0.92517007, 0.91799544,\n","       0.916491  , 0.91202063, 0.90014425, 0.90932642, 0.90116558])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["loop_resultss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQeBo6E-5TbC","executionInfo":{"status":"ok","timestamp":1647112306594,"user_tz":360,"elapsed":6,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"359d87b6-1b44-4828-8b61-8d9d24446d37"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.98318843, 0.98108273, 0.9833922 , 0.9843092 , 0.98288276,\n","       0.98250917, 0.98145632, 0.97884119, 0.98098084, 0.97897704])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["sum = 0.0\n","for value in loop_results:\n","  sum += value\n","print(f'Average micro_f1_no_o: {sum/len(loop_results)} %')\n","\n","sum = 0.0\n","for value in loop_resultss:\n","  sum += value\n","print(f'Average micro_f1: {sum/len(loop_results)} %')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZP7IxgqF5iM0","executionInfo":{"status":"ok","timestamp":1647112306594,"user_tz":360,"elapsed":5,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"5ba352da-f4a5-4cfc-9ed8-7439c708d618"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average micro_f1_no_o: 0.9136761140387948 %\n","Average micro_f1: 0.9817619888602092 %\n"]}]},{"cell_type":"code","source":["ner_model = NerModel(BertEmbModel).to('cuda')  # make sure we move the model to the GPU for training\n","ner_model.load_state_dict(torch.load(save_path))\n","ner_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7Ql0Zwa5lRi","executionInfo":{"status":"ok","timestamp":1647112306882,"user_tz":360,"elapsed":292,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"30748551-398f-4068-8172-45610b002176"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NerModel(\n","  (sci_embeddings): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (embd_dropout): Dropout(p=0.1, inplace=False)\n","  (ff_dropout): Dropout(p=0.1, inplace=False)\n","  (ff): Linear(in_features=200, out_features=14, bias=True)\n","  (tanh): Tanh()\n","  (lstm): LSTM(768, 100, bidirectional=True)\n","  (lstm_drop): Dropout(p=0.4, inplace=False)\n","  (ff_act): ReLU()\n","  (classifier): Linear(in_features=14, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"dcdDIwq0kXy2"},"source":["# Get Values of Thruth"]},{"cell_type":"code","metadata":{"id":"uCnsc2rplFzW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647112306882,"user_tz":360,"elapsed":3,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"0167c71b-fa52-44a1-aea5-67cc970ffb29"},"source":["# Pytorch thing (if we aren't training, do this)\n","ner_model.eval()\n","ner_model.to('cuda')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NerModel(\n","  (sci_embeddings): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (embd_dropout): Dropout(p=0.1, inplace=False)\n","  (ff_dropout): Dropout(p=0.1, inplace=False)\n","  (ff): Linear(in_features=200, out_features=14, bias=True)\n","  (tanh): Tanh()\n","  (lstm): LSTM(768, 100, bidirectional=True)\n","  (lstm_drop): Dropout(p=0.4, inplace=False)\n","  (ff_act): ReLU()\n","  (classifier): Linear(in_features=14, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"FVfjoHmcI0pL"},"source":["output_preds = []\n","output_real = []\n","for x in range(len(test)):\n","  inputs1 = test[x]['input_ids'][0].clone().detach().to(torch.long).unsqueeze(0).to('cuda')\n","  inputs2 = test[x]['attention_mask'][0].clone().detach().to(torch.long).unsqueeze(0).to('cuda')\n","  inputs = {'input_ids': inputs1,  'attention_mask': inputs2}\n","  #print(inputs)\n","\n","  temp_test = test[x]\n","  temp_out = temp_test.pop(\"labels\")\n","  output_real.append(np.array(temp_out[temp_out != -100])) \n","\n","  gen_preds = ner_model(**inputs)\n","  label_preds = np.argmax(gen_preds.cpu().detach().numpy(), axis=-1)[0]\n","  output_preds.append(label_preds[temp_out != -100])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXBvXjpP-VGh"},"source":["for x in range(len(output_real)):\n","  output_real[x] = [ID2Entity(y) for y in output_real[x]]\n","  output_preds[x] = [ID2Entity(z) for z in output_preds[x]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pd1Xl1giQ7z7"},"source":["#output_real[28]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rslGT68vvC8","colab":{"base_uri":"https://localhost:8080/","height":706},"executionInfo":{"status":"ok","timestamp":1647112343222,"user_tz":360,"elapsed":1185,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"dea5eb23-ebc1-40e8-ef16-ae0ca599e4ff"},"source":["from sklearn.metrics import mean_squared_error, multilabel_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams['figure.figsize'] = [12, 9]\n","plt.rcParams[\"figure.autolayout\"] = True\n","plt.rcParams.update({'font.size': 13})\n","\n","labels = [\"None\", 'B']\n","cm = confusion_matrix(output_real[0], output_preds[0], labels=labels)\n","for x in range(len(output_real)-1):\n","  cm += confusion_matrix(output_real[x+1], output_preds[x+1], labels=labels)\n","print(cm)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","disp.plot(cmap=plt.cm.Blues)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[26005   499]\n"," [  120  2822]]\n"]},{"output_type":"execute_result","data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f5a59452210>"]},"metadata":{},"execution_count":29},{"output_type":"display_data","data":{"text/plain":["<Figure size 864x648 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"m10aBi5x5Hdb"},"source":["del cm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgGd1XfheERK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647112343223,"user_tz":360,"elapsed":6,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"8fe2a365-66ca-4ffd-a35c-b31002b3d2d3"},"source":["len(output_real)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["941"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"fHlg1QV3eZCa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0w9Lc0d3iOT"},"source":["# Test over Text"]},{"cell_type":"code","metadata":{"id":"mgNM3p1q3hrf"},"source":["def prepare_input(txt):\n","  inputs = BertTokenizer(txt, return_tensors='pt', padding='max_length', truncation=True, max_length=150).to('cuda')\n","  return inputs\n","\n","input_text = [\"English is shown to be trans-context-free on the basis of coordinations of the respectively type that involve strictly syntactic cross-serial agreement.\"]\n","##input_text = [\"The transient analysis of gyro-elastic structured media, composed of periodically placed masses interconnected by elastic rods and attached to gyroscopic spinners, is presented.\"] \n","##input_text = [\"The results indicated that thermal curing promoted the early strength of mortars, while decreased the late strength of mortars.\"] \n","##input_text = [\"A wide variety of processes are attested in the literature, and we find different forms of clippings in our data, including mixtures of different clippings, homophone respellings, phonetic respellings in-cluding informal oral forms, initialisms (but no acronyms), and mixtures of clipping together with homo-phone and phonetic respellings.\"] \n","\n","#input_text = [\"The goal is to accurately predict the running time of applications for task scheduling and job migration.\"] \n","#input_text = [\"This paper reports on the development of a cross-domain framework for describing complex design practices.\"] \n","#input_text = [\"Studies of inequality in China typically ignore cost of liv-ing differences between areas.\"] \n","#input_text = [\"The present study was designed to explore the long-term differences be-tween three mouse models for depression.\"]\n","#input_text = [\"Finally, regarding professional competencies, teachers appeared to be largely unprepared to conduct language assessments consistent with the LAR demands.\"] \n","\n","##input_text = [\"propose a fast and reliable restoration method of virtual resources on OpenStack when physical servers or virtual machines are down.\"] \n","##input_text = [\"The results from our simulations reveal that the network assisted adaptation clearly outperforms the purely client-based DASH heuristics in some of the metrics, not all of them, particularly, in situations when the achievable throughput is moderately high or the link quality of the mobile clients does not differ from each other substantially.\"] \n","##input_text = [\"For hard rock drilling in coal mine, the drilling efficiency and service life of polycrystalline diamond compact bit are very low.\"] \n","##input_text = [\"Capturing changes in foreign reserves and exchange rates through the exchange market pressure, this article investigates whether economic policy uncertainty plays any role in exchange market pressure movements while controlling for the effects of domestic and external factors.\"] \n","##input_text = [\"This paper presents design of an self contained actuators unit in wide area damping control of power system in stabilizing system response for both nominal system condition and during actuator faults.\"] \n","\n","##input_text = [\"Ultrasound-based brain stimulation techniques may become a powerful new technique to modulate the human brain in a focal and targeted manner.\"] \n","#input_text = [\"Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization.\"]\n","\n","# Tokenize + pad\n","inputs = prepare_input(input_text)\n","\n","#inputs\n","#print(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaG1vUq58BFI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642361520864,"user_tz":360,"elapsed":281,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"83822697-84bd-41f7-accb-fe53c6d937b5"},"source":["# Pytorch thing (if we aren't training, do this)\n","ner_model.eval()\n","\n","# Get predictions\n","preds = ner_model(**inputs).cpu().detach().numpy()\n","preds = np.argmax(preds, axis=-1)[0]\n","pred_labels = [ID2Entity(x) for x in preds]\n","\n","# Convert token ids to text\n","tokens = BertTokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","\n","# Display result\n","for token, label in zip(tokens, pred_labels):\n","  if token == '[SEP]':\n","    break\n","  if token == '[CLS]':\n","    continue\n","  print('{} -> {}'.format(token, label))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["english -> None\n","is -> None\n","shown -> None\n","to -> None\n","be -> None\n","trans -> None\n","- -> None\n","context -> None\n","- -> None\n","free -> None\n","on -> None\n","the -> None\n","basis -> None\n","of -> None\n","coordination -> None\n","##s -> None\n","of -> None\n","the -> None\n","respectively -> None\n","type -> None\n","that -> None\n","involve -> None\n","strictly -> None\n","syntactic -> None\n","cross -> None\n","- -> None\n","serial -> None\n","agreement -> None\n",". -> None\n"]}]},{"cell_type":"markdown","metadata":{"id":"hC35X0v72kXB"},"source":["# Model Save and Load"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AnK4H5vP2kJU","executionInfo":{"status":"ok","timestamp":1639143321741,"user_tz":360,"elapsed":347,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"537a928e-691b-4471-f65b-b7650341f273"},"source":["print(\"Our model: \\n\\n\", ner_model, '\\n')\n","print(\"The state dict keys: \\n\\n\", ner_model.state_dict().keys())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our model: \n","\n"," NerModel(\n","  (sci_embeddings): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (embd_dropout): Dropout(p=0.1, inplace=False)\n","  (ff_dropout): Dropout(p=0.1, inplace=False)\n","  (ff): Linear(in_features=200, out_features=14, bias=True)\n","  (tanh): Tanh()\n","  (lstm): LSTM(768, 100, bidirectional=True)\n","  (lstm_drop): Dropout(p=0.4, inplace=False)\n","  (ff_act): ReLU()\n","  (classifier): Linear(in_features=14, out_features=3, bias=True)\n",") \n","\n","The state dict keys: \n","\n"," odict_keys(['sci_embeddings.embeddings.position_ids', 'sci_embeddings.embeddings.word_embeddings.weight', 'sci_embeddings.embeddings.position_embeddings.weight', 'sci_embeddings.embeddings.token_type_embeddings.weight', 'sci_embeddings.embeddings.LayerNorm.weight', 'sci_embeddings.embeddings.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.attention.self.query.weight', 'sci_embeddings.encoder.layer.0.attention.self.query.bias', 'sci_embeddings.encoder.layer.0.attention.self.key.weight', 'sci_embeddings.encoder.layer.0.attention.self.key.bias', 'sci_embeddings.encoder.layer.0.attention.self.value.weight', 'sci_embeddings.encoder.layer.0.attention.self.value.bias', 'sci_embeddings.encoder.layer.0.attention.output.dense.weight', 'sci_embeddings.encoder.layer.0.attention.output.dense.bias', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.intermediate.dense.weight', 'sci_embeddings.encoder.layer.0.intermediate.dense.bias', 'sci_embeddings.encoder.layer.0.output.dense.weight', 'sci_embeddings.encoder.layer.0.output.dense.bias', 'sci_embeddings.encoder.layer.0.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.attention.self.query.weight', 'sci_embeddings.encoder.layer.1.attention.self.query.bias', 'sci_embeddings.encoder.layer.1.attention.self.key.weight', 'sci_embeddings.encoder.layer.1.attention.self.key.bias', 'sci_embeddings.encoder.layer.1.attention.self.value.weight', 'sci_embeddings.encoder.layer.1.attention.self.value.bias', 'sci_embeddings.encoder.layer.1.attention.output.dense.weight', 'sci_embeddings.encoder.layer.1.attention.output.dense.bias', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.intermediate.dense.weight', 'sci_embeddings.encoder.layer.1.intermediate.dense.bias', 'sci_embeddings.encoder.layer.1.output.dense.weight', 'sci_embeddings.encoder.layer.1.output.dense.bias', 'sci_embeddings.encoder.layer.1.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.attention.self.query.weight', 'sci_embeddings.encoder.layer.2.attention.self.query.bias', 'sci_embeddings.encoder.layer.2.attention.self.key.weight', 'sci_embeddings.encoder.layer.2.attention.self.key.bias', 'sci_embeddings.encoder.layer.2.attention.self.value.weight', 'sci_embeddings.encoder.layer.2.attention.self.value.bias', 'sci_embeddings.encoder.layer.2.attention.output.dense.weight', 'sci_embeddings.encoder.layer.2.attention.output.dense.bias', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.intermediate.dense.weight', 'sci_embeddings.encoder.layer.2.intermediate.dense.bias', 'sci_embeddings.encoder.layer.2.output.dense.weight', 'sci_embeddings.encoder.layer.2.output.dense.bias', 'sci_embeddings.encoder.layer.2.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.attention.self.query.weight', 'sci_embeddings.encoder.layer.3.attention.self.query.bias', 'sci_embeddings.encoder.layer.3.attention.self.key.weight', 'sci_embeddings.encoder.layer.3.attention.self.key.bias', 'sci_embeddings.encoder.layer.3.attention.self.value.weight', 'sci_embeddings.encoder.layer.3.attention.self.value.bias', 'sci_embeddings.encoder.layer.3.attention.output.dense.weight', 'sci_embeddings.encoder.layer.3.attention.output.dense.bias', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.intermediate.dense.weight', 'sci_embeddings.encoder.layer.3.intermediate.dense.bias', 'sci_embeddings.encoder.layer.3.output.dense.weight', 'sci_embeddings.encoder.layer.3.output.dense.bias', 'sci_embeddings.encoder.layer.3.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.attention.self.query.weight', 'sci_embeddings.encoder.layer.4.attention.self.query.bias', 'sci_embeddings.encoder.layer.4.attention.self.key.weight', 'sci_embeddings.encoder.layer.4.attention.self.key.bias', 'sci_embeddings.encoder.layer.4.attention.self.value.weight', 'sci_embeddings.encoder.layer.4.attention.self.value.bias', 'sci_embeddings.encoder.layer.4.attention.output.dense.weight', 'sci_embeddings.encoder.layer.4.attention.output.dense.bias', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.intermediate.dense.weight', 'sci_embeddings.encoder.layer.4.intermediate.dense.bias', 'sci_embeddings.encoder.layer.4.output.dense.weight', 'sci_embeddings.encoder.layer.4.output.dense.bias', 'sci_embeddings.encoder.layer.4.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.attention.self.query.weight', 'sci_embeddings.encoder.layer.5.attention.self.query.bias', 'sci_embeddings.encoder.layer.5.attention.self.key.weight', 'sci_embeddings.encoder.layer.5.attention.self.key.bias', 'sci_embeddings.encoder.layer.5.attention.self.value.weight', 'sci_embeddings.encoder.layer.5.attention.self.value.bias', 'sci_embeddings.encoder.layer.5.attention.output.dense.weight', 'sci_embeddings.encoder.layer.5.attention.output.dense.bias', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.intermediate.dense.weight', 'sci_embeddings.encoder.layer.5.intermediate.dense.bias', 'sci_embeddings.encoder.layer.5.output.dense.weight', 'sci_embeddings.encoder.layer.5.output.dense.bias', 'sci_embeddings.encoder.layer.5.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.attention.self.query.weight', 'sci_embeddings.encoder.layer.6.attention.self.query.bias', 'sci_embeddings.encoder.layer.6.attention.self.key.weight', 'sci_embeddings.encoder.layer.6.attention.self.key.bias', 'sci_embeddings.encoder.layer.6.attention.self.value.weight', 'sci_embeddings.encoder.layer.6.attention.self.value.bias', 'sci_embeddings.encoder.layer.6.attention.output.dense.weight', 'sci_embeddings.encoder.layer.6.attention.output.dense.bias', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.intermediate.dense.weight', 'sci_embeddings.encoder.layer.6.intermediate.dense.bias', 'sci_embeddings.encoder.layer.6.output.dense.weight', 'sci_embeddings.encoder.layer.6.output.dense.bias', 'sci_embeddings.encoder.layer.6.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.attention.self.query.weight', 'sci_embeddings.encoder.layer.7.attention.self.query.bias', 'sci_embeddings.encoder.layer.7.attention.self.key.weight', 'sci_embeddings.encoder.layer.7.attention.self.key.bias', 'sci_embeddings.encoder.layer.7.attention.self.value.weight', 'sci_embeddings.encoder.layer.7.attention.self.value.bias', 'sci_embeddings.encoder.layer.7.attention.output.dense.weight', 'sci_embeddings.encoder.layer.7.attention.output.dense.bias', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.intermediate.dense.weight', 'sci_embeddings.encoder.layer.7.intermediate.dense.bias', 'sci_embeddings.encoder.layer.7.output.dense.weight', 'sci_embeddings.encoder.layer.7.output.dense.bias', 'sci_embeddings.encoder.layer.7.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.attention.self.query.weight', 'sci_embeddings.encoder.layer.8.attention.self.query.bias', 'sci_embeddings.encoder.layer.8.attention.self.key.weight', 'sci_embeddings.encoder.layer.8.attention.self.key.bias', 'sci_embeddings.encoder.layer.8.attention.self.value.weight', 'sci_embeddings.encoder.layer.8.attention.self.value.bias', 'sci_embeddings.encoder.layer.8.attention.output.dense.weight', 'sci_embeddings.encoder.layer.8.attention.output.dense.bias', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.intermediate.dense.weight', 'sci_embeddings.encoder.layer.8.intermediate.dense.bias', 'sci_embeddings.encoder.layer.8.output.dense.weight', 'sci_embeddings.encoder.layer.8.output.dense.bias', 'sci_embeddings.encoder.layer.8.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.attention.self.query.weight', 'sci_embeddings.encoder.layer.9.attention.self.query.bias', 'sci_embeddings.encoder.layer.9.attention.self.key.weight', 'sci_embeddings.encoder.layer.9.attention.self.key.bias', 'sci_embeddings.encoder.layer.9.attention.self.value.weight', 'sci_embeddings.encoder.layer.9.attention.self.value.bias', 'sci_embeddings.encoder.layer.9.attention.output.dense.weight', 'sci_embeddings.encoder.layer.9.attention.output.dense.bias', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.intermediate.dense.weight', 'sci_embeddings.encoder.layer.9.intermediate.dense.bias', 'sci_embeddings.encoder.layer.9.output.dense.weight', 'sci_embeddings.encoder.layer.9.output.dense.bias', 'sci_embeddings.encoder.layer.9.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.attention.self.query.weight', 'sci_embeddings.encoder.layer.10.attention.self.query.bias', 'sci_embeddings.encoder.layer.10.attention.self.key.weight', 'sci_embeddings.encoder.layer.10.attention.self.key.bias', 'sci_embeddings.encoder.layer.10.attention.self.value.weight', 'sci_embeddings.encoder.layer.10.attention.self.value.bias', 'sci_embeddings.encoder.layer.10.attention.output.dense.weight', 'sci_embeddings.encoder.layer.10.attention.output.dense.bias', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.intermediate.dense.weight', 'sci_embeddings.encoder.layer.10.intermediate.dense.bias', 'sci_embeddings.encoder.layer.10.output.dense.weight', 'sci_embeddings.encoder.layer.10.output.dense.bias', 'sci_embeddings.encoder.layer.10.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.attention.self.query.weight', 'sci_embeddings.encoder.layer.11.attention.self.query.bias', 'sci_embeddings.encoder.layer.11.attention.self.key.weight', 'sci_embeddings.encoder.layer.11.attention.self.key.bias', 'sci_embeddings.encoder.layer.11.attention.self.value.weight', 'sci_embeddings.encoder.layer.11.attention.self.value.bias', 'sci_embeddings.encoder.layer.11.attention.output.dense.weight', 'sci_embeddings.encoder.layer.11.attention.output.dense.bias', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.intermediate.dense.weight', 'sci_embeddings.encoder.layer.11.intermediate.dense.bias', 'sci_embeddings.encoder.layer.11.output.dense.weight', 'sci_embeddings.encoder.layer.11.output.dense.bias', 'sci_embeddings.encoder.layer.11.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.output.LayerNorm.bias', 'sci_embeddings.pooler.dense.weight', 'sci_embeddings.pooler.dense.bias', 'ff.weight', 'ff.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias'])\n"]}]},{"cell_type":"code","metadata":{"id":"KftdO6GD2rWF"},"source":["torch.save(ner_model.state_dict(), 'trained_model_dic.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download checkpoint file\n","files.download('trained_model_dic.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"n2xrcRkQbwvH","executionInfo":{"status":"ok","timestamp":1639143322986,"user_tz":360,"elapsed":8,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"724e6884-3d3f-440d-bbe2-2030ac185d8b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_d7f824df-1211-4d38-b2b6-dec4d84cde45\", \"trained_model_dic.pth\", 442559399)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1hvL5Nb3kmo","executionInfo":{"status":"ok","timestamp":1637655465909,"user_tz":360,"elapsed":11,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"a2910628-2592-4edd-b135-c329d0352c5e"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["checkpoint.pth\t  model-fold-1.pth  model-fold-4.pth  trained_model_dic.pth\n","dev.json\t  model-fold-2.pth  sample_data       trained_scibert_ner_model\n","model-fold-0.pth  model-fold-3.pth  test.json\t      train.json\n"]}]},{"cell_type":"markdown","metadata":{"id":"hQL7fqSY2x9e"},"source":["Loading the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ufmrdfeT6Qz","executionInfo":{"status":"ok","timestamp":1638565657653,"user_tz":360,"elapsed":324,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"fd7fd743-41b0-4b32-eefd-db4f427d5da0"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data  test_500_v2.conll\ttrain_1500_v2.conll  trained_model_dic.pth\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4Eus2gmJ-bS","executionInfo":{"status":"ok","timestamp":1638831550021,"user_tz":360,"elapsed":10896,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"1e8838d1-d51c-47db-8ee2-c0bbb2bb3a0b"},"source":["state_dict = torch.load('trained_model_dic.pth')\n","print(state_dict.keys())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["odict_keys(['sci_embeddings.embeddings.position_ids', 'sci_embeddings.embeddings.word_embeddings.weight', 'sci_embeddings.embeddings.position_embeddings.weight', 'sci_embeddings.embeddings.token_type_embeddings.weight', 'sci_embeddings.embeddings.LayerNorm.weight', 'sci_embeddings.embeddings.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.attention.self.query.weight', 'sci_embeddings.encoder.layer.0.attention.self.query.bias', 'sci_embeddings.encoder.layer.0.attention.self.key.weight', 'sci_embeddings.encoder.layer.0.attention.self.key.bias', 'sci_embeddings.encoder.layer.0.attention.self.value.weight', 'sci_embeddings.encoder.layer.0.attention.self.value.bias', 'sci_embeddings.encoder.layer.0.attention.output.dense.weight', 'sci_embeddings.encoder.layer.0.attention.output.dense.bias', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.0.intermediate.dense.weight', 'sci_embeddings.encoder.layer.0.intermediate.dense.bias', 'sci_embeddings.encoder.layer.0.output.dense.weight', 'sci_embeddings.encoder.layer.0.output.dense.bias', 'sci_embeddings.encoder.layer.0.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.0.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.attention.self.query.weight', 'sci_embeddings.encoder.layer.1.attention.self.query.bias', 'sci_embeddings.encoder.layer.1.attention.self.key.weight', 'sci_embeddings.encoder.layer.1.attention.self.key.bias', 'sci_embeddings.encoder.layer.1.attention.self.value.weight', 'sci_embeddings.encoder.layer.1.attention.self.value.bias', 'sci_embeddings.encoder.layer.1.attention.output.dense.weight', 'sci_embeddings.encoder.layer.1.attention.output.dense.bias', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.1.intermediate.dense.weight', 'sci_embeddings.encoder.layer.1.intermediate.dense.bias', 'sci_embeddings.encoder.layer.1.output.dense.weight', 'sci_embeddings.encoder.layer.1.output.dense.bias', 'sci_embeddings.encoder.layer.1.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.1.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.attention.self.query.weight', 'sci_embeddings.encoder.layer.2.attention.self.query.bias', 'sci_embeddings.encoder.layer.2.attention.self.key.weight', 'sci_embeddings.encoder.layer.2.attention.self.key.bias', 'sci_embeddings.encoder.layer.2.attention.self.value.weight', 'sci_embeddings.encoder.layer.2.attention.self.value.bias', 'sci_embeddings.encoder.layer.2.attention.output.dense.weight', 'sci_embeddings.encoder.layer.2.attention.output.dense.bias', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.2.intermediate.dense.weight', 'sci_embeddings.encoder.layer.2.intermediate.dense.bias', 'sci_embeddings.encoder.layer.2.output.dense.weight', 'sci_embeddings.encoder.layer.2.output.dense.bias', 'sci_embeddings.encoder.layer.2.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.2.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.attention.self.query.weight', 'sci_embeddings.encoder.layer.3.attention.self.query.bias', 'sci_embeddings.encoder.layer.3.attention.self.key.weight', 'sci_embeddings.encoder.layer.3.attention.self.key.bias', 'sci_embeddings.encoder.layer.3.attention.self.value.weight', 'sci_embeddings.encoder.layer.3.attention.self.value.bias', 'sci_embeddings.encoder.layer.3.attention.output.dense.weight', 'sci_embeddings.encoder.layer.3.attention.output.dense.bias', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.3.intermediate.dense.weight', 'sci_embeddings.encoder.layer.3.intermediate.dense.bias', 'sci_embeddings.encoder.layer.3.output.dense.weight', 'sci_embeddings.encoder.layer.3.output.dense.bias', 'sci_embeddings.encoder.layer.3.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.3.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.attention.self.query.weight', 'sci_embeddings.encoder.layer.4.attention.self.query.bias', 'sci_embeddings.encoder.layer.4.attention.self.key.weight', 'sci_embeddings.encoder.layer.4.attention.self.key.bias', 'sci_embeddings.encoder.layer.4.attention.self.value.weight', 'sci_embeddings.encoder.layer.4.attention.self.value.bias', 'sci_embeddings.encoder.layer.4.attention.output.dense.weight', 'sci_embeddings.encoder.layer.4.attention.output.dense.bias', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.4.intermediate.dense.weight', 'sci_embeddings.encoder.layer.4.intermediate.dense.bias', 'sci_embeddings.encoder.layer.4.output.dense.weight', 'sci_embeddings.encoder.layer.4.output.dense.bias', 'sci_embeddings.encoder.layer.4.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.4.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.attention.self.query.weight', 'sci_embeddings.encoder.layer.5.attention.self.query.bias', 'sci_embeddings.encoder.layer.5.attention.self.key.weight', 'sci_embeddings.encoder.layer.5.attention.self.key.bias', 'sci_embeddings.encoder.layer.5.attention.self.value.weight', 'sci_embeddings.encoder.layer.5.attention.self.value.bias', 'sci_embeddings.encoder.layer.5.attention.output.dense.weight', 'sci_embeddings.encoder.layer.5.attention.output.dense.bias', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.5.intermediate.dense.weight', 'sci_embeddings.encoder.layer.5.intermediate.dense.bias', 'sci_embeddings.encoder.layer.5.output.dense.weight', 'sci_embeddings.encoder.layer.5.output.dense.bias', 'sci_embeddings.encoder.layer.5.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.5.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.attention.self.query.weight', 'sci_embeddings.encoder.layer.6.attention.self.query.bias', 'sci_embeddings.encoder.layer.6.attention.self.key.weight', 'sci_embeddings.encoder.layer.6.attention.self.key.bias', 'sci_embeddings.encoder.layer.6.attention.self.value.weight', 'sci_embeddings.encoder.layer.6.attention.self.value.bias', 'sci_embeddings.encoder.layer.6.attention.output.dense.weight', 'sci_embeddings.encoder.layer.6.attention.output.dense.bias', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.6.intermediate.dense.weight', 'sci_embeddings.encoder.layer.6.intermediate.dense.bias', 'sci_embeddings.encoder.layer.6.output.dense.weight', 'sci_embeddings.encoder.layer.6.output.dense.bias', 'sci_embeddings.encoder.layer.6.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.6.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.attention.self.query.weight', 'sci_embeddings.encoder.layer.7.attention.self.query.bias', 'sci_embeddings.encoder.layer.7.attention.self.key.weight', 'sci_embeddings.encoder.layer.7.attention.self.key.bias', 'sci_embeddings.encoder.layer.7.attention.self.value.weight', 'sci_embeddings.encoder.layer.7.attention.self.value.bias', 'sci_embeddings.encoder.layer.7.attention.output.dense.weight', 'sci_embeddings.encoder.layer.7.attention.output.dense.bias', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.7.intermediate.dense.weight', 'sci_embeddings.encoder.layer.7.intermediate.dense.bias', 'sci_embeddings.encoder.layer.7.output.dense.weight', 'sci_embeddings.encoder.layer.7.output.dense.bias', 'sci_embeddings.encoder.layer.7.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.7.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.attention.self.query.weight', 'sci_embeddings.encoder.layer.8.attention.self.query.bias', 'sci_embeddings.encoder.layer.8.attention.self.key.weight', 'sci_embeddings.encoder.layer.8.attention.self.key.bias', 'sci_embeddings.encoder.layer.8.attention.self.value.weight', 'sci_embeddings.encoder.layer.8.attention.self.value.bias', 'sci_embeddings.encoder.layer.8.attention.output.dense.weight', 'sci_embeddings.encoder.layer.8.attention.output.dense.bias', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.8.intermediate.dense.weight', 'sci_embeddings.encoder.layer.8.intermediate.dense.bias', 'sci_embeddings.encoder.layer.8.output.dense.weight', 'sci_embeddings.encoder.layer.8.output.dense.bias', 'sci_embeddings.encoder.layer.8.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.8.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.attention.self.query.weight', 'sci_embeddings.encoder.layer.9.attention.self.query.bias', 'sci_embeddings.encoder.layer.9.attention.self.key.weight', 'sci_embeddings.encoder.layer.9.attention.self.key.bias', 'sci_embeddings.encoder.layer.9.attention.self.value.weight', 'sci_embeddings.encoder.layer.9.attention.self.value.bias', 'sci_embeddings.encoder.layer.9.attention.output.dense.weight', 'sci_embeddings.encoder.layer.9.attention.output.dense.bias', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.9.intermediate.dense.weight', 'sci_embeddings.encoder.layer.9.intermediate.dense.bias', 'sci_embeddings.encoder.layer.9.output.dense.weight', 'sci_embeddings.encoder.layer.9.output.dense.bias', 'sci_embeddings.encoder.layer.9.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.9.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.attention.self.query.weight', 'sci_embeddings.encoder.layer.10.attention.self.query.bias', 'sci_embeddings.encoder.layer.10.attention.self.key.weight', 'sci_embeddings.encoder.layer.10.attention.self.key.bias', 'sci_embeddings.encoder.layer.10.attention.self.value.weight', 'sci_embeddings.encoder.layer.10.attention.self.value.bias', 'sci_embeddings.encoder.layer.10.attention.output.dense.weight', 'sci_embeddings.encoder.layer.10.attention.output.dense.bias', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.10.intermediate.dense.weight', 'sci_embeddings.encoder.layer.10.intermediate.dense.bias', 'sci_embeddings.encoder.layer.10.output.dense.weight', 'sci_embeddings.encoder.layer.10.output.dense.bias', 'sci_embeddings.encoder.layer.10.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.10.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.attention.self.query.weight', 'sci_embeddings.encoder.layer.11.attention.self.query.bias', 'sci_embeddings.encoder.layer.11.attention.self.key.weight', 'sci_embeddings.encoder.layer.11.attention.self.key.bias', 'sci_embeddings.encoder.layer.11.attention.self.value.weight', 'sci_embeddings.encoder.layer.11.attention.self.value.bias', 'sci_embeddings.encoder.layer.11.attention.output.dense.weight', 'sci_embeddings.encoder.layer.11.attention.output.dense.bias', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.attention.output.LayerNorm.bias', 'sci_embeddings.encoder.layer.11.intermediate.dense.weight', 'sci_embeddings.encoder.layer.11.intermediate.dense.bias', 'sci_embeddings.encoder.layer.11.output.dense.weight', 'sci_embeddings.encoder.layer.11.output.dense.bias', 'sci_embeddings.encoder.layer.11.output.LayerNorm.weight', 'sci_embeddings.encoder.layer.11.output.LayerNorm.bias', 'sci_embeddings.pooler.dense.weight', 'sci_embeddings.pooler.dense.bias', 'ff.weight', 'ff.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'classifier.weight', 'classifier.bias'])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCFtuyBv3ADq","executionInfo":{"status":"ok","timestamp":1638831550219,"user_tz":360,"elapsed":209,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"40928121-bd3b-422d-e0c7-53907b33e69b"},"source":["ner_model = NerModel(BertEmbModel).to('cuda')\n","ner_model.load_state_dict(state_dict)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"ymiejDO5srYE"},"source":["# Obtain datasets' weights values (Do not run - fixed values)"]},{"cell_type":"code","metadata":{"id":"GsRs55zcsxhk"},"source":["zero = 0\n","one=0\n","two=0 \n","three=0\n","four=0\n","five=0\n","six=0\n","local_set = train\n","for i in range(len(local_set)):\n","  for j in range(len(local_set[i]['labels'])):\n","    if local_set[i]['labels'][j] == 0:\n","      zero += 1\n","    elif local_set[i]['labels'][j] == 1:\n","      one += 1\n","    elif local_set[i]['labels'][j] == 2:\n","      two += 1\n","    elif local_set[i]['labels'][j] == 3:\n","      three += 1\n","    elif local_set[i]['labels'][j] == 4:\n","      four += 1\n","    elif local_set[i]['labels'][j] == 5:\n","      five += 1\n","    elif local_set[i]['labels'][j] == 6:\n","      six += 1\n","    \n","print('0: ', zero, '1: ', one, '2: ', two, '3: ', three, '4: ', four, '5: ', five, '6: ', six)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzN52yRfsym4","executionInfo":{"status":"ok","timestamp":1639045440111,"user_tz":360,"elapsed":3574304,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"e643b9e7-6841-464f-b3c0-ee537eab0abf"},"source":["zero = 0\n","one=0\n","two=0 \n","three=0\n","four=0\n","five=0\n","six=0\n","local_set = test\n","for i in range(len(local_set)):\n","  for j in range(len(local_set[i]['labels'])):\n","    if local_set[i]['labels'][j] == 0:\n","      zero += 1\n","    elif local_set[i]['labels'][j] == 1:\n","      one += 1\n","    elif local_set[i]['labels'][j] == 2:\n","      two += 1\n","    elif local_set[i]['labels'][j] == 3:\n","      three += 1\n","    elif local_set[i]['labels'][j] == 4:\n","      four += 1\n","    elif local_set[i]['labels'][j] == 5:\n","      five += 1\n","    elif local_set[i]['labels'][j] == 6:\n","      six += 1\n","    \n","print('0: ', zero, '1: ', one, '2: ', two, '3: ', three, '4: ', four, '5: ', five, '6: ', six)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:  26504 1:  1496 2:  1446 3:  0 4:  0 5:  0 6:  0\n"]}]},{"cell_type":"code","source":["zero = 0\n","one=0\n","two=0 \n","three=0\n","four=0\n","five=0\n","six=0\n","local_set = val\n","for i in range(len(local_set)):\n","  for j in range(len(local_set[i]['labels'])):\n","    if local_set[i]['labels'][j] == 0:\n","      zero += 1\n","    elif local_set[i]['labels'][j] == 1:\n","      one += 1\n","    elif local_set[i]['labels'][j] == 2:\n","      two += 1\n","    elif local_set[i]['labels'][j] == 3:\n","      three += 1\n","    elif local_set[i]['labels'][j] == 4:\n","      four += 1\n","    elif local_set[i]['labels'][j] == 5:\n","      five += 1\n","    elif local_set[i]['labels'][j] == 6:\n","      six += 1\n","    \n","print('0: ', zero, '1: ', one, '2: ', two, '3: ', three, '4: ', four, '5: ', five, '6: ', six)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B64qF5V-trfK","executionInfo":{"status":"ok","timestamp":1639049108439,"user_tz":360,"elapsed":3668338,"user":{"displayName":"Andres Erazo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig70EzHF1rGC2-hxB0urBdpvl-4KrirMXydyMlpA=s64","userId":"09584327625341777910"}},"outputId":"060bcf71-222b-4c6a-80e3-635556aa3ff8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:  25964 1:  1119 2:  1382 3:  0 4:  0 5:  0 6:  0\n"]}]},{"cell_type":"code","metadata":{"id":"eEKHdlDyy1T2"},"source":[""],"execution_count":null,"outputs":[]}]}